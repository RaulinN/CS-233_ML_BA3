{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1 Introduction\n",
    "\n",
    "Welcome to the fifth practical session of CS233 - Introduction to Machine Learning.  \n",
    "In this exercise class we will start using Machine Learning methods to solve regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Regression Problem\n",
    "\n",
    "Let $f$ be a function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^v$ with a data set $\\left(X \\subseteq \\mathbb{R}^d, y\n",
    "\\subseteq\\mathbb{R}^v \\right )$. The regression problem is the task of estimating an approximation $\\hat{f}$ of $f$.\n",
    "Within this exercise we consider the special case of $v=1$, i.e. the problem is univariate as opposed to multivariate.\n",
    "Specifically, we will analyze the Boston house prices data set and predict costs based on properties such as per capita crime rate by town, pupil-teacher ratio by town etc.\n",
    "\n",
    "We will model the given data by means of a linear regression model, i.e. a model that explains a dependent variable in terms of a linear combination of independent variables.\n",
    "\n",
    "**Q.** How does a regression problem differ from a classification problem?  \n",
    "**A.** A classification problem has discrete-valued target variables, whereas the values of target variables in regression problems may be continuous.  \n",
    "**Q.** Why is the linear regression model a linear model? Is it linear in the dependent variables? Is it linear in the parameters?  \n",
    "**A.** The linear regression model is linear in the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Load and inspect data\n",
    "\n",
    "We load the data and split it such that 80% and 20% are train and test data, respectively. \n",
    "After, we normalize the data such that each feature has zero mean and unit standard deviation. Please fill in the required code and complete the function `normalize`.\n",
    "\n",
    "**Q.** Explore the relation between different features and the house prices. Describe what you see. Can you identify any trends?  \n",
    "**A.** The relations between feature values and house prices differ between feature dimensions. Some features (e.g. 4) are positively correlated with house prices, some (e.g. 11) are negatively correlated and for many others a clear trend is hard to spot by mere inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the data set and print a description\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()\n",
    "print(boston_dataset.DESCR)\n",
    "\n",
    "X = boston_dataset[\"data\"]\n",
    "y = boston_dataset[\"target\"]\n",
    "\n",
    "# remove categorical feature\n",
    "X = np.delete(X, 3, axis=1)\n",
    "# removing second mode\n",
    "ind = y<40\n",
    "X = X[ind,:]\n",
    "y = y[ind]\n",
    "\n",
    "# split the data into 80% training and 20% test data\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "splitRatio = 0.8\n",
    "n          = X.shape[0]\n",
    "X_train    = X[indices[0:int(n*splitRatio)],:] \n",
    "y_train    = y[indices[0:int(n*splitRatio)]] \n",
    "X_test     = X[indices[int(n*(splitRatio)):],:] \n",
    "y_test     = y[indices[int(n*(splitRatio)):]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train.npy',X_train,)\n",
    "np.save('y_train.npy',y_train,)\n",
    "np.save('X_test.npy',X_test,)\n",
    "np.save('y_test.npy',y_test,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train    = np.load('X_train.npy')\n",
    "y_train    = np.load('y_train.npy')\n",
    "X_test     = np.load('X_test.npy')\n",
    "y_test     = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_X(X,d):\n",
    "    \"\"\"\n",
    "    perform degree-d polynomial feature expansion of X, with bias but omitting interaction terms\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    \n",
    "    expand = np.ones((X.shape[0],1))\n",
    "    for idx in range(1,d+1): expand=np.hstack((expand, X**idx))\n",
    "    return expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = expand_X(X_train,6)[:,1:]\n",
    "X_test = expand_X(X_test,6)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train_feat_augmentation.npy',X_train,)\n",
    "np.save('X_test_feat_augmentation.npy',X_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 12)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make mean 0 and std dev 1 of the data.\n",
    "'''\n",
    "def normalize(X,mean,std):\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\" \n",
    "    X  = (X-mean)/std\n",
    "    return X\n",
    "\n",
    "#Use train stats for normalizing test set\n",
    "mean  = np.mean(X_train,0,keepdims=True)\n",
    "std   = np.std(X_train,0,keepdims=True)\n",
    "norm_X_train = normalize(X_train,mean,std)\n",
    "norm_X_test = normalize(X_test,mean,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Attribute $X_11$ vs Price $y$')"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploratory analysis of the data. Have a look at the distribution of prices vs features\n",
    "\n",
    "feature = 11\n",
    "plt.scatter(norm_X_train[:,feature], y_train)\n",
    "plt.xlabel(f\"Attribute $X_{feature}$\")\n",
    "plt.ylabel(\"Price $y$\")\n",
    "plt.title(f\"Attribute $X_{feature}$ vs Price $y$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Closed-form solution for linear regression\n",
    "\n",
    "We represent our output $\\mathbf{y} \\in R^{N\\times1}$ as linear combination of variables $\\mathbf{X} \\in R^{N\\times D}$. The objective of linear regression task is to find set of weights $\\mathbf{w} \\in R^{D\\times1}$ for predictor varibles which minimize our loss, simplest being $l_2$ loss function as shown below \n",
    "\\begin{align}\n",
    "L(\\mathbf{w}) &=\\frac{1}{N} \\| \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\|^2  \\\\\n",
    "\\nabla L(\\mathbf{w}) &= -\\frac{2}{N}\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) \n",
    "\\end{align}\n",
    "\n",
    "$\\nabla L(\\mathbf{w}) = 0$ for minimum condition, we get\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "This is called an [analytical solution](https://en.wikipedia.org/wiki/Linear_least_squares). \n",
    "Please use this solution to complete the function `get_w_analytical` and `get_loss`. Tip: before implementation think about the dimension of returned variable. \n",
    "\n",
    "**Q.** What is the time complexity of this approach?  \n",
    "**A.** Let N be the number of data points. The time complexity is determined by the slowest operation, i.e. the matrix inversion, which is in $\\mathcal{O}(N^3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_analytical(X_train,y_train):\n",
    "    \"\"\"\n",
    "    compute the weight parameters w\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "        \n",
    "    # compute w via the normal equation\n",
    "    # Tip: use np.linalg.solve instead of np.linalg.inv as it provides stable solution\n",
    "    w = np.linalg.solve(X_train.T@X_train,X_train.T@y_train)\n",
    "    return w\n",
    "\n",
    "def get_loss(w, X_train, y_train,X_test,y_test,val=False):\n",
    "    # predict dependent variables and MSE loss for seen training data\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    loss_train = (np.mean((y_train-X_train@w)**2))\n",
    "    loss_train_std = np.std((y_train-X_train@w)**2)\n",
    "    \n",
    "    # predict dependent variables and MSE loss for unseen test data\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    loss_test = (np.mean((y_test-X_test@w)**2))\n",
    "    loss_test_std = np.std((y_test-X_test@w)**2)\n",
    "    if not val:\n",
    "        print(\"The training loss is {} with std:{}. The test loss is {} with std:{}.\".format(loss_train, loss_train_std, loss_test,loss_test_std))\n",
    "    else:\n",
    "        print(\"The training loss is {} with std:{}. The val loss is {} with std:{}.\".format(loss_train, loss_train_std, loss_test,loss_test_std))\n",
    "\n",
    "    return loss_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 436.848744249655 with std:142.80383100887934. The test loss is 437.34987954093765 with std:148.91518644294177.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "437.34987954093765"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute w and calculate its goodness\n",
    "w_ana = get_w_analytical(norm_X_train,y_train)\n",
    "get_loss(w_ana, norm_X_train, y_train, norm_X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Adding $w_0$\n",
    "We add a bias term i.e. $w_0$ to our formulation such that $y_n = x_n * w_n + x_{n-1} * w_{n-1} + ... + x_1 * w_1 + 1 * w_0$ . This involves making our feature dimension from $D$ to $D+1$ and $\\mathbf{X} = [\\mathbf{1}  ~~~ \\mathbf{X} ]$, which corresponds to adding a column of ones.\n",
    "\n",
    "**Q.** How does this term help?  \n",
    "**A.** If we don't have bias term then for $y_n = x_n * w_n + x_{n-1} * w_{n-1} + ... + x_1 * w_1$ at origin $x_n=x_{n-1}=...=x_1=0$ output will be $y_n = 0$ irrespective of $w$'s. Hence our line will always pass through origin restricting the linear model to capture the data. Adding a bias term, helps providing non-zero solution at origin a.k.a intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 10.404507130540278 with std:18.876977408533154. The test loss is 11.790311742556584 with std:23.812240108124904.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.790311742556584"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a column of ones to X_train and X_test and see if loss values change.\n",
    "X_train_aug = np.hstack((np.ones((norm_X_train.shape[0],1)),norm_X_train))\n",
    "X_test_aug = np.hstack((np.ones((norm_X_test.shape[0],1)),norm_X_test))\n",
    "w_ana = get_w_analytical(X_train_aug,y_train)\n",
    "get_loss(w_ana, X_train_aug,y_train, X_test_aug,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Numerical solution for linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The linear regression model has an analytical solution, but we can also get the weight parameters $w$ numerically, e.g. via stochastic gradient descent. Please use this approach to complete the function `get_w_numerical` below.\n",
    "\n",
    "**Q.** How do these results compare against those of the analytical solution? Explain the differences or similarities!   \n",
    "**A.** The analytical solution and the numerical solution are almost identical. But when condition number of matrix is\n",
    "    very high inverse may not be stable and solution will differ.  \n",
    "**Q.** In which cases, it maybe be preferable to use the numerical approach over the analytical solution?  \n",
    "**A.** Let N be the number of data points. Computing the analytical solution has run time $\\mathcal{O}(N^3)$, which may be problematic if N is very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_numerical(X_train,y_train,X_test,y_test,epochs,lr):\n",
    "    \"\"\"compute the weight parameters w\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the weights\n",
    "#     w    = np.random.normal(0, 1e-1, X_train.shape[1])\n",
    "    w    = np.ones(X_train.shape[1])\n",
    "    # define the gradient\n",
    "    grad = lambda w,x,y: (2/X_train.shape[0])*(y-x@w)*x\n",
    "    \n",
    "    # iterate a given number of epochs over the training data\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # iterate over each data point\n",
    "        for idx,x_train in enumerate(X_train):\n",
    "            # update the weights\n",
    "            w += lr*grad(w,x_train,y_train[idx])\n",
    "            \n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {1000+epoch}/{epochs}\")\n",
    "            get_loss(w, X_train,y_train, X_test,y_test)\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/30000\n",
      "The training loss is 482.71762843717744 with std:384.5207328324778. The test loss is 574.9879011456944 with std:413.00560614556025.\n",
      "Epoch 2000/30000\n",
      "The training loss is 19.991665680796178 with std:39.099524424944214. The test loss is 23.275790613420526 with std:50.799788718231895.\n",
      "Epoch 3000/30000\n",
      "The training loss is 11.609675557921838 with std:24.73882862536724. The test loss is 14.056004746306613 with std:34.3258301821134.\n",
      "Epoch 4000/30000\n",
      "The training loss is 10.981895269948238 with std:21.871211171328415. The test loss is 13.079686961509223 with std:29.96148981554534.\n",
      "Epoch 5000/30000\n",
      "The training loss is 10.745876775435127 with std:20.74983758621468. The test loss is 12.629712106108414 with std:27.845578845201906.\n",
      "Epoch 6000/30000\n",
      "The training loss is 10.621591464716065 with std:20.152080314545348. The test loss is 12.369007927597666 with std:26.615340609150866.\n",
      "Epoch 7000/30000\n",
      "The training loss is 10.550023434714845 with std:19.786993899018093. The test loss is 12.207165439635403 with std:25.84146570738316.\n",
      "Epoch 8000/30000\n",
      "The training loss is 10.506133123294422 with std:19.548121772628207. The test loss is 12.100981890048235 with std:25.32930293806606.\n",
      "Epoch 9000/30000\n",
      "The training loss is 10.477715706109436 with std:19.3848308719112. The test loss is 12.027950661652987 with std:24.976817224088748.\n",
      "Epoch 10000/30000\n",
      "The training loss is 10.45844216930142 with std:19.269490861584085. The test loss is 11.975753852741311 with std:24.72628678634284.\n",
      "Epoch 11000/30000\n",
      "The training loss is 10.444870113125095 with std:19.185830718384057. The test loss is 11.937305071461575 with std:24.543269665100993.\n",
      "Epoch 12000/30000\n",
      "The training loss is 10.435035941314409 with std:19.123773300251926. The test loss is 11.908312512072543 with std:24.406327352784007.\n",
      "Epoch 13000/30000\n",
      "The training loss is 10.427761565536871 with std:19.076834911805122. The test loss is 11.886045314144805 with std:24.301647293210245.\n",
      "Epoch 14000/30000\n",
      "The training loss is 10.422302919836346 with std:19.040713732415444. The test loss is 11.868688049683222 with std:24.22007472177084.\n",
      "Epoch 15000/30000\n",
      "The training loss is 10.41816685297195 with std:19.012482039404237. The test loss is 11.8549892067684 with std:24.155397126682146.\n",
      "Epoch 16000/30000\n",
      "The training loss is 10.415012668102426 with std:18.990104123641228. The test loss is 11.84406082253797 with std:24.103312322892755.\n",
      "Epoch 17000/30000\n",
      "The training loss is 10.412597088627955 with std:18.972138211991307. The test loss is 11.835258635547753 with std:24.06078651653425.\n",
      "Epoch 18000/30000\n",
      "The training loss is 10.410742062521612 with std:18.95754650306946. The test loss is 11.828106985409248 with std:24.025643842559017.\n",
      "Epoch 19000/30000\n",
      "The training loss is 10.409314966432046 with std:18.945570949709147. The test loss is 11.822249762251799 with std:23.996298037605353.\n",
      "Epoch 20000/30000\n",
      "The training loss is 10.408215814720643 with std:18.935650253221485. The test loss is 11.81741724746037 with std:23.971573872656148.\n",
      "Epoch 21000/30000\n",
      "The training loss is 10.407368613692324 with std:18.927363378615325. The test loss is 11.813403077836256 with std:23.950586562452184.\n",
      "Epoch 22000/30000\n",
      "The training loss is 10.406715294072967 with std:18.92039052382461. The test loss is 11.810047903546167 with std:23.9326593000167.\n",
      "Epoch 23000/30000\n",
      "The training loss is 10.406211327196177 with std:18.914485794590046. The test loss is 11.807227607688628 with std:23.917266224329374.\n",
      "Epoch 24000/30000\n",
      "The training loss is 10.405822489539249 with std:18.909457854530075. The test loss is 11.804844708214114 with std:23.903992554962844.\n",
      "Epoch 25000/30000\n",
      "The training loss is 10.405522438867349 with std:18.90515608102573. The test loss is 11.802822019914833 with std:23.892506430747037.\n",
      "Epoch 26000/30000\n",
      "The training loss is 10.405290879829172 with std:18.901460564922317. The test loss is 11.801097943359249 with std:23.882538799691833.\n",
      "Epoch 27000/30000\n",
      "The training loss is 10.405112166194098 with std:18.898274819960296. The test loss is 11.799622937312398 with std:23.87386889432526.\n",
      "Epoch 28000/30000\n",
      "The training loss is 10.40497423101903 with std:18.89552041899471. The test loss is 11.798356859289461 with std:23.866313614204184.\n",
      "Epoch 29000/30000\n",
      "The training loss is 10.404867765409966 with std:18.893133011396838. The test loss is 11.797266947392247 with std:23.85971966516266.\n",
      "Epoch 30000/30000\n",
      "The training loss is 10.404785586912533 with std:18.891059338029052. The test loss is 11.79632627880252 with std:23.8539576612355.\n"
     ]
    }
   ],
   "source": [
    "# compute w and calculate its goodness\n",
    "\n",
    "w_num = get_w_numerical(X_train_aug,y_train,X_test_aug,y_test,30000,1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in previous section, we would like to do feature expansion to fit the non-linearity of the data, but it soon leads to overfitting. There are different ways to tackle this problem, like getting more data, changing the prediction method, regularization, etc. For the task of regression, we'll add a regularization to our training objective to mitigate this problem. Intutively, regularization restricts the domain from which the values of model parameters are taken, which means that we are biasing our model.  \n",
    "\n",
    "In Ridge Regression, we restrict the $l_2$ norm of the coefficients $\\mathbf{w}$. Our loss function looks as following,\n",
    "\\begin{align}\n",
    "L(\\mathbf{w}) &=\\frac{1}{N} \\| \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\|^2 + \\frac{\\lambda}{N}\\|\\mathbf{w}\\|^2 \\\\\n",
    "\\nabla L(\\mathbf{w}) &= -\\frac{2}{N}\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + 2\\frac{\\lambda}{N}\\mathbf{w}\n",
    "\\end{align}\n",
    "\n",
    "$\\nabla L(\\mathbf{w}) = 0$ for minimum condition, we get\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w} &= (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "dimensions are following: $\\mathbf{w}$ is $D\\times1$; $\\mathbf{y}$ is $N\\times1$; $\\mathbf{X}$ is $N\\times D$; $\\mathbf{I}$ is identity matrix of dimension $D \\times D$ .\n",
    "\n",
    "$\\lambda$ is our penality term, also know as weight decay. By varying its value, we can allow biasing in our model.\n",
    "\n",
    "**Q.**:\n",
    "When $\\lambda$ is high, our model is more complex or less?\n",
    "\n",
    "**A.**:\n",
    "High $\\lambda$ penalises the norm term more, hence restricts the value $\\mathbf{w}$ can have, rendering simpler model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_expanded = np.load('X_train_feat_augmentation.npy',)\n",
    "X_test_expanded = np.load('X_test_feat_augmentation.npy')\n",
    "\n",
    "mean  = np.mean(X_train_expanded,0,keepdims=True)\n",
    "std   = np.std(X_train_expanded,0,keepdims=True)\n",
    "norm_X_train = normalize(X_train_expanded,mean,std)\n",
    "norm_X_test = normalize(X_test_expanded,mean,std)\n",
    "\n",
    "X_train_aug = np.hstack((np.ones((norm_X_train.shape[0],1)),norm_X_train))\n",
    "X_test_aug = np.hstack((np.ones((norm_X_test.shape[0],1)),norm_X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_analytical_with_regularization(X_train,y_train,lmda):\n",
    "    \"\"\"compute the weight parameters w with ridge regression\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    #create lambda matrix \n",
    "    lmda_mat = lmda*np.eye(X_train.shape[1])\n",
    "    # compute w via the normal equation\n",
    "    # np.linalg.solve is more stable than np.linalg.inv\n",
    "    w = np.linalg.solve(X_train.T@X_train+lmda_mat,X_train.T@y_train)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 4.303778691331962 with std:9.095607582173066. The val loss is 14.87699794939581 with std:59.988802632877295.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.87699794939581"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_reg = get_w_analytical_with_regularization(X_train_aug,y_train,lmda=0)\n",
    "get_loss(w_reg,X_train_aug,y_train,X_test_aug,y_test,val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation(CV) is used to choose value of $\\lambda$. As seen in previous exercise, we will use K-fold CV.\n",
    "We will use our training set and create K splits of it to choose best $\\lambda$ and finally evaluate on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for using kth split as validation set to get accuracy\n",
    "# and k-1 splits to train our model\n",
    "def do_cross_validation_reg(k,k_fold_ind,X,Y,lmda=0):\n",
    "    \n",
    "    # use one split as val\n",
    "    val_ind = k_fold_ind[k]\n",
    "    # use k-1 split to train\n",
    "    train_splits = [i for i in range(k_fold_ind.shape[0]) if i is not k]\n",
    "    train_ind = k_fold_ind[train_splits,:].reshape(-1)\n",
    "   \n",
    "    #Get train and val \n",
    "    cv_X_train = X[train_ind,:]\n",
    "    cv_Y_train = Y[train_ind]\n",
    "    cv_X_val = X[val_ind,:]\n",
    "    cv_Y_val = Y[val_ind]\n",
    "    \n",
    "    #fit on train set using regularised version\n",
    "    w = get_w_analytical_with_regularization(cv_X_train,cv_Y_train,lmda)\n",
    "    \n",
    "    #get loss for val\n",
    "    loss_test = get_loss(w,cv_X_train,cv_Y_train,cv_X_val,cv_Y_val,val=True)\n",
    "    print(loss_test,lmda)\n",
    "    return loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do 3-fold CV. We will use same the training data splits as in non regularised case for fairer comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for {'lmda': 0.001} ...\n",
      "The training loss is 5.230617750617628 with std:11.88632764411538. The val loss is 4256.219376411774 with std:40519.43827905973.\n",
      "4256.219376411774 0.001\n",
      "The training loss is 5.564024419074517 with std:10.489469427417578. The val loss is 8.813365803091063 with std:19.809738948171933.\n",
      "8.813365803091063 0.001\n",
      "The training loss is 5.745621334225853 with std:12.35326660025177. The val loss is 7.3228460914852835 with std:13.130455359635928.\n",
      "7.3228460914852835 0.001\n",
      "The training loss is 4.914363776717929 with std:7.755470455999726. The val loss is 10.813827142550055 with std:30.140090108304477.\n",
      "10.813827142550055 0.001\n",
      "Evaluating for {'lmda': 0.0010092621909870473} ...\n",
      "The training loss is 5.232594741247836 with std:11.891238080873922. The val loss is 4252.591310726975 with std:40485.005679220674.\n",
      "4252.591310726975 0.0010092621909870473\n",
      "The training loss is 5.56677995622836 with std:10.49285272627899. The val loss is 8.822582145124159 with std:19.86335537952905.\n",
      "8.822582145124159 0.0010092621909870473\n",
      "The training loss is 5.747906489585033 with std:12.357442814925708. The val loss is 7.324734302252546 with std:13.125513511101223.\n",
      "7.324734302252546 0.0010092621909870473\n",
      "The training loss is 4.916563986708466 with std:7.757695498931347. The val loss is 10.814957941445693 with std:30.142194496762013.\n",
      "10.814957941445693 0.0010092621909870473\n",
      "Evaluating for {'lmda': 0.0010186101701559753} ...\n",
      "The training loss is 5.234576581626301 with std:11.896156715971472. The val loss is 4248.798636723362 with std:40448.99933629907.\n",
      "4248.798636723362 0.0010186101701559753\n",
      "The training loss is 5.569533359826828 with std:10.496229037728845. The val loss is 8.831844809783778 with std:19.917485394949683.\n",
      "8.831844809783778 0.0010186101701559753\n",
      "The training loss is 5.750193139701481 with std:12.361618532450866. The val loss is 7.3266194186643325 with std:13.120499300214101.\n",
      "7.3266194186643325 0.0010186101701559753\n",
      "The training loss is 4.9187647903694165 with std:7.759917817675592. The val loss is 10.81608829926031 with std:30.144311915522348.\n",
      "10.81608829926031 0.0010186101701559753\n",
      "Evaluating for {'lmda': 0.0010280447320933097} ...\n",
      "The training loss is 5.236563251530426 with std:11.901083372091799. The val loss is 4244.841877174555 with std:40411.42426562014.\n",
      "4244.841877174555 0.0010280447320933097\n",
      "The training loss is 5.572284547595806 with std:10.499598270383911. The val loss is 8.841153359889523 with std:19.972127646739757.\n",
      "8.841153359889523 0.0010280447320933097\n",
      "The training loss is 5.752481232729325 with std:12.36579365286607. The val loss is 7.328501485846577 with std:13.115413710514753.\n",
      "7.328501485846577 0.0010280447320933097\n",
      "The training loss is 4.920966165292231 with std:7.7621374179823475. The val loss is 10.817218169480977 with std:30.146442313303265.\n",
      "10.817218169480977 0.0010280447320933097\n",
      "Evaluating for {'lmda': 0.0010375666787451859} ...\n",
      "The training loss is 5.23855473051108 with std:11.906017870437212. The val loss is 4240.7215638089165 with std:40372.285568055486.\n",
      "4240.7215638089165 0.0010375666787451859\n",
      "The training loss is 5.57503343769015 with std:10.502960333616265. The val loss is 8.850507352947687 with std:20.027280680951286.\n",
      "8.850507352947687 0.0010375666787451859\n",
      "The training loss is 5.754770716953199 with std:12.369968076128217. The val loss is 7.330380549148276 with std:13.11025773450157.\n",
      "7.330380549148276 0.0010375666787451859\n",
      "The training loss is 4.923168088963981 with std:7.764354305902039. The val loss is 10.818347504365239 with std:30.148585629549608.\n",
      "10.818347504365239 0.0010375666787451859\n",
      "Evaluating for {'lmda': 0.0010471768194855202} ...\n",
      "The training loss is 5.2405509979193905 with std:11.910960030786548. The val loss is 4236.43823779381 with std:40331.588434652316.\n",
      "4236.43823779381 0.0010471768194855202\n",
      "The training loss is 5.577779948703194 with std:10.506315137405169. The val loss is 8.859906338970498 with std:20.0829429260893.\n",
      "8.859906338970498 0.0010471768194855202\n",
      "The training loss is 5.757061540871577 with std:12.374141702328656. The val loss is 7.3322566535370735 with std:13.105032369057932.\n",
      "7.3322566535370735 0.0010471768194855202\n",
      "The training loss is 4.925370538741462 with std:7.7665684876773255. The val loss is 10.819476254575978 with std:30.150741793657236.\n",
      "10.819476254575978 0.0010471768194855202\n",
      "Evaluating for {'lmda': 0.0010568759711848039} ...\n",
      "The training loss is 5.24255203283402 with std:11.915909671301248. The val loss is 4231.992449580213 with std:40289.33814514383.\n",
      "4231.992449580213 0.0010568759711848039\n",
      "The training loss is 5.580523999670881 with std:10.509662592484142. The val loss is 8.869349863139673 with std:20.139112707148048.\n",
      "8.869349863139673 0.0010568759711848039\n",
      "The training loss is 5.7593536530587235 with std:12.37831443137539. The val loss is 7.334129844051114 with std:13.099738619516762.\n",
      "7.334129844051114 0.0010568759711848039\n",
      "The training loss is 4.927573491770979 with std:7.7687799696021775. The val loss is 10.820604369809407 with std:30.15291072696233.\n",
      "10.820604369809407 0.0010568759711848039\n",
      "Evaluating for {'lmda': 0.0010666649582795388} ...\n",
      "The training loss is 5.244557814106148 with std:11.920866608646815. The val loss is 4227.384758788924 with std:40245.54006684096.\n",
      "4227.384758788924 0.0010666649582795388\n",
      "The training loss is 5.583265510044849 with std:10.513002610225598. The val loss is 8.8788374640944 with std:20.195788236559334.\n",
      "8.8788374640944 0.0010666649582795388\n",
      "The training loss is 5.761647002284214 with std:12.38248616323563. The val loss is 7.336000165506099 with std:13.094377496215197.\n",
      "7.336000165506099 0.0010666649582795388\n",
      "The training loss is 4.929776925139812 with std:7.770988758326872. The val loss is 10.821731798010928 with std:30.15509234036124.\n",
      "10.821731798010928 0.0010666649582795388\n",
      "Evaluating for {'lmda': 0.001076544612842316} ...\n",
      "The training loss is 5.246568320396971 with std:11.925830658263529. The val loss is 4222.615734476929 with std:40200.19965718886.\n",
      "4222.615734476929 0.001076544612842316\n",
      "The training loss is 5.586004399807032 with std:10.516335102790903. The val loss is 8.888368674892968 with std:20.252967618843872.\n",
      "8.888368674892968 0.001076544612842316\n",
      "The training loss is 5.763941537468622 with std:12.386656797881766. The val loss is 7.337867662600699 with std:13.088950016277641.\n",
      "7.337867662600699 0.001076544612842316\n",
      "The training loss is 4.931980815710955 with std:7.773194860541136. The val loss is 10.822858485996917 with std:30.157286536090634.\n",
      "10.822858485996917 0.001076544612842316\n",
      "Evaluating for {'lmda': 0.0010865157746525384} ...\n",
      "The training loss is 5.248583530085404 with std:11.93080163386467. The val loss is 4217.685955024127 with std:40153.32246266123.\n",
      "4217.685955024127 0.0010865157746525384\n",
      "The training loss is 5.588740589352109 with std:10.519659982988623. The val loss is 8.897943022927059 with std:20.31064885104624.\n",
      "8.897943022927059 0.0010865157746525384\n",
      "The training loss is 5.766237207672534 with std:12.390826235179846. The val loss is 7.339732379814801 with std:13.083457202047763.\n",
      "7.339732379814801 0.0010865157746525384\n",
      "The training loss is 4.934185140232783 with std:7.7753982831427635. The val loss is 10.823984378967555 with std:30.159493206211415.\n",
      "10.823984378967555 0.0010865157746525384\n",
      "Evaluating for {'lmda': 0.0010965792912678099} ...\n",
      "The training loss is 5.250603421326536 with std:11.935779347942212. The val loss is 4212.59600791358 with std:40104.91411666287.\n",
      "4212.59600791358 0.0010965792912678099\n",
      "The training loss is 5.591473999544142 with std:10.522977164475805. The val loss is 8.907560029693638 with std:20.368829821020746.\n",
      "8.907560029693638 0.0010965792912678099\n",
      "The training loss is 5.768533962090333 with std:12.394994375095887. The val loss is 7.3415943614276165 with std:13.077900081691595.\n",
      "7.3415943614276165 0.0010965792912678099\n",
      "The training loss is 4.936389875209655 with std:7.777599033069193. The val loss is 10.825109420659734 with std:30.16171223304283.\n",
      "10.825109420659734 0.0010965792912678099\n",
      "Evaluating for {'lmda': 0.0011067360180959734} ...\n",
      "The training loss is 5.252627972040681 with std:11.940763611527748. The val loss is 4207.3464903127815 with std:40054.98034507443.\n",
      "4207.3464903127815 0.0011067360180959734\n",
      "The training loss is 5.594204551774826 with std:10.526286561543474. The val loss is 8.917219211635121 with std:20.427508312153993.\n",
      "8.917219211635121 0.0011067360180959734\n",
      "The training loss is 5.770831750100777 with std:12.399161117559727. The val loss is 7.343453651551104 with std:13.072279688612827.\n",
      "7.343453651551104 0.0011067360180959734\n",
      "The training loss is 4.938594997050743 with std:7.779797117488785. The val loss is 10.826233553401485 with std:30.163943489568595.\n",
      "10.826233553401485 0.0011067360180959734\n",
      "Evaluating for {'lmda': 0.0011169868184678225} ...\n",
      "The training loss is 5.2546571599049825 with std:11.94575423424987. The val loss is 4201.9380084800305 with std:40003.52696058156.\n",
      "4201.9380084800305 0.0011169868184678225\n",
      "The training loss is 5.596932167860275 with std:10.529588089257981. The val loss is 8.926920079417311 with std:20.48668199998117.\n",
      "8.926920079417311 0.0011169868184678225\n",
      "The training loss is 5.7731305211874275 with std:12.403326362431274. The val loss is 7.345310293983136 with std:13.066597061263748.\n",
      "7.345310293983136 0.0011169868184678225\n",
      "The training loss is 4.940800481933632 with std:7.781992543621454. The val loss is 10.827356717793004 with std:30.16618683788102.\n",
      "10.827356717793004 0.0011169868184678225\n",
      "Evaluating for {'lmda': 0.0011273325637104871} ...\n",
      "The training loss is 5.256690962361936 with std:11.950751024481992. The val loss is 4196.371178221665 with std:39950.55986702068.\n",
      "4196.371178221665 0.0011273325637104871\n",
      "The training loss is 5.599656770154343 with std:10.532881663542343. The val loss is 8.936662138701221 with std:20.546348456448783.\n",
      "8.936662138701221 0.0011273325637104871\n",
      "The training loss is 5.775430225008701 with std:12.407490009657513. The val loss is 7.347164332236265 with std:13.060853242462805.\n",
      "7.347164332236265 0.0011273325637104871\n",
      "The training loss is 4.943006305828555 with std:7.784185318729815. The val loss is 10.828478852934984 with std:30.168442130552442.\n",
      "10.828478852934984 0.0011273325637104871\n",
      "Evaluating for {'lmda': 0.0011377741332214914} ...\n",
      "The training loss is 5.2587293565947775 with std:11.955753789148908. The val loss is 4190.646624464636 with std:39896.08505529723.\n",
      "4190.646624464636 0.0011377741332214914\n",
      "The training loss is 5.602378281498471 with std:10.536167200937. The val loss is 8.946444889701583 with std:20.606505147561.\n",
      "8.946444889701583 0.0011377741332214914\n",
      "The training loss is 5.777730811321163 with std:12.41165195917471. The val loss is 7.3490158095929905 with std:13.055049280147694.\n",
      "7.3490158095929905 0.0011377741332214914\n",
      "The training loss is 4.945212444519868 with std:7.786375450189065. The val loss is 10.829599896450947 with std:30.170709210159735.\n",
      "10.829599896450947 0.0011377741332214914\n",
      "Evaluating for {'lmda': 0.0011483124145435111} ...\n",
      "The training loss is 5.260772319581209 with std:11.960762334057963. The val loss is 4184.764981800618 with std:39840.10860858774.\n",
      "4184.764981800618 0.0011483124145435111\n",
      "The training loss is 5.605096625242428 with std:10.539444618880376. The val loss is 8.956267827916848 with std:20.667149438541582.\n",
      "8.956267827916848 0.0011483124145435111\n",
      "The training loss is 5.780032230089226 with std:12.415812110902412. The val loss is 7.350864768954938 with std:13.049186225686594.\n",
      "7.350864768954938 0.0011483124145435111\n",
      "The training loss is 4.947418873616371 with std:7.788562945489134. The val loss is 10.8307197839552 with std:30.1729879077821.\n",
      "10.8307197839552 0.0011483124145435111\n",
      "Evaluating for {'lmda': 0.0011589483034398105} ...\n",
      "The training loss is 5.262819827996551 with std:11.965776463408321. The val loss is 4178.72689416094 with std:39782.63669921355.\n",
      "4178.72689416094 0.0011589483034398105\n",
      "The training loss is 5.607811725260027 with std:10.542713835602827. The val loss is 8.96613044367936 with std:20.72827859053294.\n",
      "8.96613044367936 0.0011589483034398105\n",
      "The training loss is 5.782334431357653 with std:12.419970364800442. The val loss is 7.352711252877218 with std:13.04326513503992.\n",
      "7.352711252877218 0.0011589483034398105\n",
      "The training loss is 4.949625568453183 with std:7.79074781208349. The val loss is 10.831838449819214 with std:30.17527804554138.\n",
      "10.831838449819214 0.0011589483034398105\n",
      "Evaluating for {'lmda': 0.0011696827039703846} ...\n",
      "The training loss is 5.264871858310787 with std:11.970795980503178. The val loss is 4172.533014870412 with std:39723.67558915509.\n",
      "4172.533014870412 0.0011696827039703846\n",
      "The training loss is 5.610523505912838 with std:10.545974770053823. The val loss is 8.976032222500846 with std:20.789889763981694.\n",
      "8.976032222500846 0.0011696827039703846\n",
      "The training loss is 5.784637365344352 with std:12.424126620844044. The val loss is 7.35455530362271 with std:13.03728706777448.\n",
      "7.35455530362271 0.0011696827039703846\n",
      "The training loss is 4.951832504178605 with std:7.792930057485223. The val loss is 10.832955826277342 with std:30.177579433652785.\n",
      "10.832955826277342 0.0011696827039703846\n",
      "Evaluating for {'lmda': 0.0011805165285688056} ...\n",
      "The training loss is 5.266928386722495 with std:11.975820687083182. The val loss is 4166.184006733133 with std:39663.23163086628.\n",
      "4166.184006733133 0.0011805165285688056\n",
      "The training loss is 5.613231892137006 with std:10.549227342075417. The val loss is 8.985972645408872 with std:20.851980020544964.\n",
      "8.985972645408872 0.0011805165285688056\n",
      "The training loss is 5.786940982385342 with std:12.428280779046569. The val loss is 7.356396962996221 with std:13.031253086505467.\n",
      "7.356396962996221 0.0011805165285688056\n",
      "The training loss is 4.954039655728361 with std:7.7951096892612455. The val loss is 10.834071844215735 with std:30.179891873035128.\n",
      "10.834071844215735 0.0011805165285688056\n",
      "Evaluating for {'lmda': 0.001191450698119776} ...\n",
      "The training loss is 5.268989389199094 with std:11.98085038397184. The val loss is 4159.680542032911 with std:39601.31126726883.\n",
      "4159.680542032911 0.001191450698119776\n",
      "The training loss is 5.6159368093658 with std:10.552471472292604. The val loss is 8.995951188636027 with std:20.914546321793775.\n",
      "8.995951188636027 0.001191450698119776\n",
      "The training loss is 5.789245232929384 with std:12.432432739399687. The val loss is 7.3582362723708865 with std:13.025164257151843.\n",
      "7.3582362723708865 0.001191450698119776\n",
      "The training loss is 4.956246997771559 with std:7.797286714946754. The val loss is 10.835186432360148 with std:30.182215152279788.\n",
      "10.835186432360148 0.001191450698119776\n",
      "Evaluating for {'lmda': 0.0012024861420374122} ...\n",
      "The training loss is 5.271054841433756 with std:11.985884870500712. The val loss is 4153.023302550885 with std:39537.921031924234.\n",
      "4153.023302550885 0.0012024861420374122\n",
      "The training loss is 5.618638183583743 with std:10.555707082144378. The val loss is 9.005967324014714 with std:20.977585532237356.\n",
      "9.005967324014714 0.0012024861420374122\n",
      "The training loss is 5.79155006760973 with std:12.436582401973455. The val loss is 7.360073272773121 with std:13.019021648768922.\n",
      "7.360073272773121 0.0012024861420374122\n",
      "The training loss is 4.958454504758607 with std:7.7994611420734214. The val loss is 10.836299517990106 with std:30.184549050434047.\n",
      "10.836299517990106 0.0012024861420374122\n",
      "Evaluating for {'lmda': 0.0012136237983442406} ...\n",
      "The training loss is 5.2731247188593535 with std:11.990923945022772. The val loss is 4146.212979428491 with std:39473.06754770207.\n",
      "4146.212979428491 0.0012136237983442406\n",
      "The training loss is 5.621335941320646 with std:10.55893409385297. The val loss is 9.016020519063378 with std:21.041094419922082.\n",
      "9.016020519063378 0.0012136237983442406\n",
      "The training loss is 5.793855437121704 with std:12.440729666791762. The val loss is 7.361908004648665 with std:13.012826332580113.\n",
      "7.361908004648665 0.0012136237983442406\n",
      "The training loss is 4.960662150872683 with std:7.801632978158896. The val loss is 10.837411026236135 with std:30.186893334679276.\n",
      "10.837411026236135 0.0012136237983442406\n",
      "Evaluating for {'lmda': 0.0012248646137509307} ...\n",
      "The training loss is 5.27519899668577 with std:11.995967404658554. The val loss is 4139.250273566929 with std:39406.75753061134.\n",
      "4139.250273566929 0.0012248646137509307\n",
      "The training loss is 5.624030009646014 with std:10.562152430560465. The val loss is 9.026110237198022 with std:21.10506965871751.\n",
      "9.026110237198022 0.0012248646137509307\n",
      "The training loss is 5.796161292354397 with std:12.444874434092457. The val loss is 7.363740508120941 with std:13.006579382811275.\n",
      "7.363740508120941 0.0012248646137509307\n",
      "The training loss is 4.962869910077647 with std:7.803802230699482. The val loss is 10.838520880673006 with std:30.189247761920637.\n",
      "10.838520880673006 0.0012248646137509307\n",
      "Evaluating for {'lmda': 0.0012362095437367692} ...\n",
      "The training loss is 5.2772776498205864 with std:12.001015045389453. The val loss is 4132.135895212506 with std:39338.99778581594.\n",
      "4132.135895212506 0.0012362095437367692\n",
      "The training loss is 5.626720316205902 with std:10.565362016227079. The val loss is 9.036235937479665 with std:21.16950782756374.\n",
      "9.036235937479665 0.0012362095437367692\n",
      "The training loss is 5.798467584258183 with std:12.449016603907788. The val loss is 7.36557082262642 with std:13.000281874816068.\n",
      "7.36557082262642 0.0012362095437367692\n",
      "The training loss is 4.965077756074045 with std:7.805968907138089. The val loss is 10.839629002681173 with std:30.191612077121228.\n",
      "10.839629002681173 0.0012362095437367692\n",
      "Evaluating for {'lmda': 0.0012476595526308698} ...\n",
      "The training loss is 5.279360652951611 with std:12.006066662154419. The val loss is 4124.870564322876 with std:39269.795211137636.\n",
      "4124.870564322876 0.0012476595526308698\n",
      "The training loss is 5.629406789191754 with std:10.568562775663178. The val loss is 9.046397075188338 with std:21.234405413834402.\n",
      "9.046397075188338 0.0012476595526308698\n",
      "The training loss is 5.80077426393756 with std:12.453156076463367. The val loss is 7.367398987272784 with std:12.993934887005612.\n",
      "7.367398987272784 0.0012476595526308698\n",
      "The training loss is 4.967285662278971 with std:7.808133014842514. The val loss is 10.840735311925746 with std:30.193986014431562.\n",
      "10.840735311925746 0.0012476595526308698\n",
      "Evaluating for {'lmda': 0.001259215613694151} ...\n",
      "The training loss is 5.281447980457699 with std:12.011122048698466. The val loss is 4117.455010301703 with std:39199.15679451677.\n",
      "4117.455010301703 0.001259215613694151\n",
      "The training loss is 5.632089357355454 with std:10.571754634462538. The val loss is 9.056593101842797 with std:21.29975881473726.\n",
      "9.056593101842797 0.001259215613694151\n",
      "The training loss is 5.803081282609829 with std:12.457292752029417. The val loss is 7.369225040411989 with std:12.987539497888891.\n",
      "7.369225040411989 0.001259215613694151\n",
      "The training loss is 4.969493601868676 with std:7.810294561117757. The val loss is 10.841839726168205 with std:30.196369297121823.\n",
      "10.841839726168205 0.001259215613694151\n",
      "Evaluating for {'lmda': 0.0012708787092020582} ...\n",
      "The training loss is 5.283539606483397 with std:12.016180997771414. The val loss is 4109.889972124503 with std:39127.089615201214.\n",
      "4109.889972124503 0.0012708787092020582\n",
      "The training loss is 5.6347679500371175 with std:10.574937519166628. The val loss is 9.06682346511038 with std:21.365564337474137.\n",
      "9.06682346511038 0.0012708787092020582\n",
      "The training loss is 5.805388591593594 with std:12.46142653080586. The val loss is 7.3710490199798615 with std:12.98109678854416.\n",
      "7.3710490199798615 0.0012708787092020582\n",
      "The training loss is 4.971701547723387 with std:7.81245355317919. The val loss is 10.842942160889178 with std:30.198761635727298.\n",
      "10.842942160889178 0.0012708787092020582\n",
      "Evaluating for {'lmda': 0.0012826498305280598} ...\n",
      "The training loss is 5.285635504887491 with std:12.02124330110367. The val loss is 4102.176198618578 with std:39053.60084642797.\n",
      "4102.176198618578 0.0012826498305280598\n",
      "The training loss is 5.637442497122813 with std:10.578111357158221. The val loss is 9.07708760911937 with std:21.43181820144994.\n",
      "9.07708760911937 0.0012826498305280598\n",
      "The training loss is 5.807696142336685 with std:12.46555731317888. The val loss is 7.3728709632331295 with std:12.97460783994766.\n",
      "7.3728709632331295 0.0012826498305280598\n",
      "The training loss is 4.973909472456328 with std:7.814609998135169. The val loss is 10.844042530052079 with std:30.201162730976225.\n",
      "10.844042530052079 0.0012826498305280598\n",
      "Evaluating for {'lmda': 0.001294529978227916} ...\n",
      "The training loss is 5.287735649296103 with std:12.026308749354186. The val loss is 4094.3144479862317 with std:38978.69775084839.\n",
      "4094.3144479862317 0.001294529978227916\n",
      "The training loss is 5.6401129291370475 with std:10.581276076705329. The val loss is 9.087384974555611 with std:21.498516540314945.\n",
      "9.087384974555611 0.001294529978227916\n",
      "The training loss is 5.810003886392847 with std:12.469684999503105. The val loss is 7.374690906873718 with std:12.968073734499866.\n",
      "7.374690906873718 0.001294529978227916\n",
      "The training loss is 4.9761173484277785 with std:7.816763903009123. The val loss is 10.845140745173419 with std:30.203572270654014.\n",
      "10.845140745173419 0.001294529978227916\n",
      "Evaluating for {'lmda': 0.0013065201621247212} ...\n",
      "The training loss is 5.289840013018933 with std:12.031377132253317. The val loss is 4086.3054882030397 with std:38902.38768433673.\n",
      "4086.3054882030397 0.0013065201621247212\n",
      "The training loss is 5.642779177112817 with std:10.584431606838562. The val loss is 9.097714998948046 with std:21.565655403332908.\n",
      "9.097714998948046 0.0013065201621247212\n",
      "The training loss is 5.812311775415502 with std:12.473809490194178. The val loss is 7.376508886887133 with std:12.9614955547675.\n",
      "7.376508886887133 0.0013065201621247212\n",
      "The training loss is 4.978325147677333 with std:7.818915274683777. The val loss is 10.846236716048413 with std:30.20598993205659.\n",
      "10.846236716048413 0.0013065201621247212\n",
      "Evaluating for {'lmda': 0.0013186214013947485} ...\n",
      "The training loss is 5.291948569083145 with std:12.036448238411491. The val loss is 4078.1500968757796 with std:38824.6780946368.\n",
      "4078.1500968757796 0.0013186214013947485\n",
      "The training loss is 5.645441172732476 with std:10.587577877649508. The val loss is 9.10807711634481 with std:21.633230755777838.\n",
      "9.10807711634481 0.0013186214013947485\n",
      "The training loss is 5.8146197611480925 with std:12.477930685748616. The val loss is 7.378324938694536 with std:12.954874383656913.\n",
      "7.378324938694536 0.0013186214013947485\n",
      "The training loss is 4.980532841948145 with std:7.821064119875823. The val loss is 10.847330350080577 with std:30.20841537946019.\n",
      "10.847330350080577 0.0013186214013947485\n",
      "Evaluating for {'lmda': 0.0013308347246540747} ...\n",
      "The training loss is 5.294061290298299 with std:12.041521855667792. The val loss is 4069.8490612592727 with std:38745.57652149609.\n",
      "4069.8490612592727 0.0013308347246540747\n",
      "The training loss is 5.6480988482608465 with std:10.590714820014828. The val loss is 9.118470757971899 with std:21.701238482777352.\n",
      "9.118470757971899 0.0013308347246540747\n",
      "The training loss is 5.816927795462822 with std:12.48204848669816. The val loss is 7.380139096971147 with std:12.948211303735611.\n",
      "7.380139096971147 0.0013308347246540747\n",
      "The training loss is 4.9827404027552085 with std:7.823210445221675. The val loss is 10.848421552906249 with std:30.210848266703827.\n",
      "10.848421552906249 0.0013308347246540747\n",
      "Evaluating for {'lmda': 0.0013431611700460153} ...\n",
      "The training loss is 5.296178149153638 with std:12.046597770859647. The val loss is 4061.4031784541826 with std:38665.090598569346.\n",
      "4061.4031784541826 0.0013431611700460153\n",
      "The training loss is 5.650752136542095 with std:10.593842365661438. The val loss is 9.128895351999573 with std:21.769674388665397.\n",
      "9.128895351999573 0.0013431611700460153\n",
      "The training loss is 5.819235830329031 with std:12.486162793635298. The val loss is 7.381951395675673 with std:12.941507397111536.\n",
      "7.381951395675673 0.0013431611700460153\n",
      "The training loss is 4.984947801251665 with std:7.825354257122008. The val loss is 10.849510227917342 with std:30.21328823522793.\n",
      "10.849510227917342 0.0013431611700460153\n",
      "Evaluating for {'lmda': 0.001355601785329369} ...\n",
      "The training loss is 5.298299117858144 with std:12.051675769911059. The val loss is 4052.8132551302556 with std:38583.22805076708.\n",
      "4052.8132551302556 0.001355601785329369\n",
      "The training loss is 5.653400971021152 with std:10.596960447264681. The val loss is 9.139350324040215 with std:21.838534201635213.\n",
      "9.139350324040215 0.001355601785329369\n",
      "The training loss is 5.821543817783075 with std:12.490273507228453. The val loss is 7.383761868115565 with std:12.934763745771066.\n",
      "7.383761868115565 0.001355601785329369\n",
      "The training loss is 4.987155008316787 with std:7.827495561867288. The val loss is 10.850596276389732 with std:30.215734914571737.\n",
      "10.850596276389732 0.001355601785329369\n",
      "Evaluating for {'lmda': 0.001368157627967472} ...\n",
      "The training loss is 5.300424168335236 with std:12.056755637855996. The val loss is 4044.0801076827956 with std:38499.99669573693.\n",
      "4044.0801076827956 0.001368157627967472\n",
      "The training loss is 5.656045285763889 with std:10.600068998388927. The val loss is 9.149835096831609 with std:21.90781357224455.\n",
      "9.149835096831609 0.001368157627967472\n",
      "The training loss is 5.823851709954483 with std:12.494380528166486. The val loss is 7.385570546750659 with std:12.927981430136144.\n",
      "7.385570546750659 0.001368157627967472\n",
      "The training loss is 4.989361994517825 with std:7.82963436550925. The val loss is 10.851679597368697 with std:30.218187922093787.\n",
      "10.851679597368697 0.001368157627967472\n",
      "Evaluating for {'lmda': 0.0013808297652180923} ...\n",
      "The training loss is 5.302553272249078 with std:12.061837158905899. The val loss is 4035.2045624083103 with std:38415.4044455486.\n",
      "4035.2045624083103 0.0013808297652180923\n",
      "The training loss is 5.658685015444511 with std:10.603167953471424. The val loss is 9.16034909062366 with std:21.977508076875523.\n",
      "9.16034909062366 0.0013808297652180923\n",
      "The training loss is 5.826159459091515 with std:12.498483757250296. The val loss is 7.3873774633834906 with std:12.921161529826874.\n",
      "7.3873774633834906 0.0013808297652180923\n",
      "The training loss is 4.991568730124173 with std:7.831770673949505. The val loss is 10.852760087937096 with std:30.22064686382755.\n",
      "10.852760087937096 0.0013808297652180923\n",
      "Evaluating for {'lmda': 0.0013936192742241421} ...\n",
      "The training loss is 5.304686400945236 with std:12.066920116426742. The val loss is 4026.1874551340716 with std:38329.45930314136.\n",
      "4026.1874551340716 0.0013936192742241421\n",
      "The training loss is 5.661320095355438 with std:10.606257247893172. The val loss is 9.17089172334678 with std:22.04761321974137.\n",
      "9.17089172334678 0.0013936192742241421\n",
      "The training loss is 5.828467017511641 with std:12.502583095356467. The val loss is 7.389182648933841 with std:12.914305122756005.\n",
      "7.389182648933841 0.0013936192742241421\n",
      "The training loss is 4.993775185107966 with std:7.833904492855686. The val loss is 10.85383764276767 with std:30.22311133305029.\n",
      "10.85383764276767 0.0013936192742241421\n",
      "Evaluating for {'lmda': 0.0014065272421052363} ...\n",
      "The training loss is 5.306823525526063 with std:12.072004293063959. The val loss is 4017.0296317267134 with std:38242.16936718641.\n",
      "4017.0296317267134 0.0014065272421052363\n",
      "The training loss is 5.663950461384218 with std:10.609336817844202. The val loss is 9.18146241069832 with std:22.11812443416918.\n",
      "9.18146241069832 0.0014065272421052363\n",
      "The training loss is 5.830774337607925 with std:12.506678443376527. The val loss is 7.3909861335941995 with std:12.907413285374163.\n",
      "7.3909861335941995 0.0014065272421052363\n",
      "The training loss is 4.995981329112118 with std:7.836035827685425. The val loss is 10.854912154581848 with std:30.225580911845555.\n",
      "10.854912154581848 0.0014065272421052363\n",
      "Evaluating for {'lmda': 0.0014195547660501016} ...\n",
      "The training loss is 5.308964616731385 with std:12.077089470494812. The val loss is 4007.731947583425 with std:38153.54282721983.\n",
      "4007.731947583425 0.0014195547660501016\n",
      "The training loss is 5.666576050047325 with std:10.612406600500744. The val loss is 9.192060566342093 with std:22.18903708548932.\n",
      "9.192060566342093 0.0014195547660501016\n",
      "The training loss is 5.833081371853205 with std:12.510769702302543. The val loss is 7.392787946691312 with std:12.900487091994762.\n",
      "7.392787946691312 0.0014195547660501016\n",
      "The training loss is 4.998187131479206 with std:7.838164683662258. The val loss is 10.855983513607672 with std:30.22805516917636.\n",
      "10.855983513607672 0.0014195547660501016\n",
      "Evaluating for {'lmda': 0.001432702953409831} ...\n",
      "The training loss is 5.31110964507457 with std:12.082175429789402. The val loss is 3998.2952680311246 with std:38063.58796744504.\n",
      "3998.2952680311246 0.001432702953409831\n",
      "The training loss is 5.6691967984990885 with std:10.615466533959792. The val loss is 9.202685601800168 with std:22.260346470504537.\n",
      "9.202685601800168 0.001432702953409831\n",
      "The training loss is 5.835388072792423 with std:12.514856773178028. The val loss is 7.3945881166259175 with std:12.893527614260233.\n",
      "7.3945881166259175 0.001432702953409831\n",
      "The training loss is 5.000392561240085 with std:7.840291065792253. The val loss is 10.857051608111561 with std:30.230533662796056.\n",
      "10.857051608111561 0.001432702953409831\n",
      "Evaluating for {'lmda': 0.0014459729217920197} ...\n",
      "The training loss is 5.3132585807402375 with std:12.087261951230515. The val loss is 3988.7204682387805 with std:37972.31316590676.\n",
      "3988.7204682387805 0.0014459729217920197\n",
      "The training loss is 5.67181264448268 with std:10.618516557131583. The val loss is 9.213336926903722 with std:22.332047822103597.\n",
      "9.213336926903722 0.0014459729217920197\n",
      "The training loss is 5.837694393064631 with std:12.518939557191747. The val loss is 7.396386671145479 with std:12.886535922375904.\n",
      "7.396386671145479 0.0014459729217920197\n",
      "The training loss is 5.002597587082506 with std:7.842414978767721. The val loss is 10.858116323930325 with std:30.23301593753893.\n",
      "10.858116323930325 0.0014459729217920197\n",
      "Evaluating for {'lmda': 0.0014593657991557561} ...\n",
      "The training loss is 5.315411393623881 with std:12.092348814360365. The val loss is 3979.008433045955 with std:37879.726892830324.\n",
      "3979.008433045955 0.0014593657991557561\n",
      "The training loss is 5.674423526423453 with std:10.621556609938224. The val loss is 9.224013949669493 with std:22.404136309149116.\n",
      "9.224013949669493 0.0014593657991557561\n",
      "The training loss is 5.840000285383719 with std:12.5230179555718. The val loss is 7.398183636891369 with std:12.879513082305484.\n",
      "7.398183636891369 0.0014593657991557561\n",
      "The training loss is 5.004802177448573 with std:7.844536427155403. The val loss is 10.859177544695376 with std:30.235501526272575.\n",
      "10.859177544695376 0.0014593657991557561\n",
      "Evaluating for {'lmda': 0.0014728827239075019} ...\n",
      "The training loss is 5.317568053326473 with std:12.097435798182172. The val loss is 3969.1600573122814 with std:37785.83771397294.\n",
      "3969.1600573122814 0.0014728827239075019\n",
      "The training loss is 5.677029383326821 with std:10.6245866331286. The val loss is 9.234716076659272 with std:22.476607039692432.\n",
      "9.234716076659272 0.0014728827239075019\n",
      "The training loss is 5.842305702489683 with std:12.527091869570333. The val loss is 7.3999790397361975 with std:12.872460158031965.\n",
      "7.3999790397361975 0.0014728827239075019\n",
      "The training loss is 5.007006300362329 with std:7.8466554150777235. The val loss is 10.860235151837378 with std:30.237989949917868.\n",
      "10.860235151837378 0.0014728827239075019\n",
      "Evaluating for {'lmda': 0.0014865248449978572} ...\n",
      "The training loss is 5.31972852915362 with std:12.10252268085318. The val loss is 3959.176245531762 with std:37690.654286924386.\n",
      "3959.176245531762 0.0014865248449978572\n",
      "The training loss is 5.679630154864246 with std:10.627606568387925. The val loss is 9.245442713084334 with std:22.54945506316327.\n",
      "9.245442713084334 0.0014865248449978572\n",
      "The training loss is 5.8446105971996785 with std:12.531161200544659. The val loss is 7.401772904520663 with std:12.865378209604003.\n",
      "7.401772904520663 0.0014865248449978572\n",
      "The training loss is 5.009209923601671 with std:7.848771946515148. The val loss is 10.861289024411688 with std:30.240480716718967.\n",
      "10.861289024411688 0.0014865248449978572\n",
      "Evaluating for {'lmda': 0.0015002933220192183} ...\n",
      "The training loss is 5.321892790091228 with std:12.107609239987745. The val loss is 3949.0579121647734 with std:37594.185364282384.\n",
      "3949.0579121647734 0.0015002933220192183\n",
      "The training loss is 5.682225781333594 with std:10.630616358337681. The val loss is 9.256193262624475 with std:22.622675369814512.\n",
      "9.256193262624475 0.0015002933220192183\n",
      "The training loss is 5.846914922428248 with std:12.535225849998458. The val loss is 7.4035652552763 with std:12.858268293952088.\n",
      "7.4035652552763 0.0015002933220192183\n",
      "The training loss is 5.011413014615993 with std:7.850886025080111. The val loss is 10.862339039291376 with std:30.242973323162314.\n",
      "10.862339039291376 0.0015002933220192183\n",
      "Evaluating for {'lmda': 0.001514189325304352} ...\n",
      "The training loss is 5.324060804831668 with std:12.11269525258326. The val loss is 3938.805981424954 with std:37496.43979161213.\n",
      "3938.805981424954 0.001514189325304352\n",
      "The training loss is 5.684816203653655 with std:10.633615946455373. The val loss is 9.266967127989107 with std:22.696262895538304.\n",
      "9.266967127989107 0.001514189325304352\n",
      "The training loss is 5.849218631116809 with std:12.539285719479244. The val loss is 7.405356115003092 with std:12.851131463753958.\n",
      "7.405356115003092 0.001514189325304352\n",
      "The training loss is 5.01361554050821 with std:7.852997654139957. The val loss is 10.863385070899486 with std:30.2454672527143.\n",
      "10.863385070899486 0.001514189325304352\n",
      "Evaluating for {'lmda': 0.0015282140360258693} ...\n",
      "The training loss is 5.3262325417568475 with std:12.117780495003075. The val loss is 3928.4213874576853 with std:37397.42650914212.\n",
      "3928.4213874576853 0.0015282140360258693\n",
      "The training loss is 5.687401363412738 with std:10.636605277169828. The val loss is 9.277763710886076 with std:22.77021252303777.\n",
      "9.277763710886076 0.0015282140360258693\n",
      "The training loss is 5.851521676259916 with std:12.543340710587186. The val loss is 7.407145505664655 with std:12.843968767495904.\n",
      "7.407145505664655 0.0015282140360258693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 5.015817468094312 with std:7.855106836735018. The val loss is 10.864426991497455 with std:30.24796197721351.\n",
      "10.864426991497455 0.0015282140360258693\n",
      "Evaluating for {'lmda': 0.001542368646296629} ...\n",
      "The training loss is 5.328407968962219 with std:12.122864743148222. The val loss is 3917.905074199865 with std:37297.15455042875.\n",
      "3917.905074199865 0.001542368646296629\n",
      "The training loss is 5.6899812028249235 with std:10.639584295829252. The val loss is 9.288582412102812 with std:22.8445190828982.\n",
      "9.288582412102812 0.001542368646296629\n",
      "The training loss is 5.853824010940628 with std:12.547390725067121. The val loss is 7.408933448387139 with std:12.836781249923348.\n",
      "7.408933448387139 0.001542368646296629\n",
      "The training loss is 5.018018763855851 with std:7.857213575586442. The val loss is 10.865464671000286 with std:30.250456956106973.\n",
      "10.865464671000286 0.001542368646296629\n",
      "Evaluating for {'lmda': 0.001556654359271062} ...\n",
      "The training loss is 5.3305870542002864 with std:12.12794777230057. The val loss is 3907.257995380893 with std:37195.63304235995.\n",
      "3907.257995380893 0.001556654359271062\n",
      "The training loss is 5.6925556647463935 with std:10.642552948641537. The val loss is 9.29942263176669 with std:22.919177357057126.\n",
      "9.29942263176669 0.001556654359271062\n",
      "The training loss is 5.856125588246494 with std:12.551435664713063. The val loss is 7.410719963124066 with std:12.829569950440279.\n",
      "7.410719963124066 0.001556654359271062\n",
      "The training loss is 5.020219393950021 with std:7.85931787308401. The val loss is 10.866497976942808 with std:30.252951636234968.\n",
      "10.866497976942808 0.001556654359271062\n",
      "Evaluating for {'lmda': 0.0015710723892474504} ...\n",
      "The training loss is 5.332769764944295 with std:12.133029357258026. The val loss is 3896.481114711716 with std:37092.871206956086.\n",
      "3896.481114711716 0.0015710723892474504\n",
      "The training loss is 5.695124692699718 with std:10.645511182732562. The val loss is 9.310283769247208 with std:22.994182078635784.\n",
      "9.310283769247208 0.0015710723892474504\n",
      "The training loss is 5.858426361346915 with std:12.555475431454433. The val loss is 7.412505068946461 with std:12.822335904136128.\n",
      "7.412505068946461 0.0015710723892474504\n",
      "The training loss is 5.022419324232155 with std:7.861419731321063. The val loss is 10.867526774598776 with std:30.255445452388738.\n",
      "10.867526774598776 0.0015710723892474504\n",
      "Evaluating for {'lmda': 0.001585623961771137} ...\n",
      "The training loss is 5.334956068335432 with std:12.138109272280174. The val loss is 3885.5754055852776 with std:36988.87835850948.\n",
      "3885.5754055852776 0.001585623961771137\n",
      "The training loss is 5.6976882308210195 with std:10.648458946158863. The val loss is 9.321165223611047 with std:23.069527936348138.\n",
      "9.321165223611047 0.001585623961771137\n",
      "The training loss is 5.86072628344157 with std:12.559509927262114. The val loss is 7.414288783736177 with std:12.815080140386769.\n",
      "7.414288783736177 0.001585623961771137\n",
      "The training loss is 5.024618520222829 with std:7.863519152001735. The val loss is 10.868550926796537 with std:30.257937826798724.\n",
      "10.868550926796537 0.001585623961771137\n",
      "Evaluating for {'lmda': 0.0016003103137387001} ...\n",
      "The training loss is 5.337145931219275 with std:12.143187291310145. The val loss is 3874.5418513294658 with std:36883.66390599524.\n",
      "3874.5418513294658 0.0016003103137387001\n",
      "The training loss is 5.700246223922708 with std:10.651396187873981. The val loss is 9.332066393478888 with std:23.145209574688966.\n",
      "9.332066393478888 0.0016003103137387001\n",
      "The training loss is 5.8630253077590675 with std:12.563539054214234. The val loss is 7.4160711244654465 with std:12.807803683938543.\n",
      "7.4160711244654465 0.0016003103137387001\n",
      "The training loss is 5.0268169471459485 with std:7.865616136537981. The val loss is 10.869570294190929 with std:30.260428169937924.\n",
      "10.869570294190929 0.0016003103137387001\n",
      "Evaluating for {'lmda': 0.0016151326935030914} ...\n",
      "The training loss is 5.339339320124084 with std:12.148263187697728. The val loss is 3863.3814450943505 with std:36777.23735198897.\n",
      "3863.3814450943505 0.0016151326935030914\n",
      "The training loss is 5.702798617460259 with std:10.65432285771208. The val loss is 9.342986677401687 with std:23.22122159737011.\n",
      "9.342986677401687 0.0016151326935030914\n",
      "The training loss is 5.865323387626941 with std:12.56756271455701. The val loss is 7.417852106913394 with std:12.800507553225989.\n",
      "7.417852106913394 0.0016151326935030914\n",
      "The training loss is 5.029014569928306 with std:7.867710685968441. The val loss is 10.87058473502251 with std:30.262915879558026.\n",
      "10.87058473502251 0.0016151326935030914\n",
      "Evaluating for {'lmda': 0.0016300923609797412} ...\n",
      "The training loss is 5.341536201244345 with std:12.153336734444322. The val loss is 3852.095189759857 with std:36669.6082917923.\n",
      "3852.095189759857 0.0016300923609797412\n",
      "The training loss is 5.705345357536658 with std:10.65723890640796. The val loss is 9.35392547374669 with std:23.297558567819035.\n",
      "9.35392547374669 0.0016300923609797412\n",
      "The training loss is 5.867620476336664 with std:12.57158081052239. The val loss is 7.419631745887776 with std:12.793192761424205.\n",
      "7.419631745887776 0.0016300923609797412\n",
      "The training loss is 5.031211353128968 with std:7.869802800943937. The val loss is 10.871594105149253 with std:30.26540034133064.\n",
      "10.871594105149253 0.0016300923609797412\n",
      "Evaluating for {'lmda': 0.0016451905877536625} ...\n",
      "The training loss is 5.343736540485949 with std:12.158407704200915. The val loss is 3840.684098114275 with std:36560.786415119146.\n",
      "3840.684098114275 0.0016451905877536625\n",
      "The training loss is 5.707886390906626 with std:10.660144285595665. The val loss is 9.364882181151051 with std:23.374215013080757.\n",
      "9.364882181151051 0.0016451905877536625\n",
      "The training loss is 5.869916527271396 with std:12.575593244553254. The val loss is 7.42141005499178 with std:12.785860315097215.\n",
      "7.42141005499178 0.0016451905877536625\n",
      "The training loss is 5.033407261054465 with std:7.871892481790555. The val loss is 10.872598258211509 with std:30.26788092891815.\n",
      "10.872598258211509 0.0016451905877536625\n",
      "Evaluating for {'lmda': 0.0016604286571875295} ...\n",
      "The training loss is 5.345940303426762 with std:12.163475869250359. The val loss is 3829.149192681975 with std:36450.78150446333.\n",
      "3829.149192681975 0.0016604286571875295\n",
      "The training loss is 5.710421664995609 with std:10.663038947858684. The val loss is 9.375856198113901 with std:23.451185422429578.\n",
      "9.375856198113901 0.0016604286571875295\n",
      "The training loss is 5.872211493831254 with std:12.57959991914813. The val loss is 7.423187046767921 with std:12.778511214743112.\n",
      "7.423187046767921 0.0016604286571875295\n",
      "The training loss is 5.035602257695949 with std:7.87397972846854. The val loss is 10.873597045258295 with std:30.270357002977576.\n",
      "10.873597045258295 0.0016604286571875295\n",
      "Evaluating for {'lmda': 0.0016758078645307671} ...\n",
      "The training loss is 5.3481474553360835 with std:12.168541001458632. The val loss is 3817.4915058043025 with std:36339.60343585944.\n",
      "3817.4915058043025 0.0016758078645307671\n",
      "The training loss is 5.712951127867298 with std:10.665922846579344. The val loss is 9.386846923842763 with std:23.528464254242675.\n",
      "9.386846923842763 0.0016758078645307671\n",
      "The training loss is 5.874505329440938 with std:12.58360073691353. The val loss is 7.424962732599704 with std:12.771146454153254.\n",
      "7.424962732599704 0.0016758078645307671\n",
      "The training loss is 5.037796306724385 with std:7.876064540517943. The val loss is 10.874590315425198 with std:30.272827913587413.\n",
      "10.874590315425198 0.0016758078645307671\n",
      "Evaluating for {'lmda': 0.0016913295170296488} ...\n",
      "The training loss is 5.3503579611319845 with std:12.173602872468763. The val loss is 3805.7120795171872 with std:36227.2621777148.\n",
      "3805.7120795171872 0.0016913295170296488\n",
      "The training loss is 5.715474728229723 with std:10.668795936086822. The val loss is 9.39785375781306 with std:23.606045933883298.\n",
      "9.39785375781306 0.0016913295170296488\n",
      "The training loss is 5.8767979875498995 with std:12.587595600529125. The val loss is 7.426737122802637 with std:12.763767020766693.\n",
      "7.426737122802637 0.0016913295170296488\n",
      "The training loss is 5.03998937150445 with std:7.8781469171204135. The val loss is 10.87557791508419 with std:30.275292997006172.\n",
      "10.87557791508419 0.0016913295170296488\n",
      "Evaluating for {'lmda': 0.001706994934038408} ...\n",
      "The training loss is 5.352571785448331 with std:12.17866125359168. The val loss is 3793.8119656822796 with std:36113.767792061415.\n",
      "3793.8119656822796 0.001706994934038408\n",
      "The training loss is 5.7179924154717074 with std:10.671658171566092. The val loss is 9.408876100237965 with std:23.68392485779175.\n",
      "9.408876100237965 0.001706994934038408\n",
      "The training loss is 5.879089421676564 with std:12.591584412878364. The val loss is 7.4285102264363365 with std:12.756373894313406.\n",
      "7.4285102264363365 0.001706994934038408\n",
      "The training loss is 5.042181415153207 with std:7.880226857086544. The val loss is 10.876559688536286 with std:30.27775157852333.\n",
      "10.876559688536286 0.001706994934038408\n",
      "Evaluating for {'lmda': 0.001722805447131394} ...\n",
      "The training loss is 5.354788892627111 with std:12.183715916032538. The val loss is 3781.7922258184867 with std:35999.13043293757.\n",
      "3781.7922258184867 0.001722805447131394\n",
      "The training loss is 5.720504139635896 with std:10.674509509108777. The val loss is 9.419913351937026 with std:23.76209539422102.\n",
      "9.419913351937026 0.001722805447131394\n",
      "The training loss is 5.881379585329386 with std:12.595567076883674. The val loss is 7.430282051478853 with std:12.748968047675277.\n",
      "7.430282051478853 0.001722805447131394\n",
      "The training loss is 5.044372400444018 with std:7.8823043588019575. The val loss is 10.87753547766467 with std:30.28020297102587.\n",
      "10.87753547766467 0.001722805447131394\n",
      "Evaluating for {'lmda': 0.0017387624002162504} ...\n",
      "The training loss is 5.357009246621217 with std:12.18876663054796. The val loss is 3769.6539311819656 with std:35883.36034716027.\n",
      "3769.6539311819656 0.0017387624002162504\n",
      "The training loss is 5.723009851415347 with std:10.677349905678991. The val loss is 9.430964914680455 with std:23.840551886463544.\n",
      "9.430964914680455 0.0017387624002162504\n",
      "The training loss is 5.883668432062258 with std:12.599543495629282. The val loss is 7.4320526047292645 with std:12.741550446394093.\n",
      "7.4320526047292645 0.0017387624002162504\n",
      "The training loss is 5.046562289886506 with std:7.884379420294837. The val loss is 10.878505122029594 with std:30.282646475355044.\n",
      "10.878505122029594 0.0017387624002162504\n",
      "Evaluating for {'lmda': 0.0017548671496481502} ...\n",
      "The training loss is 5.359232811099053 with std:12.193813167768816. The val loss is 3757.398162654015 with std:35766.46787324495.\n",
      "3757.398162654015 0.0017548671496481502\n",
      "The training loss is 5.725509502141469 with std:10.680179319073797. The val loss is 9.442030191231805 with std:23.919288653802507.\n",
      "9.442030191231805 0.0017548671496481502\n",
      "The training loss is 5.885955915426956 with std:12.603513572237697. The val loss is 7.433821891681216 with std:12.734122047826386.\n",
      "7.433821891681216 0.0017548671496481502\n",
      "The training loss is 5.048751045695976 with std:7.88645203915864. The val loss is 10.879468458914177 with std:30.285081380651054.\n",
      "10.879468458914177 0.0017548671496481502\n",
      "Evaluating for {'lmda': 0.0017711210643450886} ...\n",
      "The training loss is 5.361459549419926 with std:12.198855298255944. The val loss is 3745.026010863842 with std:35648.46344257682.\n",
      "3745.026010863842 0.0017711210643450886\n",
      "The training loss is 5.728003043830847 with std:10.682997708037703. The val loss is 9.45310858533372 with std:23.998299993192443.\n",
      "9.45310858533372 0.0017711210643450886\n",
      "The training loss is 5.888241989036892 with std:12.607477210059644. The val loss is 7.435589916815812 with std:12.726683801971646.\n",
      "7.435589916815812 0.0017711210643450886\n",
      "The training loss is 5.050938629820884 with std:7.888522212595973. The val loss is 10.880425323240365 with std:30.28750696380755.\n",
      "10.880425323240365 0.0017711210643450886\n",
      "Evaluating for {'lmda': 0.0017875255259042354} ...\n",
      "The training loss is 5.363689424619885 with std:12.203892792274958. The val loss is 3732.5385758863945 with std:35529.35757652759.\n",
      "3732.5385758863945 0.0017875255259042354\n",
      "The training loss is 5.730490429159098 with std:10.685805032096189. The val loss is 9.464199502333415 with std:24.077580184052177.\n",
      "9.464199502333415 0.0017875255259042354\n",
      "The training loss is 5.890526606508058 with std:12.611434312538547. The val loss is 7.4373566832746025 with std:12.719236650249178.\n",
      "7.4373566832746025 0.0017875255259042354\n",
      "The training loss is 5.053125003931207 with std:7.890589937408609. The val loss is 10.881375547702387 with std:30.28992249044122.\n",
      "10.881375547702387 0.0017875255259042354\n",
      "Evaluating for {'lmda': 0.0018040819287193828} ...\n",
      "The training loss is 5.36592239939436 with std:12.208925420029209. The val loss is 3719.936967583959 with std:35409.16088971171.\n",
      "3719.936967583959 0.0018040819287193828\n",
      "The training loss is 5.73297161143163 with std:10.688601251674234. The val loss is 9.475302348551306 with std:24.157123485436163.\n",
      "9.475302348551306 0.0018040819287193828\n",
      "The training loss is 5.892809721482876 with std:12.615384783198154. The val loss is 7.43912219302079 with std:12.711781526091867.\n",
      "7.43912219302079 0.0018040819287193828\n",
      "The training loss is 5.055310129418697 with std:7.892655209978508. The val loss is 10.882318962668494 with std:30.292327214185907.\n",
      "10.882318962668494 0.0018040819287193828\n",
      "Evaluating for {'lmda': 0.0018207916800994624} ...\n",
      "The training loss is 5.36815843615206 with std:12.213952951641865. The val loss is 3707.222305164438 with std:35287.88408576328.\n",
      "3707.222305164438 0.0018207916800994624\n",
      "The training loss is 5.735446544638973 with std:10.691386328061192. The val loss is 9.486416532079408 with std:24.23692414199152.\n",
      "9.486416532079408 0.0018207916800994624\n",
      "The training loss is 5.895091287623264 with std:12.61932852575328. The val loss is 7.440886446831386 with std:12.704319354271236.\n",
      "7.440886446831386 0.0018207916800994624\n",
      "The training loss is 5.057493967434308 with std:7.894718026288543. The val loss is 10.883255396183946 with std:30.29472037672825.\n",
      "10.883255396183946 0.0018207916800994624\n",
      "Evaluating for {'lmda': 0.0018376562003881705} ...\n",
      "The training loss is 5.370397496971063 with std:12.218975157161317. The val loss is 3694.3957174506595 with std:35165.53795991808.\n",
      "3694.3957174506595 0.0018376562003881705\n",
      "The training loss is 5.737915183421826 with std:10.69416022337736. The val loss is 9.497541462523767 with std:24.31697638399008.\n",
      "9.497541462523767 0.0018376562003881705\n",
      "The training loss is 5.897371258609394 with std:12.623265444004. The val loss is 7.442649444190651 with std:12.696851050811649.\n",
      "7.442649444190651 0.0018376562003881705\n",
      "The training loss is 5.059676478849265 with std:7.89677838190548. The val loss is 10.884184674116257 with std:30.297101208589567.\n",
      "10.884184674116257 0.0018376562003881705\n",
      "Evaluating for {'lmda': 0.0018546769230846994} ...\n",
      "The training loss is 5.372639543592255 with std:12.223991806523891. The val loss is 3681.4583427502357 with std:35042.13339775625.\n",
      "3681.4583427502357 0.0018546769230846994\n",
      "The training loss is 5.740377483058344 with std:10.696922900584148. The val loss is 9.508676551306324 with std:24.397274429752574.\n",
      "9.508676551306324 0.0018546769230846994\n",
      "The training loss is 5.899649588133096 with std:12.627195441907693. The val loss is 7.444411183359656 with std:12.68937752272492.\n",
      "7.444411183359656 0.0018546769230846994\n",
      "The training loss is 5.061857624279064 with std:7.898836271969668. The val loss is 10.885106619949337 with std:30.299468928255212.\n",
      "10.885106619949337 0.0018546769230846994\n",
      "Evaluating for {'lmda': 0.0018718552949655793} ...\n",
      "The training loss is 5.374884537461213 with std:12.22900266969327. The val loss is 3668.411328715302 with std:34917.6813738707.\n",
      "3668.411328715302 0.0018718552949655793\n",
      "The training loss is 5.742833399524919 with std:10.699674323489896. The val loss is 9.51982121164024 with std:24.47781248698832.\n",
      "9.51982121164024 0.0018718552949655793\n",
      "The training loss is 5.901926229936518 with std:12.631118423563349. The val loss is 7.4461716613688225 with std:12.681899668089168.\n",
      "7.4461716613688225 0.0018718552949655793\n",
      "The training loss is 5.0640373641225 with std:7.900891691213021. The val loss is 10.886021055154846 with std:30.3018227437005.\n",
      "10.886021055154846 0.0018718552949655793\n",
      "Evaluating for {'lmda': 0.0018891927762076663} ...\n",
      "The training loss is 5.377132439717192 with std:12.234007516683382. The val loss is 3655.255832426892 with std:34792.19295266094.\n",
      "3655.255832426892 0.0018891927762076663\n",
      "The training loss is 5.74528288940855 with std:10.70241445674724. The val loss is 9.530974858757862 with std:24.558584755138728.\n",
      "9.530974858757862 0.0018891927762076663\n",
      "The training loss is 5.904201137756247 with std:12.635034293235798. The val loss is 7.4479308739927585 with std:12.674418375549369.\n",
      "7.4479308739927585 0.0018891927762076663\n",
      "The training loss is 5.066215658518579 with std:7.902944633937707. The val loss is 10.88692779868995 with std:30.304161850537913.\n",
      "10.88692779868995 0.0018891927762076663\n",
      "Evaluating for {'lmda': 0.001906690840512252} ...\n",
      "The training loss is 5.37938321114654 with std:12.239006117425381. The val loss is 3641.99302028309 with std:34665.67928727818.\n",
      "3641.99302028309 0.001906690840512252\n",
      "The training loss is 5.747725909974747 with std:10.705143265815623. The val loss is 9.542136910049434 with std:24.639585427540194.\n",
      "9.542136910049434 0.001906690840512252\n",
      "The training loss is 5.906474265336715 with std:12.638942955248485. The val loss is 7.449688815750013 with std:12.66693452438968.\n",
      "7.449688815750013 0.001906690840512252\n",
      "The training loss is 5.068392467370196 with std:7.904995094052568. The val loss is 10.887826667533371 with std:30.306485433678564.\n",
      "10.887826667533371 0.001906690840512252\n",
      "Evaluating for {'lmda': 0.0019243509752303303} ...\n",
      "The training loss is 5.381636812268947 with std:12.243998242057867. The val loss is 3628.6240679225566 with std:34538.151618879405.\n",
      "3628.6240679225566 0.0019243509752303303\n",
      "The training loss is 5.750162419151591 with std:10.707860716995034. The val loss is 9.553306785153419 with std:24.72080869277626.\n",
      "9.553306785153419 0.0019243509752303303\n",
      "The training loss is 5.908745566480415 with std:12.642844314197841. The val loss is 7.451445479860923 with std:12.659448983931968.\n",
      "7.451445479860923 0.0019243509752303303\n",
      "The training loss is 5.070567750369481 with std:7.907043065016071. The val loss is 10.88871747632154 with std:30.308792666655446.\n",
      "10.88871747632154 0.0019243509752303303\n",
      "Evaluating for {'lmda': 0.0019421746814890265} ...\n",
      "The training loss is 5.383893203246734 with std:12.248983660656467. The val loss is 3615.1501602967796 with std:34409.62127732561.\n",
      "3615.1501602967796 0.0019421746814890265\n",
      "The training loss is 5.752592375515433 with std:10.710566777377938. The val loss is 9.564483906018282 with std:24.802248736437942.\n",
      "9.564483906018282 0.0019421746814890265\n",
      "The training loss is 5.911014994972131 with std:12.646738274745646. The val loss is 7.4532008582975715 with std:12.651962613919011.\n",
      "7.4532008582975715 0.0019421746814890265\n",
      "The training loss is 5.0727414669802835 with std:7.909088539886398. The val loss is 10.889600037611256 with std:30.31108271223684.\n",
      "10.889600037611256 0.0019421746814890265\n",
      "Evaluating for {'lmda': 0.0019601634743191855} ...\n",
      "The training loss is 5.386152343972591 with std:12.253962143540761. The val loss is 3601.5724914101943 with std:34280.099678695486.\n",
      "3601.5724914101943 0.0019601634743191855\n",
      "The training loss is 5.755015738274634 with std:10.713261414897236. The val loss is 9.575667696924103 with std:24.883899741934535.\n",
      "9.575667696924103 0.0019601634743191855\n",
      "The training loss is 5.913282504634031 with std:12.650624741708757. The val loss is 7.454954941727826 with std:12.644476263867269.\n",
      "7.454954941727826 0.0019601634743191855\n",
      "The training loss is 5.074913576460566 with std:7.9111315113017815. The val loss is 10.890474161706484 with std:30.31335472180137.\n",
      "10.890474161706484 0.0019601634743191855\n",
      "Evaluating for {'lmda': 0.001978318882784164} ...\n",
      "The training loss is 5.388414193991171 with std:12.258933461038605. The val loss is 3587.8922644225217 with std:34149.59832626149.\n",
      "3587.8922644225217 0.001978318882784164\n",
      "The training loss is 5.75743246730962 with std:10.715944598249552. The val loss is 9.586857584923651 with std:24.965755895005465.\n",
      "9.586857584923651 0.001978318882784164\n",
      "The training loss is 5.915548049312875 with std:12.654503620132681. The val loss is 7.456707719585325 with std:12.636990773138256.\n",
      "7.456707719585325 0.001978318882784164\n",
      "The training loss is 5.077084037855098 with std:7.913171971462302. The val loss is 10.89133965685273 with std:30.315607836200257.\n",
      "10.89133965685273 0.001978318882784164\n",
      "Evaluating for {'lmda': 0.0019966424501097935} ...\n",
      "The training loss is 5.390678712577902 with std:12.263897383791731. The val loss is 3574.1106915651653 with std:34018.1288096904.\n",
      "3574.1106915651653 0.0019966424501097935\n",
      "The training loss is 5.7598425231812325 with std:10.718616296963994. The val loss is 9.59805299962538 with std:25.04781138251786.\n",
      "9.59805299962538 0.0019966424501097935\n",
      "The training loss is 5.917811582875586 with std:12.658374815183796. The val loss is 7.458459180035734 with std:12.629506971021202.\n",
      "7.458459180035734 0.0019966424501097935\n",
      "The training loss is 5.079252810035181 with std:7.915209912196149. The val loss is 10.89219632908128 with std:30.31784118532543.\n",
      "10.89219632908128 0.0019966424501097935\n",
      "Evaluating for {'lmda': 0.0020151357338155586} ...\n",
      "The training loss is 5.392945858658251 with std:12.268853682450136. The val loss is 3560.2289939427337 with std:33885.70280314839.\n",
      "3560.2289939427337 0.0020151357338155586\n",
      "The training loss is 5.762245867031424 with std:10.721276481313092. The val loss is 9.609253373623671 with std:25.13006039689014.\n",
      "9.609253373623671 0.0020151357338155586\n",
      "The training loss is 5.920073059196437 with std:12.66223823214326. The val loss is 7.460209309880815 with std:12.622025675798726.\n",
      "7.460209309880815 0.0020151357338155586\n",
      "The training loss is 5.081419851665667 with std:7.917245324868558. The val loss is 10.89304398244314 with std:30.32005388866903.\n",
      "10.89304398244314 0.0020151357338155586\n",
      "Evaluating for {'lmda': 0.002033800305846982} ...\n",
      "The training loss is 5.395215590902578 with std:12.273802128091901. The val loss is 3546.2484016565013 with std:33752.33206647789.\n",
      "3546.2484016565013 0.002033800305846982\n",
      "The training loss is 5.764642460716178 with std:10.723925122426406. The val loss is 9.620458142238414 with std:25.21249713512831.\n",
      "9.620458142238414 0.002033800305846982\n",
      "The training loss is 5.922332432184219 with std:12.66609377654461. The val loss is 7.461958094661443 with std:12.614547695347603.\n",
      "7.461958094661443 0.002033800305846982\n",
      "The training loss is 5.083585121247609 with std:7.919278200462149. The val loss is 10.893882418805779 with std:30.322245055022005.\n",
      "10.893882418805779 0.002033800305846982\n",
      "Evaluating for {'lmda': 0.002052637752709252} ...\n",
      "The training loss is 5.397487867635434 with std:12.278742491840577. The val loss is 3532.170153601062 with std:33618.02844325684.\n",
      "3532.170153601062 0.002052637752709252\n",
      "The training loss is 5.7670322666983544 with std:10.726562192136754. The val loss is 9.63166674383756 with std:25.295115801947805.\n",
      "9.63166674383756 0.002052637752709252\n",
      "The training loss is 5.924589655774688 with std:12.66994135407794. The val loss is 7.463705518700828 with std:12.607073826851694.\n",
      "7.463705518700828 0.002052637752709252\n",
      "The training loss is 5.085748577107152 with std:7.921308529530566. The val loss is 10.894711438097659 with std:30.32441378315068.\n",
      "10.894711438097659 0.002052637752709252\n",
      "Evaluating for {'lmda': 0.002071649675602069} ...\n",
      "The training loss is 5.399762646927734 with std:12.283674545246328. The val loss is 3517.995497432976 with std:33482.80386049375.\n",
      "3517.995497432976 0.002071649675602069\n",
      "The training loss is 5.769415248116803 with std:10.729187663073187. The val loss is 9.642878619873919 with std:25.377910610836857.\n",
      "9.642878619873919 0.002071649675602069\n",
      "The training loss is 5.9268446839320275 with std:12.673780870582853. The val loss is 7.465451564907536 with std:12.599604856056004.\n",
      "7.465451564907536 0.002071649675602069\n",
      "The training loss is 5.087910177405438 with std:7.923336302227818. The val loss is 10.895530838031124 with std:30.326559160813858.\n",
      "10.895530838031124 0.002071649675602069\n",
      "Evaluating for {'lmda': 0.00209083769055575} ...\n",
      "The training loss is 5.402039886503413 with std:12.288598060031354. The val loss is 3503.7256894780667 with std:33346.67032774977.\n",
      "3503.7256894780667 0.00209083769055575\n",
      "The training loss is 5.771791368741905 with std:10.731801508639595. The val loss is 9.654093215254507 with std:25.46087578793041.\n",
      "9.654093215254507 0.00209083769055575\n",
      "The training loss is 5.929097470634811 with std:12.677612232113772. The val loss is 7.467196215027547 with std:12.592141558120012.\n",
      "7.467196215027547 0.00209083769055575\n",
      "The training loss is 5.090069880162941 with std:7.925361508318405. The val loss is 10.896340414439143 with std:30.32868026629448.\n",
      "10.896340414439143 0.00209083769055575\n",
      "Evaluating for {'lmda': 0.0021102034285685966} ...\n",
      "The training loss is 5.4043195438346885 with std:12.293512808349432. The val loss is 3489.3619946221042 with std:33209.63993608599.\n",
      "3489.3619946221042 0.0021102034285685966\n",
      "The training loss is 5.774160592983556 with std:10.734403702962684. The val loss is 9.665309977945617 with std:25.544005569656214.\n",
      "9.665309977945617 0.0021102034285685966\n",
      "The training loss is 5.9313479699079386 with std:12.681435344912833. The val loss is 7.468939449423746 with std:12.584684696528164.\n",
      "7.468939449423746 0.0021102034285685966\n",
      "The training loss is 5.092227643244285 with std:7.92738413712073. The val loss is 10.897139961168216 with std:30.33077616771711.\n",
      "10.897139961168216 0.0021102034285685966\n",
      "Evaluating for {'lmda': 0.002129748535745521} ...\n",
      "The training loss is 5.406601576087249 with std:12.298418562631465. The val loss is 3474.905686302542 with std:33071.724857988505.\n",
      "3474.905686302542 0.002129748535745521\n",
      "The training loss is 5.776522885910643 with std:10.736994220931894. The val loss is 9.676528359484307 with std:25.62729420728935.\n",
      "9.676528359484307 0.002129748535745521\n",
      "The training loss is 5.9335961357927625 with std:12.685250115378928. The val loss is 7.470681247189798 with std:12.577235023648566.\n",
      "7.470681247189798 0.002129748535745521\n",
      "The training loss is 5.09438342440546 with std:7.929404177606613. The val loss is 10.897929270096911 with std:30.332845923420074.\n",
      "10.897929270096911 0.002129748535745521\n",
      "Evaluating for {'lmda': 0.0021494746734379805} ...\n",
      "The training loss is 5.408885940120858 with std:12.303315095674028. The val loss is 3460.358046368939 with std:32932.93734603539.\n",
      "3460.358046368939 0.0021494746734379805\n",
      "The training loss is 5.778878213211567 with std:10.739573038213306. The val loss is 9.687747814920119 with std:25.710735967757763.\n",
      "9.687747814920119 0.0021494746734379805\n",
      "The training loss is 5.935841922369103 with std:12.689056450133204. The val loss is 7.472421586123407 with std:12.569793280033217.\n",
      "7.472421586123407 0.0021494746734379805\n",
      "The training loss is 5.096537181241945 with std:7.931421618305786. The val loss is 10.898708131148053 with std:30.334888581834694.\n",
      "10.898708131148053 0.0021494746734379805\n",
      "Evaluating for {'lmda': 0.002169383518385184} ...\n",
      "The training loss is 5.411172592535546 with std:12.308202180738698. The val loss is 3445.7203649930852 with std:32793.28973203795.\n",
      "3445.7203649930852 0.002169383518385184\n",
      "The training loss is 5.781226541230219 with std:10.742140131120795. The val loss is 9.69896780288556 with std:25.794325134705506.\n",
      "9.69896780288556 0.002169383518385184\n",
      "The training loss is 5.9380852837490465 with std:12.692854255975838. The val loss is 7.474160442742088 with std:12.562360194853325.\n",
      "7.474160442742088 0.002169383518385184\n",
      "The training loss is 5.098688871284844 with std:7.933436447411306. The val loss is 10.899476332322001 with std:30.336903181796146.\n",
      "10.899476332322001 0.002169383518385184\n",
      "Evaluating for {'lmda': 0.002189476762856621} ...\n",
      "The training loss is 5.41346148964933 with std:12.3130795914874. The val loss is 3430.9939405119053 with std:32652.79442553528.\n",
      "3430.9939405119053 0.002189476762856621\n",
      "The training loss is 5.783567836947765 with std:10.744695476762427. The val loss is 9.710187785866212 with std:25.878056011099474.\n",
      "9.710187785866212 0.002189476762856621\n",
      "The training loss is 5.9403261740995505 with std:12.6966434399926. The val loss is 7.475897792300203 with std:12.554936485408295.\n",
      "7.475897792300203 0.002189476762856621\n",
      "The training loss is 5.100838451930794 with std:7.935448652681144. The val loss is 10.900233659830414 with std:30.33888875313931.\n",
      "10.900233659830414 0.002189476762856621\n",
      "Evaluating for {'lmda': 0.002209756114795903} ...\n",
      "The training loss is 5.415752587491076 with std:12.317947102025684. The val loss is 3416.1800795354916 with std:32511.463914832733.\n",
      "3416.1800795354916 0.002209756114795903\n",
      "The training loss is 5.785902067980999 with std:10.747239052939811. The val loss is 9.721407230074067 with std:25.961922919507554.\n",
      "9.721407230074067 0.002209756114795903\n",
      "The training loss is 5.942564547608515 with std:12.70042390935408. The val loss is 7.477633608753408 with std:12.547522857262068.\n",
      "7.477633608753408 0.002209756114795903\n",
      "The training loss is 5.102985880485703 with std:7.937458221526636. The val loss is 10.900979897899479 with std:30.340844315600446.\n",
      "10.900979897899479 0.002209756114795903\n",
      "Evaluating for {'lmda': 0.0022302232979659387} ...\n",
      "The training loss is 5.418045841814652 with std:12.32280448692985. The val loss is 3401.280096531323 with std:32369.31076302786.\n",
      "3401.280096531323 0.0022302232979659387\n",
      "The training loss is 5.788229202578744 with std:10.749770838153438. The val loss is 9.732625605565556 with std:26.045920203449185.\n",
      "9.732625605565556 0.0022302232979659387\n",
      "The training loss is 5.944800358516887 with std:12.704195571566176. The val loss is 7.4793678647613095 with std:12.540120003808427.\n",
      "7.4793678647613095 0.0022302232979659387\n",
      "The training loss is 5.1051311141796845 with std:7.9394651409627395. The val loss is 10.901714829077395 with std:30.342768880756022.\n",
      "10.901714829077395 0.0022302232979659387\n",
      "Evaluating for {'lmda': 0.002250880052095462} ...\n",
      "The training loss is 5.420341208117975 with std:12.3276515213235. The val loss is 3386.2953140337595 with std:32226.347610010584.\n",
      "3386.2953140337595 0.002250880052095462\n",
      "The training loss is 5.790549209623928 with std:10.752290811607683. The val loss is 9.743842386630796 with std:26.13004223101473.\n",
      "9.743842386630796 0.002250880052095462\n",
      "The training loss is 5.9470335611024545 with std:12.707958334270034. The val loss is 7.481100531697561 with std:12.532728606262893.\n",
      "7.481100531697561 0.002250880052095462\n",
      "The training loss is 5.107274110192344 with std:7.941469397659532. The val loss is 10.902438234058991 with std:30.344661450682892.\n",
      "10.902438234058991 0.002250880052095462\n",
      "Evaluating for {'lmda': 0.0022717281330269052} ...\n",
      "The training loss is 5.422638641615569 with std:12.332487980828862. The val loss is 3371.2270622619667 with std:32082.587168814713.\n",
      "3371.2270622619667 0.0022717281330269052\n",
      "The training loss is 5.792862058633741 with std:10.75479895322578. The val loss is 9.755057051431267 with std:26.214283393100214.\n",
      "9.755057051431267 0.0022717281330269052\n",
      "The training loss is 5.94926410971249 with std:12.711712105391287. The val loss is 7.482831579718794 with std:12.525349334040387.\n",
      "7.482831579718794 0.0022717281330269052\n",
      "The training loss is 5.109414825615955 with std:7.943470977914141. The val loss is 10.903149891779 with std:30.346521018702653.\n",
      "10.903149891779 0.0022717281330269052\n",
      "Evaluating for {'lmda': 0.002292769312865649} ...\n",
      "The training loss is 5.4249380972636345 with std:12.33731364162501. The val loss is 3356.0766792811264 with std:31938.042227160353.\n",
      "3356.0766792811264 0.002292769312865649\n",
      "The training loss is 5.795167719740356 with std:10.757295243570544. The val loss is 9.766269082506698 with std:26.298638107790985.\n",
      "9.766269082506698 0.002292769312865649\n",
      "The training loss is 5.951491958715631 with std:12.715456793026634. The val loss is 7.484560977622294 with std:12.51798284373637.\n",
      "7.484560977622294 0.002292769312865649\n",
      "The training loss is 5.111553217503512 with std:7.945469867662574. The val loss is 10.90384957948159 with std:30.34834656958267.\n",
      "10.90384957948159 0.002292769312865649\n",
      "Evaluating for {'lmda': 0.002314005380130654} ...\n",
      "The training loss is 5.427239529763081 with std:12.342128280463266. The val loss is 3340.845510608769 with std:31792.725643688198.\n",
      "3340.845510608769 0.002314005380130654\n",
      "The training loss is 5.797466163720359 with std:10.759779663920806. The val loss is 9.777477966411018 with std:26.383100818436088.\n",
      "9.777477966411018 0.002314005380130654\n",
      "The training loss is 5.953717062573236 with std:12.719192305576012. The val loss is 7.486288693027576 with std:12.510629779970955.\n",
      "7.486288693027576 0.002314005380130654\n",
      "The training loss is 5.11368924287245 with std:7.947466052496298. The val loss is 10.904537072664596 with std:30.350137079551995.\n",
      "10.904537072664596 0.002314005380130654\n",
      "Evaluating for {'lmda': 0.002335438139906479} ...\n",
      "The training loss is 5.429542893564407 with std:12.34693167475257. The val loss is 3325.5349093200675 with std:31646.65034897322.\n",
      "3325.5349093200675 0.002335438139906479\n",
      "The training loss is 5.799757361973439 with std:10.762252196189461. The val loss is 9.788683194404634 with std:26.467665999375594.\n",
      "9.788683194404634 0.002335438139906479\n",
      "The training loss is 5.955939375771316 with std:12.722918551636296. The val loss is 7.488014692237635 with std:12.503290774721428.\n",
      "7.488014692237635 0.002335438139906479\n",
      "The training loss is 5.1158228587188574 with std:7.949459517667863. The val loss is 10.905212145220695 with std:30.35189151661155.\n",
      "10.905212145220695 0.002335438139906479\n",
      "Evaluating for {'lmda': 0.002357069413996728} ...\n",
      "The training loss is 5.431848142873174 with std:12.351723602504098. The val loss is 3310.1462357402565 with std:31499.829342581874.\n",
      "3310.1462357402565 0.002357069413996728\n",
      "The training loss is 5.802041286526409 with std:10.764712822989875. The val loss is 9.799884261942966 with std:26.552328152922353.\n",
      "9.799884261942966 0.002357069413996728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 5.958158852894818 with std:12.726635440070801. The val loss is 7.489738940409093 with std:12.495966447666085.\n",
      "7.489738940409093 0.002357069413996728\n",
      "The training loss is 5.117954022020541 with std:7.951450248103553. The val loss is 10.90587456942562 with std:30.353608840790876.\n",
      "10.90587456942562 0.002357069413996728\n",
      "Evaluating for {'lmda': 0.0023789010410788934} ...\n",
      "The training loss is 5.434155231627845 with std:12.356503842393662. The val loss is 3294.6808575319233 with std:31352.27569390784.\n",
      "3294.6808575319233 0.0023789010410788934\n",
      "The training loss is 5.80431791000963 with std:10.76716152755305. The val loss is 9.811080669001987 with std:26.637081812432058.\n",
      "9.811080669001987 0.0023789010410788934\n",
      "The training loss is 5.960375448558528 with std:12.730342879992815. The val loss is 7.4914614012941465 with std:12.48865740541851.\n",
      "7.4914614012941465 0.0023789010410788934\n",
      "The training loss is 5.12008268975513 with std:7.953438228394653. The val loss is 10.906524115892987 with std:30.355288003802826.\n",
      "10.906524115892987 0.0023789010410788934\n",
      "Evaluating for {'lmda': 0.0024009348768606518} ...\n",
      "The training loss is 5.436464113557193 with std:12.361272173799845. The val loss is 3279.1401493644526 with std:31204.002539014662.\n",
      "3279.1401493644526 0.0024009348768606518\n",
      "The training loss is 5.806587205680835 with std:10.769598293756987. The val loss is 9.822271920267308 with std:26.72192154445655.\n",
      "9.822271920267308 0.0024009348768606518\n",
      "The training loss is 5.962589117476206 with std:12.734040780759841. The val loss is 7.493182037541109 with std:12.48136424228837.\n",
      "7.493182037541109 0.0024009348768606518\n",
      "The training loss is 5.122208818901728 with std:7.955423442803935. The val loss is 10.90716055377758 with std:30.35692795001562.\n",
      "10.90716055377758 0.0024009348768606518\n",
      "Evaluating for {'lmda': 0.0024231727942376005} ...\n",
      "The training loss is 5.4387747421321135 with std:12.366028376752077. The val loss is 3263.52549283313 with std:31055.023079867166.\n",
      "3263.52549283313 0.0024231727942376005\n",
      "The training loss is 5.808849147431909 with std:10.772023106139718. The val loss is 9.833457525007457 with std:26.80684194826337.\n",
      "9.833457525007457 0.0024231727942376005\n",
      "The training loss is 5.964799814427318 with std:12.737729052022907. The val loss is 7.4949008105126005 with std:12.474087539632139.\n",
      "7.4949008105126005 0.0024231727942376005\n",
      "The training loss is 5.124332366478972 with std:7.957405875309978. The val loss is 10.907783650640939 with std:30.358527615931756.\n",
      "10.907783650640939 0.0024231727942376005\n",
      "Evaluating for {'lmda': 0.0024456166834524464} ...\n",
      "The training loss is 5.441087070612699 with std:12.370772232110452. The val loss is 3247.838276436773 with std:30905.35058411388.\n",
      "3247.838276436773 0.0024456166834524464\n",
      "The training loss is 5.811103709743865 with std:10.774435949829732. The val loss is 9.844636997334796 with std:26.891837658483027.\n",
      "9.844636997334796 0.0024456166834524464\n",
      "The training loss is 5.967007494278195 with std:12.741407603695166. The val loss is 7.496617680370909 with std:12.466827866004394.\n",
      "7.496617680370909 0.0024456166834524464\n",
      "The training loss is 5.126453289520618 with std:7.959385509577748. The val loss is 10.908393172608413 with std:30.360085930815448.\n",
      "10.908393172608413 0.0024456166834524464\n",
      "Evaluating for {'lmda': 0.0024682684522556926} ...\n",
      "The training loss is 5.44340105201254 with std:12.375503521392693. The val loss is 3232.0798952664413 with std:30754.998382117177.\n",
      "3232.0798952664413 0.0024682684522556926\n",
      "The training loss is 5.813350867731973 with std:10.776836810617743. The val loss is 9.855809856166859 with std:26.976903345562555.\n",
      "9.855809856166859 0.0024682684522556926\n",
      "The training loss is 5.969212111958355 with std:12.745076345922115. The val loss is 7.498332606055334 with std:12.459585777231968.\n",
      "7.498332606055334 0.0024682684522556926\n",
      "The training loss is 5.12857154512068 with std:7.961362328981658. The val loss is 10.908988884347755 with std:30.361601816756856.\n",
      "10.908988884347755 0.0024682684522556926\n",
      "Evaluating for {'lmda': 0.0024911300260677884} ...\n",
      "The training loss is 5.445716639147412 with std:12.380222026989745. The val loss is 3216.251751017875 with std:30603.979867073267.\n",
      "3216.251751017875 0.0024911300260677884\n",
      "The training loss is 5.815590597099292 with std:10.779225674867233. The val loss is 9.866975625203423 with std:27.062033716129676.\n",
      "9.866975625203423 0.0024911300260677884\n",
      "The training loss is 5.971413622513123 with std:12.748735189216765. The val loss is 7.500045545296932 with std:12.452361815967642.\n",
      "7.500045545296932 0.0024911300260677884\n",
      "The training loss is 5.130687090418893 with std:7.963336316617938. The val loss is 10.909570549079561 with std:30.363074188572362.\n",
      "10.909570549079561 0.0024911300260677884\n",
      "Evaluating for {'lmda': 0.0025142033481427967} ...\n",
      "The training loss is 5.4480337846220435 with std:12.384927532078802. The val loss is 3200.355251717715 with std:30452.308492396693.\n",
      "3200.355251717715 0.0025142033481427967\n",
      "The training loss is 5.817822874191563 with std:10.781602529570371. The val loss is 9.878133833253965 with std:27.147223515813707.\n",
      "9.878133833253965 0.0025142033481427967\n",
      "The training loss is 5.973611981080454 with std:12.752384044326554. The val loss is 7.501756454697078 with std:12.445156512165171.\n",
      "7.501756454697078 0.0025142033481427967\n",
      "The training loss is 5.132799882649604 with std:7.965307455310514. The val loss is 10.91013792872802 with std:30.36450195470321.\n",
      "10.91013792872802 0.0025142033481427967\n",
      "Evaluating for {'lmda': 0.0025374903797335717} ...\n",
      "The training loss is 5.450352440815209 with std:12.389619820632946. The val loss is 3184.3918116704244 with std:30299.997771214465.\n",
      "3184.3918116704244 0.0025374903797335717\n",
      "The training loss is 5.820047675931237 with std:10.78396736230629. The val loss is 9.889284014150798 with std:27.232467529482424.\n",
      "9.889284014150798 0.0025374903797335717\n",
      "The training loss is 5.975807142877666 with std:12.756022822293682. The val loss is 7.503465289595911 with std:12.437970382491134.\n",
      "7.503465289595911 0.0025374903797335717\n",
      "The training loss is 5.13490987909665 with std:7.9672757276130755. The val loss is 10.910690783840574 with std:30.365884016583433.\n",
      "10.910690783840574 0.0025374903797335717\n",
      "Evaluating for {'lmda': 0.002560993100258459} ...\n",
      "The training loss is 5.452672559920484 with std:12.394298677540444. The val loss is 3168.3628512339283 with std:30147.06127422663.\n",
      "3168.3628512339283 0.002560993100258459\n",
      "The training loss is 5.822264979852301 with std:10.786320161217118. The val loss is 9.900425706687766 with std:27.317760581204098.\n",
      "9.900425706687766 0.002560993100258459\n",
      "The training loss is 5.9779990632367594 with std:12.759651434469223. The val loss is 7.505172004202071 with std:12.430803930749876.\n",
      "7.505172004202071 0.002560993100258459\n",
      "The training loss is 5.137017037168104 with std:7.9692411158474545. The val loss is 10.911228873713203 with std:30.367219269620158.\n",
      "10.911228873713203 0.002560993100258459\n",
      "Evaluating for {'lmda': 0.002584713507469564} ...\n",
      "The training loss is 5.454994093933185 with std:12.39896388854987. The val loss is 3152.2697966862465 with std:29993.512628430064.\n",
      "3152.2697966862465 0.002584713507469564\n",
      "The training loss is 5.824474764097461 with std:10.788660915062314. The val loss is 9.911558455028395 with std:27.40309753788313.\n",
      "9.911558455028395 0.002584713507469564\n",
      "The training loss is 5.980187697604062 with std:12.763269792538273. The val loss is 7.506876551565858 with std:12.423657647573027.\n",
      "7.506876551565858 0.002584713507469564\n",
      "The training loss is 5.139121314375294 with std:7.97120360207973. The val loss is 10.911751956320426 with std:30.368506602585562.\n",
      "10.911751956320426 0.002584713507469564\n",
      "Evaluating for {'lmda': 0.002608653617622548} ...\n",
      "The training loss is 5.457316994646349 with std:12.403615240279988. The val loss is 3136.1140800658973 with std:29839.36551559795.\n",
      "3136.1140800658973 0.002608653617622548\n",
      "The training loss is 5.826677007404356 with std:10.790989613150213. The val loss is 9.922681808372902 with std:27.48847330712535.\n",
      "9.922681808372902 0.002608653617622548\n",
      "The training loss is 5.982373001540396 with std:12.766877808459107. The val loss is 7.508578883584526 with std:12.416532010462072.\n",
      "7.508578883584526 0.002608653617622548\n",
      "The training loss is 5.141222668339608 with std:7.973163168149178. The val loss is 10.91225978850213 with std:30.369744898594604.\n",
      "10.91225978850213 0.002608653617622548\n",
      "Evaluating for {'lmda': 0.00263281546564802} ...\n",
      "The training loss is 5.459641213703176 with std:12.408252520388192. The val loss is 3119.8971390612182 with std:29684.633671221345.\n",
      "3119.8971390612182 0.00263281546564802\n",
      "The training loss is 5.828871689108793 with std:10.79330624535174. The val loss is 9.933795321299028 with std:27.57388284078311.\n",
      "9.933795321299028 0.00263281546564802\n",
      "The training loss is 5.9845549307146655 with std:12.770475394536414. The val loss is 7.5102789510382415 with std:12.409427483936028.\n",
      "7.5102789510382415 0.00263281546564802\n",
      "The training loss is 5.143321056833443 with std:7.975119795681335. The val loss is 10.912752125875219 with std:30.37093303479516.\n",
      "10.912752125875219 0.00263281546564802\n",
      "Evaluating for {'lmda': 0.0026572011053245066} ...\n",
      "The training loss is 5.4619667025348235 with std:12.412875517350871. The val loss is 3103.6204167360183 with std:29529.330881893206.\n",
      "3103.6204167360183 0.0026572011053245066\n",
      "The training loss is 5.831058789136155 with std:10.795610802094922. The val loss is 9.94489855371317 with std:27.659321134330963.\n",
      "9.94489855371317 0.0026572011053245066\n",
      "The training loss is 5.9867334409445885 with std:12.774062463418083. The val loss is 7.5119767035606975 with std:12.402344519135694.\n",
      "7.5119767035606975 0.0026572011053245066\n",
      "The training loss is 5.145416437763159 with std:7.977073466075127. The val loss is 10.913228722914432 with std:30.372069882646294.\n",
      "10.913228722914432 0.0026572011053245066\n",
      "Evaluating for {'lmda': 0.002681812609453016} ...\n",
      "The training loss is 5.4642934124287 with std:12.417484020732774. The val loss is 3087.2853614038163 with std:29373.470984107455.\n",
      "3087.2853614038163 0.002681812609453016\n",
      "The training loss is 5.833238288014678 with std:10.797903274339435. The val loss is 9.9559910710103 with std:27.74478322933436.\n",
      "9.9559910710103 0.002681812609453016\n",
      "The training loss is 5.988908488160289 with std:12.777638928044654. The val loss is 7.513672089713258 with std:12.395283554239768.\n",
      "7.513672089713258 0.002681812609453016\n",
      "The training loss is 5.1475087692183985 with std:7.979024160587015. The val loss is 10.91368933310951 with std:30.37315430862942.\n",
      "10.91368933310951 0.002681812609453016\n",
      "Evaluating for {'lmda': 0.0027066520700332413} ...\n",
      "The training loss is 5.466621294495742 with std:12.422077821047424. The val loss is 3070.8934265178195 with std:29217.067863207758.\n",
      "3070.8934265178195 0.0027066520700332413\n",
      "The training loss is 5.835410166858499 with std:10.800183653614868. The val loss is 9.967072443897605 with std:27.830264212065085.\n",
      "9.967072443897605 0.0027066520700332413\n",
      "The training loss is 5.991080028437479 with std:12.78120470172456. The val loss is 7.515365056945808 with std:12.388245014198693.\n",
      "7.515365056945808 0.0027066520700332413\n",
      "The training loss is 5.149598009431163 with std:7.980971860220985. The val loss is 10.914133708766244 with std:30.374185173640534.\n",
      "10.914133708766244 0.0027066520700332413\n",
      "Evaluating for {'lmda': 0.002731721598441376} ...\n",
      "The training loss is 5.4689502996952415 with std:12.426656709851144. The val loss is 3054.4460704132302 with std:29060.135450931488.\n",
      "3054.4460704132302 0.002731721598441376\n",
      "The training loss is 5.837574407360154 with std:10.802451931939757. The val loss is 9.978142248652432 with std:27.915759216346476.\n",
      "9.978142248652432 0.002731721598441376\n",
      "The training loss is 5.993248018000181 with std:12.784759698132085. The val loss is 7.517055551591097 with std:12.381229310594888.\n",
      "7.517055551591097 0.002731721598441376\n",
      "The training loss is 5.151684116841306 with std:7.982916545842854. The val loss is 10.914561601268876 with std:30.375161333798598.\n",
      "10.914561601268876 0.002731721598441376\n",
      "Evaluating for {'lmda': 0.0027570233256095826} ...\n",
      "The training loss is 5.471280378846031 with std:12.431220479776368. The val loss is 3037.9447561118336 with std:28902.687723538096.\n",
      "3037.9447561118336 0.0027570233256095826\n",
      "The training loss is 5.839730991814162 with std:10.804708101890254. The val loss is 9.989200067105173 with std:28.001263423351983.\n",
      "9.989200067105173 0.0027570233256095826\n",
      "The training loss is 5.995412413217698 with std:12.78830383127564. The val loss is 7.5187435189802425 with std:12.37423684208997.\n",
      "7.5187435189802425 0.0027570233256095826\n",
      "The training loss is 5.153767050099823 with std:7.984858198172381. The val loss is 10.914972761024144 with std:30.37608164055797.\n",
      "10.914972761024144 0.0027570233256095826\n",
      "Evaluating for {'lmda': 0.0027825594022071257} ...\n",
      "The training loss is 5.473611482612474 with std:12.435768924470601. The val loss is 3021.3909512609916 with std:28744.738701237737.\n",
      "3021.3909512609916 0.0027825594022071257\n",
      "The training loss is 5.841879903088278 with std:10.806952156525496. The val loss is 10.000245486784307 with std:28.08677206359638.\n",
      "10.000245486784307 0.0027825594022071257\n",
      "The training loss is 5.997573170615318 with std:12.79183701550796. The val loss is 7.520428903334757 with std:12.367267993985712.\n",
      "7.520428903334757 0.0027825594022071257\n",
      "The training loss is 5.15584676804717 with std:7.986796797774337. The val loss is 10.915366937529225 with std:30.376944940818145.\n",
      "10.915366937529225 0.0027825594022071257\n",
      "Evaluating for {'lmda': 0.0028083319988231725} ...\n",
      "The training loss is 5.475943561529379 with std:12.440301838705333. The val loss is 3004.7861277670777 with std:28586.30244468293.\n",
      "3004.7861277670777 0.0028083319988231725\n",
      "The training loss is 5.8440211246396405 with std:10.80918408944771. The val loss is 10.011278100713819 with std:28.17228041557736.\n",
      "10.011278100713819 0.0028083319988231725\n",
      "The training loss is 5.99973024689402 with std:12.795359165613617. The val loss is 7.522111647888973 with std:12.360323138538226.\n",
      "7.522111647888973 0.0028083319988231725\n",
      "The training loss is 5.157923229767401 with std:7.988732325066778. The val loss is 10.915743879378011 with std:30.377750077206688.\n",
      "10.915743879378011 0.0028083319988231725\n",
      "Evaluating for {'lmda': 0.002834343306151309} ...\n",
      "The training loss is 5.478276566010343 with std:12.444819018396265. The val loss is 2988.13176180434 with std:28427.393055060104.\n",
      "2988.13176180434 0.002834343306151309\n",
      "The training loss is 5.846154640486831 with std:10.811403894722934. The val loss is 10.022297507694741 with std:28.25778380846716.\n",
      "10.022297507694741 0.002834343306151309\n",
      "The training loss is 6.001883598912776 with std:12.798870196695155. The val loss is 7.52379169486989 with std:12.353402634950736.\n",
      "7.52379169486989 0.002834343306151309\n",
      "The training loss is 5.159996394584215 with std:7.990664760362601. The val loss is 10.916103334357457 with std:30.378495888274212.\n",
      "10.916103334357457 0.002834343306151309\n",
      "Evaluating for {'lmda': 0.002860595535175742} ...\n",
      "The training loss is 5.480610446336676 with std:12.449320260532511. The val loss is 2971.4293334162126 with std:28268.02467028215.\n",
      "2971.4293334162126 0.002860595535175742\n",
      "The training loss is 5.84828043523455 with std:10.813611566908866. The val loss is 10.033303312332036 with std:28.343277622474684.\n",
      "10.033303312332036 0.002860595535175742\n",
      "The training loss is 6.004033183703713 with std:12.802370024252. The val loss is 7.525468985403929 with std:12.346506828895821.\n",
      "7.525468985403929 0.002860595535175742\n",
      "The training loss is 5.162066222069182 with std:7.99259408386389. The val loss is 10.9164450494516 with std:30.37918120864945.\n",
      "10.9164450494516 0.002860595535175742\n",
      "Evaluating for {'lmda': 0.0028870909173592347} ...\n",
      "The training loss is 5.482945152692827 with std:12.453805363338219. The val loss is 2954.6803264906453 with std:28108.211464752396.\n",
      "2954.6803264906453 0.0028870909173592347\n",
      "The training loss is 5.850398494065066 with std:10.81580710106767. The val loss is 10.044295124951601 with std:28.428757288996543.\n",
      "10.044295124951601 0.0028870909173592347\n",
      "The training loss is 6.006178958489835 with std:12.805858564195178. The val loss is 7.527143459721358 with std:12.339636053323405.\n",
      "7.527143459721358 0.0028870909173592347\n",
      "The training loss is 5.164132672063253 with std:7.9945202756755736. The val loss is 10.916768770909881 with std:30.379804869542973.\n",
      "10.916768770909881 0.0028870909173592347\n",
      "Evaluating for {'lmda': 0.0029138317048327885} ...\n",
      "The training loss is 5.485280635153141 with std:12.458274126191178. The val loss is 2937.8862284657657 with std:27947.967646559.\n",
      "2937.8862284657657 0.0029138317048327885\n",
      "The training loss is 5.85250880273434 with std:10.8179904927368. The val loss is 10.055272561681019 with std:28.514218291172163.\n",
      "10.055272561681019 0.0029138317048327885\n",
      "The training loss is 6.008320880701424 with std:12.80933573284058. The val loss is 7.528815057078876 with std:12.332790627999035.\n",
      "7.528815057078876 0.0029138317048327885\n",
      "The training loss is 5.166195704706866 with std:7.996443315838592. The val loss is 10.917074244315446 with std:30.380365698847143.\n",
      "10.917074244315446 0.0029138317048327885\n",
      "Evaluating for {'lmda': 0.002940820170587064} ...\n",
      "The training loss is 5.487616843691443 with std:12.462726349657418. The val loss is 2921.0485301319245 with std:27787.307455585567.\n",
      "2921.0485301319245 0.002940820170587064\n",
      "The training loss is 5.854611347549908 with std:10.820161737888844. The val loss is 10.066235244546297 with std:28.599656165210003.\n",
      "10.066235244546297 0.002940820170587064\n",
      "The training loss is 6.010458907928464 with std:12.812801446869203. The val loss is 7.530483715744738 with std:12.32597085968121.\n",
      "7.530483715744738 0.002940820170587064\n",
      "The training loss is 5.16825528038947 with std:7.998363184295052. The val loss is 10.91736121447653 with std:30.380862520690982.\n",
      "10.91736121447653 0.002940820170587064\n",
      "Evaluating for {'lmda': 0.0029680586086656023} ...\n",
      "The training loss is 5.489953728190848 with std:12.467161835531417. The val loss is 2904.1687254733024 with std:27626.245161995328.\n",
      "2904.1687254733024 0.0029680586086656023\n",
      "The training loss is 5.856706115403973 with std:10.82232083300621. The val loss is 10.077182801485645 with std:28.68506650088125.\n",
      "10.077182801485645 0.0029680586086656023\n",
      "The training loss is 6.0125929979982 with std:12.816255623430097. The val loss is 7.532149373036509 with std:12.319177041866629.\n",
      "7.532149373036509 0.0029680586086656023\n",
      "The training loss is 5.170311359867206 with std:8.000279860983362. The val loss is 10.917629425732828 with std:30.381294157147515.\n",
      "10.917629425732828 0.0029680586086656023\n",
      "Evaluating for {'lmda': 0.002995549334359813} ...\n",
      "The training loss is 5.492291238470025 with std:12.471580386906085. The val loss is 2887.2483114127276 with std:27464.795063803114.\n",
      "2887.2483114127276 0.002995549334359813\n",
      "The training loss is 5.8587930937414905 with std:10.824467774974186. The val loss is 10.08811486635411 with std:28.770444941748263.\n",
      "10.08811486635411 0.002995549334359813\n",
      "The training loss is 6.014723108934753 with std:12.819698180045568. The val loss is 7.533811965405089 with std:12.3124094553488.\n",
      "7.533811965405089 0.002995549334359813\n",
      "The training loss is 5.172363904175352 with std:8.002193325763693. The val loss is 10.917878621811461 with std:30.381659427137496.\n",
      "10.917878621811461 0.002995549334359813\n",
      "Evaluating for {'lmda': 0.0030232946844057766} ...\n",
      "The training loss is 5.494629324249542 with std:12.475981808053037. The val loss is 2870.2887876138348 with std:27302.971484983133.\n",
      "2870.2887876138348 0.0030232946844057766\n",
      "The training loss is 5.8608722705748075 with std:10.826602561150679. The val loss is 10.099031078999893 with std:28.85578718619266.\n",
      "10.099031078999893 0.0030232946844057766\n",
      "The training loss is 6.016849198990192 with std:12.8231290347114. The val loss is 7.535471428390313 with std:12.30566836800464.\n",
      "7.535471428390313 0.0030232946844057766\n",
      "The training loss is 5.1744128747004385 with std:8.004103558495014. The val loss is 10.91810854587046 with std:30.38195714705007.\n",
      "10.91810854587046 0.0030232946844057766\n",
      "Evaluating for {'lmda': 0.0030512970171828707} ...\n",
      "The training loss is 5.496967935217164 with std:12.480365904633071. The val loss is 2853.291656291218 with std:27140.78877365942.\n",
      "2853.291656291218 0.0030512970171828707\n",
      "The training loss is 5.862943634475703 with std:10.828725189340036. The val loss is 10.109931085205174 with std:28.941088987140926.\n",
      "10.109931085205174 0.0030512970171828707\n",
      "The training loss is 6.0189712266294775 with std:12.826548105809243. The val loss is 7.53712769658645 with std:12.298954034460236.\n",
      "7.53712769658645 0.0030512970171828707\n",
      "The training loss is 5.176458233178162 with std:8.006010539029118. The val loss is 10.91831894069014 with std:30.382186131453093.\n",
      "10.91831894069014 0.0030512970171828707\n",
      "Evaluating for {'lmda': 0.0030795587129142264} ...\n",
      "The training loss is 5.499307020969748 with std:12.484732483522334. The val loss is 2836.258421978594 with std:26978.261299892758.\n",
      "2836.258421978594 0.0030795587129142264\n",
      "The training loss is 5.865007174561712 with std:10.830835657765563. The val loss is 10.120814536835589 with std:29.026346153664207.\n",
      "10.120814536835589 0.0030795587129142264\n",
      "The training loss is 6.021089150565303 with std:12.829955312224016. The val loss is 7.538780703799359 with std:12.292266696948568.\n",
      "7.538780703799359 0.0030795587129142264\n",
      "The training loss is 5.178499941695962 with std:8.007914247210687. The val loss is 10.91850954854032 with std:30.382345192630464.\n",
      "10.91850954854032 0.0030795587129142264\n",
      "Evaluating for {'lmda': 0.003108082173869064} ...\n",
      "The training loss is 5.501646531081428 with std:12.489081352985174. The val loss is 2819.190591237235 with std:26815.403452901006.\n",
      "2819.190591237235 0.003108082173869064\n",
      "The training loss is 5.867062880516857 with std:10.832933965058942. The val loss is 10.13168109175558 with std:29.1115545504403.\n",
      "10.13168109175558 0.003108082173869064\n",
      "The training loss is 6.023202929743295 with std:12.833350573240434. The val loss is 7.5404303829870925 with std:12.285606584825203.\n",
      "7.5404303829870925 0.003108082173869064\n",
      "The training loss is 5.18053796272929 with std:8.009814662918291. The val loss is 10.918680111324978 with std:30.38243314109533.\n",
      "10.918680111324978 0.003108082173869064\n",
      "Evaluating for {'lmda': 0.0031368698245668766} ...\n",
      "The training loss is 5.503986415075981 with std:12.493412322619209. The val loss is 2802.0896725708253 with std:26652.22964024393.\n",
      "2802.0896725708253 0.0031368698245668766\n",
      "The training loss is 5.8691107425808 with std:10.83502011031792. The val loss is 10.142530414006703 with std:29.196710099605415.\n",
      "10.142530414006703 0.0031368698245668766\n",
      "The training loss is 6.0253125233635485 with std:12.836733808623896. The val loss is 7.542076666208466 with std:12.278973914417188.\n",
      "7.542076666208466 0.0031368698245668766\n",
      "The training loss is 5.182572259130996 with std:8.011711766066325. The val loss is 10.918830370642057 with std:30.382448786107926.\n",
      "10.918830370642057 0.0031368698245668766\n",
      "Evaluating for {'lmda': 0.0031659241119835206} ...\n",
      "The training loss is 5.506326622444676 with std:12.4977252033658. The val loss is 2784.957176091788 with std:26488.7542846416.\n",
      "2784.957176091788 0.0031659241119835206\n",
      "The training loss is 5.871150751526939 with std:10.83709409298825. The val loss is 10.153362173587023 with std:29.281808779043793.\n",
      "10.153362173587023 0.0031659241119835206\n",
      "The training loss is 6.027417890881942 with std:12.840104938627436. The val loss is 7.543719484804651 with std:12.272368889807545.\n",
      "7.543719484804651 0.0031659241119835206\n",
      "The training loss is 5.1846027941646335 with std:8.013605536628694. The val loss is 10.918960067781352 with std:30.382390935505136.\n",
      "10.918960067781352 0.0031659241119835206\n",
      "Evaluating for {'lmda': 0.0031952475057592136} ...\n",
      "The training loss is 5.508667102657978 with std:12.50201980758009. The val loss is 2767.794613325418 with std:26324.991822104967.\n",
      "2767.794613325418 0.0031952475057592136\n",
      "The training loss is 5.873182898683123 with std:10.839155912969613. The val loss is 10.164176046743142 with std:29.366846625381505.\n",
      "10.164176046743142 0.0031952475057592136\n",
      "The training loss is 6.029518992007582 with std:12.843463883943391. The val loss is 7.545358769240494 with std:12.265791702107864.\n",
      "7.545358769240494 0.0031952475057592136\n",
      "The training loss is 5.186629531500068 with std:8.015495954618972. The val loss is 10.919068943789634 with std:30.382258396122218.\n",
      "10.919068943789634 0.0031952475057592136\n",
      "Evaluating for {'lmda': 0.003224842498408442} ...\n",
      "The training loss is 5.511007805177914 with std:12.506295949024649. The val loss is 2750.6034970006685 with std:26160.956699940572.\n",
      "2750.6034970006685 0.003224842498408442\n",
      "The training loss is 5.875207175930986 with std:10.841205570542488. The val loss is 10.174971715808208 with std:29.451819732347367.\n",
      "10.174971715808208 0.003224842498408442\n",
      "The training loss is 6.0316157867430675 with std:12.846810565773914. The val loss is 7.546994449316127 with std:12.25924253030674.\n",
      "7.546994449316127 0.003224842498408442\n",
      "The training loss is 5.1886524352405115 with std:8.017383000169191. The val loss is 10.919156739545079 with std:30.38204997417726.\n",
      "10.919156739545079 0.003224842498408442\n",
      "Evaluating for {'lmda': 0.003254711605531848} ...\n",
      "The training loss is 5.513348679449662 with std:12.510553442864573. The val loss is 2733.38534081327 with std:25996.663374490796.\n",
      "2733.38534081327 0.003254711605531848\n",
      "The training loss is 5.877223575681996 with std:10.843243066377044. The val loss is 10.18574886926282 with std:29.53672425196615.\n",
      "10.18574886926282 0.003254711605531848\n",
      "The training loss is 6.0337082353455616 with std:12.85014490578428. The val loss is 7.548626453989437 with std:12.252721540591256.\n",
      "7.548626453989437 0.003254711605531848\n",
      "The training loss is 5.190671469922863 with std:8.019266653483298. The val loss is 10.919223195685378 with std:30.381764474950216.\n",
      "10.919223195685378 0.003254711605531848\n",
      "Evaluating for {'lmda': 0.0032848573660300435} ...\n",
      "The training loss is 5.515689674935817 with std:12.514792105730413. The val loss is 2716.1416591421985 with std:25832.126308427003.\n",
      "2716.1416591421985 0.0032848573660300435\n",
      "The training loss is 5.879232090900138 with std:10.845268401538114. The val loss is 10.196507201860202 with std:29.62155639550002.\n",
      "10.196507201860202 0.0032848573660300435\n",
      "The training loss is 6.035796298388765 with std:12.853466826175891. The val loss is 7.550254711574559 with std:12.246228887071199.\n",
      "7.550254711574559 0.0032848573660300435\n",
      "The training loss is 5.192686600560299 with std:8.021146894911437. The val loss is 10.91926805285599 with std:30.381400703863797.\n",
      "10.91926805285599 0.0032848573660300435\n",
      "Evaluating for {'lmda': 0.0033152823423194234} ...\n",
      "The training loss is 5.518030741096195 with std:12.519011755701994. The val loss is 2698.873966871505 with std:25667.3599690515.\n",
      "2698.873966871505 0.0033152823423194234\n",
      "The training loss is 5.881232715063698 with std:10.847281577449712. The val loss is 10.207246414374454 with std:29.706312431858645.\n",
      "10.207246414374454 0.0033152823423194234\n",
      "The training loss is 6.037879936713172 with std:12.856776249623096. The val loss is 7.551879149608793 with std:12.23976471136488.\n",
      "7.551879149608793 0.0033152823423194234\n",
      "The training loss is 5.194697792599254 with std:8.023023704890369. The val loss is 10.919291051604887 with std:30.380957466215197.\n",
      "10.919291051604887 0.0033152823423194234\n",
      "Evaluating for {'lmda': 0.0033459891205499747} ...\n",
      "The training loss is 5.520371827414559 with std:12.523212212305044. The val loss is 2681.583779120141 with std:25502.378825719774.\n",
      "2681.583779120141 0.0033459891205499747\n",
      "The training loss is 5.883225442218313 with std:10.849282595934033. The val loss is 10.217966213999533 with std:29.79098869079741.\n",
      "10.217966213999533 0.0033459891205499747\n",
      "The training loss is 6.039959111479039 with std:12.860073099311649. The val loss is 7.5534996949678685 with std:12.233329142899988.\n",
      "7.5534996949678685 0.0033459891205499747\n",
      "The training loss is 5.19670501199186 with std:8.02489706403918. The val loss is 10.919291932436945 with std:30.380433567301832.\n",
      "10.919291932436945 0.0033459891205499747\n",
      "Evaluating for {'lmda': 0.0033769803108250913} ...\n",
      "The training loss is 5.522712883409359 with std:12.527393296616609. The val loss is 2664.272611020079 with std:25337.19734772408.\n",
      "2664.272611020079 0.0033769803108250913\n",
      "The training loss is 5.8852102669145605 with std:10.851271459150329. The val loss is 10.228666313965627 with std:29.87558156012507.\n",
      "10.228666313965627 0.0033769803108250913\n",
      "The training loss is 6.042033784158885 with std:12.863357298982613. The val loss is 7.555116273898824 with std:12.22692229919561.\n",
      "7.555116273898824 0.0033769803108250913\n",
      "The training loss is 5.198708225177088 with std:8.02676695312782. The val loss is 10.919270435930459 with std:30.379827813034403.\n",
      "10.919270435930459 0.0033769803108250913\n",
      "Evaluating for {'lmda': 0.003408258547423452} ...\n",
      "The training loss is 5.525053858636735 with std:12.531554831182188. The val loss is 2646.9419774478843 with std:25171.83000173014.\n",
      "2646.9419774478843 0.003408258547423452\n",
      "The training loss is 5.887187184260402 with std:10.853248169649659. The val loss is 10.239346433907341 with std:29.96008748885473.\n",
      "10.239346433907341 0.003408258547423452\n",
      "The training loss is 6.0441039165354855 with std:12.866628772866395. The val loss is 7.55672881194212 with std:12.220544285410783.\n",
      "7.55672881194212 0.003408258547423452\n",
      "The training loss is 5.200707399102502 with std:8.028633353106214. The val loss is 10.919226302733838 with std:30.37913900995816.\n",
      "10.919226302733838 0.003408258547423452\n",
      "Evaluating for {'lmda': 0.0034398264890229246} ...\n",
      "The training loss is 5.527394702690542 with std:12.53569664009331. The val loss is 2629.593392810802 with std:25006.29124974134.\n",
      "2629.593392810802 0.0034398264890229246\n",
      "The training loss is 5.889156189859262 with std:10.85521273031992. The val loss is 10.250006299463685 with std:30.044502984174734.\n",
      "10.250006299463685 0.0034398264890229246\n",
      "The training loss is 6.046169470726933 with std:12.869887445760856. The val loss is 7.55833723408268 with std:12.214195195122384.\n",
      "7.55833723408268 0.0034398264890229246\n",
      "The training loss is 5.202702501208671 with std:8.030496245104718. The val loss is 10.919159273634925 with std:30.378365965557315.\n",
      "10.919159273634925 0.0034398264890229246\n",
      "Evaluating for {'lmda': 0.0034716868189265592} ...\n",
      "The training loss is 5.529735365233237 with std:12.539818548986121. The val loss is 2612.2283707838337 with std:24840.59554658502.\n",
      "2612.2283707838337 0.0034716868189265592\n",
      "The training loss is 5.8911172798806275 with std:10.857165144419993. The val loss is 10.260645642764253 with std:30.128824615428396.\n",
      "10.260645642764253 0.0034716868189265592\n",
      "The training loss is 6.048230409186701 with std:12.87313324298827. The val loss is 7.559941464635711 with std:12.207875109785816.\n",
      "7.559941464635711 0.0034716868189265592\n",
      "The training loss is 5.204693499510447 with std:8.032355610490969. The val loss is 10.919069089581084 with std:30.37750748851468.\n",
      "10.919069089581084 0.0034716868189265592\n",
      "Evaluating for {'lmda': 0.003503842245290676} ...\n",
      "The training loss is 5.532075795979267 with std:12.54392038505148. The val loss is 2594.8484240166886 with std:24674.7573371206.\n",
      "2594.8484240166886 0.003503842245290676\n",
      "The training loss is 5.893070450974454 with std:10.859105415501634. The val loss is 10.271264202071034 with std:30.21304901154901.\n",
      "10.271264202071034 0.003503842245290676\n",
      "The training loss is 6.050286694693288 with std:12.876366090408936. The val loss is 7.561541427370972 with std:12.20158409922983.\n",
      "7.561541427370972 0.003503842245290676\n",
      "The training loss is 5.206680362526624 with std:8.034211430833416. The val loss is 10.91895549177262 with std:30.37656238899107.\n",
      "10.91895549177262 0.003503842245290676\n",
      "Evaluating for {'lmda': 0.0035362955013550426} ...\n",
      "The training loss is 5.534415944729752 with std:12.548001977050285. The val loss is 2577.4550640194325 with std:24508.791055146743.\n",
      "2577.4550640194325 0.0035362955013550426\n",
      "The training loss is 5.895015700353452 with std:10.861033547508761. The val loss is 10.281861721906802 with std:30.297172861995804.\n",
      "10.281861721906802 0.0035362955013550426\n",
      "The training loss is 6.052338290413886 with std:12.879585914485054. The val loss is 7.563137045528685 with std:12.195322221680998.\n",
      "7.563137045528685 0.0035362955013550426\n",
      "The training loss is 5.208663059353207 with std:8.03606368796672. The val loss is 10.918818221602496 with std:30.375529478448318.\n",
      "10.918818221602496 0.0035362955013550426\n",
      "Evaluating for {'lmda': 0.0035690493456752297} ...\n",
      "The training loss is 5.536755761360637 with std:12.552063155346902. The val loss is 2560.049800724401 with std:24342.711119224397.\n",
      "2560.049800724401 0.0035690493456752297\n",
      "The training loss is 5.896953025722666 with std:10.86294954470361. The val loss is 10.292437953075007 with std:30.381192917356397.\n",
      "10.292437953075007 0.0035690493456752297\n",
      "The training loss is 6.0543851598390175 with std:12.88279264221163. The val loss is 7.564728241739606 with std:12.189089523745638.\n",
      "7.564728241739606 0.0035690493456752297\n",
      "The training loss is 5.21064155965016 with std:8.037912363960867. The val loss is 10.918657020918387 with std:30.374407571057947.\n",
      "10.918657020918387 0.0035690493456752297\n",
      "Evaluating for {'lmda': 0.00360210656235707} ...\n",
      "The training loss is 5.53909519584389 with std:12.5561037519121. The val loss is 2542.6341423773333 with std:24176.5319316357.\n",
      "2542.6341423773333 0.00360210656235707\n",
      "The training loss is 5.898882425312716 with std:10.864853411667523. The val loss is 10.302992652609714 with std:30.465105988883938.\n",
      "10.302992652609714 0.00360210656235707\n",
      "The training loss is 6.056427266844176 with std:12.885986201160785. The val loss is 7.5663149381958315 with std:12.182886040771423.\n",
      "7.5663149381958315 0.00360210656235707\n",
      "The training loss is 5.212615833648059 with std:8.039757441185428. The val loss is 10.918471631814059 with std:30.373195482421963.\n",
      "10.918471631814059 0.00360210656235707\n",
      "Evaluating for {'lmda': 0.0036354699612933176} ...\n",
      "The training loss is 5.5414341982445965 with std:12.56012360031205. The val loss is 2525.2095952190234 with std:24010.267875348058.\n",
      "2525.2095952190234 0.0036354699612933176\n",
      "The training loss is 5.90080389786826 with std:10.866745153302983. The val loss is 10.313525583851744 with std:30.548908949139303.\n",
      "10.313525583851744 0.0036354699612933176\n",
      "The training loss is 6.058464575677547 with std:12.889166519491642. The val loss is 7.56789705654931 with std:12.176711796590503.\n",
      "7.56789705654931 0.0036354699612933176\n",
      "The training loss is 5.214585852184577 with std:8.041598902292435. The val loss is 10.918261796842726 with std:30.37189203089968.\n",
      "10.918261796842726 0.0036354699612933176\n",
      "Evaluating for {'lmda': 0.003669142378402494} ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 5.543772718744109 with std:12.564122535775688. The val loss is 2507.777663268801 with std:23843.93331194987.\n",
      "2507.777663268801 0.003669142378402494\n",
      "The training loss is 5.902717442645419 with std:10.86862477485323. The val loss is 10.324036516336482 with std:30.632598731530603.\n",
      "10.324036516336482 0.003669142378402494\n",
      "The training loss is 6.060497050974394 with std:12.89233352596124. The val loss is 7.569474517996403 with std:12.170566803941979.\n",
      "7.569474517996403 0.003669142378402494\n",
      "The training loss is 5.216551586689275 with std:8.043436730231862. The val loss is 10.918027258966683 with std:30.370496037235984.\n",
      "10.918027258966683 0.003669142378402494\n",
      "Evaluating for {'lmda': 0.003703126675869927} ...\n",
      "The training loss is 5.546110707645651 with std:12.568100395168656. The val loss is 2490.3398480106252 with std:23677.542578653796.\n",
      "2490.3398480106252 0.003703126675869927\n",
      "The training loss is 5.9046230594235825 with std:10.870492281845053. The val loss is 10.33452522598498 with std:30.716172331650696.\n",
      "10.33452522598498 0.003703126675869927\n",
      "The training loss is 6.062524657751356 with std:12.895487149902216. The val loss is 7.571047243272367 with std:12.1644510644387.\n",
      "7.571047243272367 0.003703126675869927\n",
      "The training loss is 5.2185130092109 with std:8.045270908302175. The val loss is 10.917767761755481 with std:30.369006325652006.\n",
      "10.917767761755481 0.003703126675869927\n",
      "Evaluating for {'lmda': 0.0037374257423910644} ...\n",
      "The training loss is 5.548448115374564 with std:12.572057017020134. The val loss is 2472.897648187026 with std:23511.109986333584.\n",
      "2472.897648187026 0.0037374257423910644\n",
      "The training loss is 5.906520748467888 with std:10.872347680136993. The val loss is 10.344991494841237 with std:30.799626805398397.\n",
      "10.344991494841237 0.0037374257423910644\n",
      "The training loss is 6.064547361432197 with std:12.898627321259985. The val loss is 7.572615152683942 with std:12.158364568711818.\n",
      "7.572615152683942 0.0037374257423910644\n",
      "The training loss is 5.220470092428678 with std:8.047101420127547. The val loss is 10.917483049201797 with std:30.36742172284772.\n",
      "10.917483049201797 0.0037374257423910644\n",
      "Evaluating for {'lmda': 0.0037720424934169976} ...\n",
      "The training loss is 5.550784892497748 with std:12.575992241545158. The val loss is 2455.4525595212403 with std:23344.64981687125.\n",
      "2455.4525595212403 0.0037720424934169976\n",
      "The training loss is 5.908410510567541 with std:10.874190975878577. The val loss is 10.355435111217476 with std:30.882959270195812.\n",
      "10.355435111217476 0.0037720424934169976\n",
      "The training loss is 6.066565127854478 with std:12.901753970605082. The val loss is 7.5741781661629615 with std:12.152307296605311.\n",
      "7.5741781661629615 0.0037720424934169976\n",
      "The training loss is 5.222422809656748 with std:8.048928249691375. The val loss is 10.917172865999625 with std:30.36574105942289.\n",
      "10.917172865999625 0.0037720424934169976\n",
      "Evaluating for {'lmda': 0.003806979871402284} ...\n",
      "The training loss is 5.553120989722336 with std:12.579905910616022. The val loss is 2438.0060744389157 with std:23178.176320504353.\n",
      "2438.0060744389157 0.003806979871402284\n",
      "The training loss is 5.91029234700744 with std:10.876022175528659. The val loss is 10.365855869689547 with std:30.966166905225037.\n",
      "10.365855869689547 0.003806979871402284\n",
      "The training loss is 6.0685779232539385 with std:12.904867029085024. The val loss is 7.575736203208842 with std:12.146279217159549.\n",
      "7.575736203208842 0.003806979871402284\n",
      "The training loss is 5.224371134855588 with std:8.050751381354312. The val loss is 10.916836957449165 with std:30.363963169499705.\n",
      "10.916836957449165 0.003806979871402284\n",
      "Evaluating for {'lmda': 0.003842240846055061} ...\n",
      "The training loss is 5.555456357917111 with std:12.583797867834415. The val loss is 2420.559681870876 with std:23011.703713943396.\n",
      "2420.559681870876 0.003842240846055061\n",
      "The training loss is 5.912166259575211 with std:10.877841285838125. The val loss is 10.376253571129677 with std:31.04924695135312.\n",
      "10.376253571129677 0.003842240846055061\n",
      "The training loss is 6.0705857143108455 with std:12.90796642853347. The val loss is 7.577289183014332 with std:12.140280288858612.\n",
      "7.577289183014332 0.003842240846055061\n",
      "The training loss is 5.226315042658776 with std:8.052570799873745. The val loss is 10.91647506947402 with std:30.36208689078821.\n",
      "10.91647506947402 0.003842240846055061\n",
      "Evaluating for {'lmda': 0.003877828414589457} ...\n",
      "The training loss is 5.557790948116069 with std:12.587667958516594. The val loss is 2403.1148668897677 with std:22845.246176905774.\n",
      "2403.1148668897677 0.003877828414589457\n",
      "The training loss is 5.914032250562727 with std:10.87964831384482. The val loss is 10.386628022480751 with std:31.132196709850923.\n",
      "10.386628022480751 0.003877828414589457\n",
      "The training loss is 6.072588468116023 with std:12.911052101348762. The val loss is 7.57883702438331 with std:12.134310459664748.\n",
      "7.57883702438331 0.003877828414589457\n",
      "The training loss is 5.228254508363785 with std:8.054386490409634. The val loss is 10.916086948846747 with std:30.360111065618533.\n",
      "10.916086948846747 0.003877828414589457\n",
      "Evaluating for {'lmda': 0.003913745601980384} ...\n",
      "The training loss is 5.560124711520812 with std:12.591516029697265. The val loss is 2385.673110557114 with std:22678.817850656797.\n",
      "2385.673110557114 0.003913745601980384\n",
      "The training loss is 5.9158903227573365 with std:10.881443266878144. The val loss is 10.396979037040774 with std:31.215013544514537.\n",
      "10.396979037040774 0.003913745601980384\n",
      "The training loss is 6.074586152215456 with std:12.914123980620248. The val loss is 7.580379645810251 with std:12.128369667118708.\n",
      "7.580379645810251 0.003913745601980384\n",
      "The training loss is 5.230189507952952 with std:8.056198438545035. The val loss is 10.91567234306571 with std:30.358034540540267.\n",
      "10.91567234306571 0.003913745601980384\n",
      "Evaluating for {'lmda': 0.003949995461220643} ...\n",
      "The training loss is 5.562457599509436 with std:12.595341930098998. The val loss is 2368.2358895782804 with std:22512.432834718904.\n",
      "2368.2358895782804 0.003949995461220643\n",
      "The training loss is 5.9177404794316635 with std:10.883226152551753. The val loss is 10.407306434228664 with std:31.297694879802958.\n",
      "10.407306434228664 0.003949995461220643\n",
      "The training loss is 6.0765787345899875 with std:12.917182000049356. The val loss is 7.581916965528772 with std:12.122457838759104.\n",
      "7.581916965528772 0.003949995461220643\n",
      "The training loss is 5.2321200181013054 with std:8.058006630302687. The val loss is 10.915231000415476 with std:30.355856166444873.\n",
      "10.915231000415476 0.003949995461220643\n",
      "Evaluating for {'lmda': 0.003986581073580439} ...\n",
      "The training loss is 5.564789563672631 with std:12.599145510275054. The val loss is 2350.8046760923403 with std:22346.105184868535.\n",
      "2350.8046760923403 0.003986581073580439\n",
      "The training loss is 5.919582724372551 with std:10.884996978754245. The val loss is 10.417610039713121 with std:31.380238202033304.\n",
      "10.417610039713121 0.003986581073580439\n",
      "The training loss is 6.078566183691652 with std:12.920226094024766. The val loss is 7.583448901462173 with std:12.116574891743783.\n",
      "7.583448901462173 0.003986581073580439\n",
      "The training loss is 5.234046016196406 with std:8.059811052163694. The val loss is 10.914762670152612 with std:30.35357479965695.\n",
      "10.914762670152612 0.003986581073580439\n",
      "Evaluating for {'lmda': 0.004023505548869293} ...\n",
      "The training loss is 5.567120555780792 with std:12.602926622494481. The val loss is 2333.3809373775243 with std:22179.848910322817.\n",
      "2333.3809373775243 0.004023505548869293\n",
      "The training loss is 5.921417061836324 with std:10.88675575365444. The val loss is 10.427889685230102 with std:31.462641058025905.\n",
      "10.427889685230102 0.004023505548869293\n",
      "The training loss is 6.080548468414003 with std:12.923256197547554. The val loss is 7.584975371291768 with std:12.11072073342956.\n",
      "7.584975371291768 0.004023505548869293\n",
      "The training loss is 5.23596748032787 with std:8.06161169108282. The val loss is 10.914267102344443 with std:30.351189301082904.\n",
      "10.914267102344443 0.004023505548869293\n",
      "Evaluating for {'lmda': 0.004060772025700365} ...\n",
      "The training loss is 5.569450527824133 with std:12.606685120770845. The val loss is 2315.9661356212223 with std:22013.677971549674.\n",
      "2315.9661356212223 0.004060772025700365\n",
      "The training loss is 5.923243496585338 with std:10.888502485714724. The val loss is 10.438145208764812 with std:31.54490105640182.\n",
      "10.438145208764812 0.004060772025700365\n",
      "The training loss is 6.082525558140442 with std:12.9262722463363. The val loss is 7.5864962924790715 with std:12.104895261351238.\n",
      "7.5864962924790715 0.004060772025700365\n",
      "The training loss is 5.237884389324786 with std:8.063408534507712. The val loss is 10.913744048104675 with std:30.348698537376304.\n",
      "10.913744048104675 0.004060772025700365\n",
      "Evaluating for {'lmda': 0.0040983836717572615} ...\n",
      "The training loss is 5.571779432009235 with std:12.610420860948103. The val loss is 2298.56172761691 with std:21847.606277373707.\n",
      "2298.56172761691 0.0040983836717572615\n",
      "The training loss is 5.92506203386015 with std:10.890237183616552. The val loss is 10.448376454455282 with std:31.62701586696364.\n",
      "10.448376454455282 0.0040983836717572615\n",
      "The training loss is 6.084497422716493 with std:12.929274176741462. The val loss is 7.588011582269712 with std:12.099098363266782.\n",
      "7.588011582269712 0.0040983836717572615\n",
      "The training loss is 5.239796722744087 with std:8.065201570386607. The val loss is 10.913193259507182 with std:30.346101380571536.\n",
      "10.913193259507182 0.0040983836717572615\n",
      "Evaluating for {'lmda': 0.004136343684063274} ...\n",
      "The training loss is 5.57410722076659 with std:12.614133700604292. The val loss is 2281.169164535221 with std:21681.647682795847.\n",
      "2281.169164535221 0.004136343684063274\n",
      "The training loss is 5.92687267938836 with std:10.891959856357143. The val loss is 10.458583272436053 with std:31.708983219507854.\n",
      "10.458583272436053 0.004136343684063274\n",
      "The training loss is 6.086464032483677 with std:12.932261925816755. The val loss is 7.589521157708928 with std:12.09332991738196.\n",
      "7.589521157708928 0.004136343684063274\n",
      "The training loss is 5.241704460897064 with std:8.06699078719186. The val loss is 10.912614489710055 with std:30.34339670876111.\n",
      "10.912614489710055 0.004136343684063274\n",
      "Evaluating for {'lmda': 0.004174655289253135} ...\n",
      "The training loss is 5.576433846762168 with std:12.617823499140764. The val loss is 2263.7898916339914 with std:21515.815986223864.\n",
      "2263.7898916339914 0.004174655289253135\n",
      "The training loss is 5.928675439378331 with std:10.893670513161753. The val loss is 10.468765519117547 with std:31.790800906046133.\n",
      "10.468765519117547 0.004174655289253135\n",
      "The training loss is 6.088425358267912 with std:12.93523543127667. The val loss is 7.5910249357053425 with std:12.087589792555764.\n",
      "7.5910249357053425 0.004174655289253135\n",
      "The training loss is 5.243607584836497 with std:8.068776173924602. The val loss is 10.912007492969401 with std:30.340583406045987.\n",
      "10.912007492969401 0.004174655289253135\n",
      "Evaluating for {'lmda': 0.004213321743847289} ...\n",
      "The training loss is 5.57875926291595 with std:12.6214901177837. The val loss is 2246.4253480466737 with std:21350.124927458135.\n",
      "2246.4253480466737 0.004213321743847289\n",
      "The training loss is 5.930470320523829 with std:10.895369163537541. The val loss is 10.478923056837637 with std:31.872466778064847.\n",
      "10.478923056837637 0.004213321743847289\n",
      "The training loss is 6.090381371407835 with std:12.938194631576344. The val loss is 7.5925228330158365 with std:12.081877848303748.\n",
      "7.5925228330158365 0.004213321743847289\n",
      "The training loss is 5.245506076396919 with std:8.070557720152804. The val loss is 10.911372024653703 with std:30.337660362756324.\n",
      "10.911372024653703 0.004213321743847289\n",
      "Evaluating for {'lmda': 0.004252346334528682} ...\n",
      "The training loss is 5.581083422394362 with std:12.625133419535137. The val loss is 2229.07696646454 with std:21184.588184657132.\n",
      "2229.07696646454 0.004252346334528682\n",
      "The training loss is 5.932257329996269 with std:10.897055817241249. The val loss is 10.489055754138697 with std:31.953978748580145.\n",
      "10.489055754138697 0.004252346334528682\n",
      "The training loss is 6.092332043732956 with std:12.941139465813558. The val loss is 7.594014766252251 with std:12.076193934975405.\n",
      "7.594014766252251 0.004252346334528682\n",
      "The training loss is 5.247399918173549 with std:8.072335416009828. The val loss is 10.910707841355666 with std:30.33462647595571.\n",
      "10.910707841355666 0.004252346334528682\n",
      "Evaluating for {'lmda': 0.004291732378422158} ...\n",
      "The training loss is 5.5834062786211165 with std:12.628753269236302. The val loss is 2211.7461728846474 with std:21019.21937193507.\n",
      "2211.7461728846474 0.004291732378422158\n",
      "The training loss is 5.934036475442613 with std:10.89873048427706. The val loss is 10.49916348541905 with std:32.035334789687234.\n",
      "10.49916348541905 0.004291732378422158\n",
      "The training loss is 6.0942773476006025 with std:12.944069873819709. The val loss is 7.595500651951272 with std:12.07053789394387.\n",
      "7.595500651951272 0.004291732378422158\n",
      "The training loss is 5.249289093557383 with std:8.074109252210631. The val loss is 10.910014700866494 with std:30.331480649415163.\n",
      "10.910014700866494 0.004291732378422158\n",
      "Evaluating for {'lmda': 0.004331483223376399} ...\n",
      "The training loss is 5.5857277853050356 with std:12.632349533574102. The val loss is 2194.4343863975023 with std:20854.032037336237.\n",
      "2194.4343863975023 0.004331483223376399\n",
      "The training loss is 5.935807764984841 with std:10.900393174913054. The val loss is 10.509246131290233 with std:32.116532935170135.\n",
      "10.509246131290233 0.004331483223376399\n",
      "The training loss is 6.096217255884872 with std:12.946985796130134. The val loss is 7.596980406552462 with std:12.064909557691921.\n",
      "7.596980406552462 0.004331483223376399\n",
      "The training loss is 5.251173586732016 with std:8.07587922007548. The val loss is 10.909292362306793 with std:30.3282217941137.\n",
      "10.909292362306793 0.004331483223376399\n",
      "Evaluating for {'lmda': 0.004371602248248502} ...\n",
      "The training loss is 5.588047896418766 with std:12.635922081035636. The val loss is 2177.1430188649556 with std:20689.03965976345.\n",
      "2177.1430188649556 0.004371602248248502\n",
      "The training loss is 5.937571207221666 with std:10.902043899657135. The val loss is 10.519303578277784 with std:32.197571278140764.\n",
      "10.519303578277784 0.004371602248248502\n",
      "The training loss is 6.098151741987826 with std:12.949887174008502. The val loss is 7.598453946468786 with std:12.059308750119303.\n",
      "7.598453946468786 0.004371602248248502\n",
      "The training loss is 5.253053382675988 with std:8.07764531153675. The val loss is 10.908540586080623 with std:30.324848828249536.\n",
      "10.908540586080623 0.004371602248248502\n",
      "Evaluating for {'lmda': 0.00441209286319119} ...\n",
      "The training loss is 5.590366566231935 with std:12.639470781985304. The val loss is 2159.8734747109293 with std:20524.255646979873.\n",
      "2159.8734747109293 0.00441209286319119\n",
      "The training loss is 5.9393268112149205 with std:10.903682669266724. The val loss is 10.529335718810097 with std:32.2784479710224.\n",
      "10.529335718810097 0.00441209286319119\n",
      "The training loss is 6.100080779853315 with std:12.952773949429075. The val loss is 7.599921188042135 with std:12.053735286325992.\n",
      "7.599921188042135 0.00441209286319119\n",
      "The training loss is 5.254928467184843 with std:8.07940751916359. The val loss is 10.90775913395998 with std:30.321360677521916.\n",
      "10.90775913395998 0.00441209286319119\n",
      "Evaluating for {'lmda': 0.004452958509942656} ...\n",
      "The training loss is 5.592683749317196 with std:12.642995508641114. The val loss is 2142.627150651687 with std:20359.693333039722.\n",
      "2142.627150651687 0.004452958509942656\n",
      "The training loss is 5.941074586504318 with std:10.90530949474303. The val loss is 10.539342451348265 with std:32.35916122659346.\n",
      "10.539342451348265 0.004452958509942656\n",
      "The training loss is 6.102004343963335 with std:12.955646065095692. The val loss is 7.601382047606639 with std:12.048188973119805.\n",
      "7.601382047606639 0.004452958509942656\n",
      "The training loss is 5.256798826878049 with std:8.081165836173763. The val loss is 10.906947769143299 with std:30.31775627542749.\n",
      "10.906947769143299 0.004452958509942656\n",
      "Evaluating for {'lmda': 0.004494202662119142} ...\n",
      "The training loss is 5.594999400536423 with std:12.646496135045165. The val loss is 2125.405435404434 with std:20195.365975505254.\n",
      "2125.405435404434 0.004494202662119142\n",
      "The training loss is 5.942814543091502 with std:10.90692438732929. The val loss is 10.549323680241546 with std:32.43970931660932.\n",
      "10.549323680241546 0.004494202662119142\n",
      "The training loss is 6.103922409344267 with std:12.958503464435704. The val loss is 7.602836441502442 with std:12.042669608975828.\n",
      "7.602836441502442 0.004494202662119142\n",
      "The training loss is 5.258664449195607 with std:8.08292025643882. The val loss is 10.90610625629947 with std:30.314034563654303.\n",
      "10.90610625629947 0.004494202662119142\n",
      "Evaluating for {'lmda': 0.004535828825510187} ...\n",
      "The training loss is 5.597313475081303 with std:12.649972537135843. The val loss is 2108.2097095028976 with std:20031.286753691333.\n",
      "2108.2097095028976 0.004535828825510187\n",
      "The training loss is 5.9445466914459 with std:10.908527358504644. The val loss is 10.559279315693434 with std:32.520090571816574.\n",
      "10.559279315693434 0.004535828825510187\n",
      "The training loss is 6.105834951594292 with std:12.96134609164754. The val loss is 7.604284286114831 with std:12.037176984291213.\n",
      "7.604284286114831 0.004535828825510187\n",
      "The training loss is 5.260525322420166 with std:8.084670774522024. The val loss is 10.905234361530837 with std:30.31019449181644.\n",
      "10.905234361530837 0.004535828825510187\n",
      "Evaluating for {'lmda': 0.004577840538376616} ...\n",
      "The training loss is 5.599625928454382 with std:12.65342459270687. The val loss is 2091.041344965415 with std:19867.468765496247.\n",
      "2091.041344965415 0.004577840538376616\n",
      "The training loss is 5.946271042497062 with std:10.91011842000191. The val loss is 10.569209273834609 with std:32.60030338225843.\n",
      "10.569209273834609 0.004577840538376616\n",
      "The training loss is 6.10774194686152 with std:12.964173891641451. The val loss is 7.605725497838479 with std:12.031710881377565.\n",
      "7.605725497838479 0.004577840538376616\n",
      "The training loss is 5.262381435678486 with std:8.086417385668693. The val loss is 10.904331852536698 with std:30.306235018219287.\n",
      "10.904331852536698 0.004577840538376616\n",
      "Evaluating for {'lmda': 0.004620241371751313} ...\n",
      "The training loss is 5.6019367164888685 with std:12.656852181421222. The val loss is 2073.90170511449 with std:19703.925025683682.\n",
      "2073.90170511449 0.004620241371751313\n",
      "The training loss is 5.947987607637078 with std:10.91169758376708. The val loss is 10.579113476587573 with std:32.680346196316044.\n",
      "10.579113476587573 0.004620241371751313\n",
      "The training loss is 6.109643371864968 with std:12.96698681009061. The val loss is 7.607159993147017 with std:12.026271074700404.\n",
      "7.607159993147017 0.004620241371751313\n",
      "The training loss is 5.264232778949857 with std:8.088160085839778. The val loss is 10.903398498618806 with std:30.302155110005824.\n",
      "10.903398498618806 0.004620241371751313\n",
      "Evaluating for {'lmda': 0.0046630349297427315} ...\n",
      "The training loss is 5.604245795365578 with std:12.660255184838569. The val loss is 2056.7921442781385 with std:19540.668463032962.\n",
      "2056.7921442781385 0.0046630349297427315\n",
      "The training loss is 5.94969639871972 with std:10.913264861987502. The val loss is 10.588991851621728 with std:32.760217520250585.\n",
      "10.588991851621728 0.0046630349297427315\n",
      "The training loss is 6.111539203909028 with std:12.96978479344108. The val loss is 7.6085876886277815 with std:12.020857331125633.\n",
      "7.6085876886277815 0.0046630349297427315\n",
      "The training loss is 5.2660793430769415 with std:8.089898871716631. The val loss is 10.902434070616907 with std:30.297953742922054.\n",
      "10.902434070616907 0.0046630349297427315\n",
      "Evaluating for {'lmda': 0.004706224849841282} ...\n",
      "The training loss is 5.606553121606995 with std:12.66363348640215. The val loss is 2039.7140075501254 with std:19377.71191805175.\n",
      "2039.7140075501254 0.004706224849841282\n",
      "The training loss is 5.951397428050153 with std:10.914820267096976. The val loss is 10.598844332469858 with std:32.839915919134384.\n",
      "10.598844332469858 0.004706224849841282\n",
      "The training loss is 6.113429420870595 with std:12.972567788863179. The val loss is 7.6100085009225324 with std:12.015469409782135.\n",
      "7.6100085009225324 0.004706224849841282\n",
      "The training loss is 5.267921119767421 with std:8.091633740714808. The val loss is 10.901438341148019 with std:30.293629902252864.\n",
      "10.901438341148019 0.004706224849841282\n",
      "Evaluating for {'lmda': 0.0047498148032285} ...\n",
      "The training loss is 5.608858652088218 with std:12.666986971423247. The val loss is 2022.66863058997 with std:19215.068141070144.\n",
      "2022.66863058997 0.0047498148032285\n",
      "The training loss is 5.953090708399835 with std:10.916363811742274. The val loss is 10.6086708583473 with std:32.91944001536258.\n",
      "10.6086708583473 0.0047498148032285\n",
      "The training loss is 6.115314001220168 with std:12.975335744330632. The val loss is 7.611422346840141 with std:12.010107062595567.\n",
      "7.611422346840141 0.0047498148032285\n",
      "The training loss is 5.269758101605017 with std:8.093364691011734. The val loss is 10.900411084474207 with std:30.289182582505394.\n",
      "10.900411084474207 0.0047498148032285\n",
      "Evaluating for {'lmda': 0.004793808495089107} ...\n",
      "The training loss is 5.6111623440509675 with std:12.670315527125041. The val loss is 2005.657339302269 with std:19052.749789180983.\n",
      "2005.657339302269 0.004793808495089107\n",
      "The training loss is 5.9547762529848205 with std:10.917895508817146. The val loss is 10.61847137418682 with std:32.99878848883342.\n",
      "10.61847137418682 0.004793808495089107\n",
      "The training loss is 6.11719292402319 with std:12.978088608561396. The val loss is 7.612829143313978 with std:12.004770034053035.\n",
      "7.612829143313978 0.004793808495089107\n",
      "The training loss is 5.271590282061127 with std:8.095091721541769. The val loss is 10.899352076609574 with std:30.28461078783032.\n",
      "10.899352076609574 0.004793808495089107\n",
      "Evaluating for {'lmda': 0.004838209664925958} ...\n",
      "The training loss is 5.613464155100319 with std:12.673619042609348. The val loss is 1988.681449657783 with std:18890.769424534035.\n",
      "1988.681449657783 0.004838209664925958\n",
      "The training loss is 5.956454075471426 with std:10.91941537141502. The val loss is 10.628245830615077 with std:33.077960076782425.\n",
      "10.628245830615077 0.004838209664925958\n",
      "The training loss is 6.119066168941915 with std:12.980826331056928. The val loss is 7.614228807444892 with std:11.999458061635206.\n",
      "7.614228807444892 0.004838209664925958\n",
      "The training loss is 5.273417655491029 with std:8.096814832020042. The val loss is 10.898261095434258 with std:30.279913532440105.\n",
      "10.898261095434258 0.004838209664925958\n",
      "Evaluating for {'lmda': 0.00488302208687788} ...\n",
      "The training loss is 5.615764043235318 with std:12.676897408912273. The val loss is 1971.742267422492 with std:18729.139511752102.\n",
      "1971.742267422492 0.00488302208687788\n",
      "The training loss is 5.958124189986885 with std:10.920923412896778. The val loss is 10.637994183852028 with std:33.15695357292191.\n",
      "10.637994183852028 0.00488302208687788\n",
      "The training loss is 6.120933716248823 with std:12.983548862101646. The val loss is 7.615621256506601 with std:11.994170875753627.\n",
      "7.615621256506601 0.00488302208687788\n",
      "The training loss is 5.275240217154843 with std:8.0985340229749. The val loss is 10.897137920594588 with std:30.27508984043676.\n",
      "10.897137920594588 0.00488302208687788\n",
      "Evaluating for {'lmda': 0.004928249570040513} ...\n",
      "The training loss is 5.618061966824228 with std:12.68015051893076. The val loss is 1954.8410879185233 with std:18567.872415650505.\n",
      "1954.8410879185233 0.004928249570040513\n",
      "The training loss is 5.959786611095295 with std:10.92241964681616. The val loss is 10.64771639576027 with std:33.235767827725894.\n",
      "10.64771639576027 0.004928249570040513\n",
      "The training loss is 6.122795546832561 with std:12.986256152765524. The val loss is 7.617006408002724 with std:11.988908200073435.\n",
      "7.617006408002724 0.004928249570040513\n",
      "The training loss is 5.277057963206381 with std:8.100249295712942. The val loss is 10.895982333658512 with std:30.270138746422155.\n",
      "10.895982333658512 0.004928249570040513\n",
      "Evaluating for {'lmda': 0.004973895958790063} ...\n",
      "The training loss is 5.620357884648463 with std:12.683378267515119. The val loss is 1937.9791958328735 with std:18406.98039941336.\n",
      "1937.9791958328735 0.004973895958790063\n",
      "The training loss is 5.961441353812182 with std:10.92390408696599. The val loss is 10.657412433746211 with std:33.31440174767967.\n",
      "10.657412433746211 0.004973895958790063\n",
      "The training loss is 6.124651642191135 with std:12.988948154910155. The val loss is 7.618384179614044 with std:11.983669751548245.\n",
      "7.618384179614044 0.004973895958790063\n",
      "The training loss is 5.278870890718503 with std:8.101960652393988. The val loss is 10.894794118092493 with std:30.26505929539782.\n",
      "10.894794118092493 0.004973895958790063\n",
      "Evaluating for {'lmda': 0.005019965133110079} ...\n",
      "The training loss is 5.622651755867002 with std:12.68658055135222. The val loss is 1921.1578649062237 with std:18246.47562162608.\n",
      "1921.1578649062237 0.005019965133110079\n",
      "The training loss is 5.963088433593295 with std:10.92537674735145. The val loss is 10.667082270773484 with std:33.39285429523824.\n",
      "10.667082270773484 0.005019965133110079\n",
      "The training loss is 6.126501984448342 with std:12.991624821174057. The val loss is 7.619754489301302 with std:11.978455240646053.\n",
      "7.619754489301302 0.005019965133110079\n",
      "The training loss is 5.280678997671731 with std:8.103668095988606. The val loss is 10.893573059353146 with std:30.259850543305014.\n",
      "10.893573059353146 0.005019965133110079\n",
      "Evaluating for {'lmda': 0.005066461008921269} ...\n",
      "The training loss is 5.624943540071399 with std:12.689757269120676. The val loss is 1904.3783577786412 with std:18086.37013480269.\n",
      "1904.3783577786412 0.005066461008921269\n",
      "The training loss is 5.964727866343177 with std:10.926837642218178. The val loss is 10.676725885268551 with std:33.471124488082154.\n",
      "10.676725885268551 0.005066461008921269\n",
      "The training loss is 6.128346556363434 with std:12.994286105013213. The val loss is 7.6211172552357285 with std:11.973264371301312.\n",
      "7.6211172552357285 0.005066461008921269\n",
      "The training loss is 5.282482282980096 with std:8.105371630345113. The val loss is 10.892318944883066 with std:30.254511556950188.\n",
      "10.892318944883066 0.005066461008921269\n",
      "Evaluating for {'lmda': 0.005113387538414326} ...\n",
      "The training loss is 5.627233197258219 with std:12.69290832135949. The val loss is 1887.641925718046 with std:17926.675882797914.\n",
      "1887.641925718046 0.005113387538414326\n",
      "The training loss is 5.966359668398738 with std:10.92828678602317. The val loss is 10.68634326115876 with std:33.549211399277446.\n",
      "10.68634326115876 0.005113387538414326\n",
      "The training loss is 6.130185341320985 with std:12.996931960662714. The val loss is 7.622472395910987 with std:11.968096841411834.\n",
      "7.622472395910987 0.005113387538414326\n",
      "The training loss is 5.2842807464762 with std:8.10707126015278. The val loss is 10.891031564219391 with std:30.249041414656865.\n",
      "10.891031564219391 0.005113387538414326\n",
      "Evaluating for {'lmda': 0.005160748710385908} ...\n",
      "The training loss is 5.629520687849734 with std:12.696033610523278. The val loss is 1870.9498084425156 with std:17767.40469911129.\n",
      "1870.9498084425156 0.005160748710385908\n",
      "The training loss is 5.967983856544615 with std:10.929724193444372. The val loss is 10.695934387751901 with std:33.62711415631588.\n",
      "10.695934387751901 0.005160748710385908\n",
      "The training loss is 6.132018323356143 with std:12.99956234320666. The val loss is 7.623819830079516 with std:11.962952342675527.\n",
      "7.623819830079516 0.005160748710385908\n",
      "The training loss is 5.2860743889296975 with std:8.108766990982762. The val loss is 10.889710708909876 with std:30.24343920578075.\n",
      "10.889710708909876 0.005160748710385908\n",
      "Evaluating for {'lmda': 0.00520854855057766} ...\n",
      "The training loss is 5.63180597270299 with std:12.699133041008874. The val loss is 1854.3032338172118 with std:17608.568303997705.\n",
      "1854.3032338172118 0.00520854855057766\n",
      "The training loss is 5.969600447993047 with std:10.931149879382069. The val loss is 10.705499259774225 with std:33.70483194134297.\n",
      "10.705499259774225 0.00520854855057766\n",
      "The training loss is 6.13384548713579 with std:13.002177208481887. The val loss is 7.625159476803505 with std:11.957830560770661.\n",
      "7.625159476803505 0.00520854855057766\n",
      "The training loss is 5.287863212049778 with std:8.110458829302416. The val loss is 10.888356172707502 with std:30.237704031757378.\n",
      "10.888356172707502 0.00520854855057766\n",
      "Evaluating for {'lmda': 0.005256791122018419} ...\n",
      "The training loss is 5.6340890131106685 with std:12.70220651910264. The val loss is 1837.7034177376815 with std:17450.178303354376.\n",
      "1837.7034177376815 0.005256791122018419\n",
      "The training loss is 5.971209460397024 with std:10.932563858961252. The val loss is 10.715037877296217 with std:33.78236399047384.\n",
      "10.715037877296217 0.005256791122018419\n",
      "The training loss is 6.135666817981451 with std:13.004776513169467. The val loss is 7.626491255479184 with std:11.952731175707928.\n",
      "7.626491255479184 0.005256791122018419\n",
      "The training loss is 5.289647218491282 with std:8.112146782472227. The val loss is 10.886967751497824 with std:30.231835005687145.\n",
      "10.886967751497824 0.005256791122018419\n",
      "Evaluating for {'lmda': 0.005305480525369574} ...\n",
      "The training loss is 5.636369770815204 with std:12.705253953012605. The val loss is 1821.1515638322303 with std:17292.246185882275.\n",
      "1821.1515638322303 0.005305480525369574\n",
      "The training loss is 5.972810911839851 with std:10.933966147515239. The val loss is 10.724550245663217 with std:33.85970959312316.\n",
      "10.724550245663217 0.005305480525369574\n",
      "The training loss is 6.137482301875688 with std:13.007360214776357. The val loss is 7.627815085851366 with std:11.947653861759235.\n",
      "7.627815085851366 0.005305480525369574\n",
      "The training loss is 5.291426411860015 with std:8.113830858777444. The val loss is 10.885545243382584 with std:30.225831252840035.\n",
      "10.885545243382584 0.005305480525369574\n",
      "Evaluating for {'lmda': 0.005354620899273608} ...\n",
      "The training loss is 5.638648208008713 with std:12.708275252870797. The val loss is 1804.6488632910598 with std:17134.78332145733.\n",
      "1804.6488632910598 0.005354620899273608\n",
      "The training loss is 5.974404820838668 with std:10.935356760612498. The val loss is 10.734036375526289 with std:33.936868092317205.\n",
      "10.734036375526289 0.005354620899273608\n",
      "The training loss is 6.139291925447553 with std:13.009928271597426. The val loss is 7.629130887990728 with std:11.94259828760788.\n",
      "7.629130887990728 0.005354620899273608\n",
      "The training loss is 5.293200796710475 with std:8.115511067424686. The val loss is 10.8840884487074 with std:30.21969191078573.\n",
      "10.8840884487074 0.005354620899273608\n",
      "Evaluating for {'lmda': 0.005404216420705915} ...\n",
      "The training loss is 5.640924287346064 with std:12.711270330721794. The val loss is 1788.1964946785515 with std:16977.800959339907.\n",
      "1788.1964946785515 0.005404216420705915\n",
      "The training loss is 5.975991206337248 with std:10.936735714030013. The val loss is 10.743496282752453 with std:34.01383888370343.\n",
      "10.743496282752453 0.005404216420705915\n",
      "The training loss is 6.141095675999839 with std:13.012480642785361. The val loss is 7.630438582393987 with std:11.937564116749128.\n",
      "7.630438582393987 0.005404216420705915\n",
      "The training loss is 5.294970378565951 with std:8.117187418559395. The val loss is 10.882597170157927 with std:30.213416129852927.\n",
      "10.882597170157927 0.005404216420705915\n",
      "Evaluating for {'lmda': 0.005454271305329836} ...\n",
      "The training loss is 5.643197971945489 with std:12.714239100513193. The val loss is 1771.7956236585733 with std:16821.310225556124.\n",
      "1771.7956236585733 0.005454271305329836\n",
      "The training loss is 5.977570087707917 with std:10.93810302376791. The val loss is 10.75292998837371 with std:34.09062141523038.\n",
      "10.75292998837371 0.005454271305329836\n",
      "The training loss is 6.142893541495844 with std:13.01501728829235. The val loss is 7.6317380899032266 with std:11.932551007275741.\n",
      "7.6317380899032266 0.005454271305329836\n",
      "The training loss is 5.296735163907655 with std:8.118859923287175. The val loss is 10.88107121269398 with std:30.2070030728824.\n",
      "10.88107121269398 0.005454271305329836\n",
      "Evaluating for {'lmda': 0.005504789807854967} ...\n",
      "The training loss is 5.645469225404632 with std:12.717181478128417. The val loss is 1755.447402871922 with std:16665.322121728346.\n",
      "1755.447402871922 0.005504789807854967\n",
      "The training loss is 5.979141484744602 with std:10.93945870604189. The val loss is 10.76233751859846 with std:34.167215187041926.\n",
      "10.76233751859846 0.005504789807854967\n",
      "The training loss is 6.144685510573486 with std:13.017538168909889. The val loss is 7.633029331777006 with std:11.927558612188234.\n",
      "7.633029331777006 0.005504789807854967\n",
      "The training loss is 5.298495160181504 with std:8.120528593662826. The val loss is 10.879510383681664 with std:30.20045191581929.\n",
      "10.879510383681664 0.005504789807854967\n",
      "Evaluating for {'lmda': 0.005555776222398878} ...\n",
      "The training loss is 5.647738011796074 with std:12.72009738134251. The val loss is 1739.1529716969198 with std:16509.847522792974.\n",
      "1739.1529716969198 0.005555776222398878\n",
      "The training loss is 5.980705417674593 with std:10.940802777292486. The val loss is 10.771718904740563 with std:34.243619750958885.\n",
      "10.771718904740563 0.005555776222398878\n",
      "The training loss is 6.146471572545543 with std:13.020043246270843. The val loss is 7.634312229706324 with std:11.922586579623902.\n",
      "7.634312229706324 0.005555776222398878\n",
      "The training loss is 5.300250375817295 with std:8.122193442725976. The val loss is 10.877914492872604 with std:30.19376184772824.\n",
      "10.877914492872604 0.005555776222398878\n",
      "Evaluating for {'lmda': 0.005607234882852033} ...\n",
      "The training loss is 5.65000429567886 with std:12.722986729868444. The val loss is 1722.9134560595362 with std:16354.89717518946.\n",
      "1722.9134560595362 0.005607234882852033\n",
      "The training loss is 5.982261907132585 with std:10.942135254164429. The val loss is 10.781074183151134 with std:34.31983470968092.\n",
      "10.781074183151134 0.005607234882852033\n",
      "The training loss is 6.148251717404631 with std:13.022532482822369. The val loss is 7.635586705809622 with std:11.917634552839472.\n",
      "7.635586705809622 0.005607234882852033\n",
      "The training loss is 5.302000820210761 with std:8.123854484504902. The val loss is 10.876283352517998 with std:30.186932071256546.\n",
      "10.876283352517998 0.005607234882852033\n",
      "Evaluating for {'lmda': 0.005659170163246243} ...\n",
      "The training loss is 5.652268042111734 with std:12.725849445304972. The val loss is 1706.7299682559837 with std:16200.481695169427.\n",
      "1706.7299682559837 0.005659170163246243\n",
      "The training loss is 5.9838109741892636 with std:10.943456153547418. The val loss is 10.79040339525271 with std:34.39585971704491.\n",
      "10.79040339525271 0.005659170163246243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 6.150025935827755 with std:13.025005841869582. The val loss is 7.636852682645274 with std:11.912702170397456.\n",
      "7.636852682645274 0.005659170163246243\n",
      "The training loss is 5.303746503743668 with std:8.125511734022133. The val loss is 10.874616777316149 with std:30.17996180249479.\n",
      "10.874616777316149 0.005659170163246243\n",
      "Evaluating for {'lmda': 0.0057115864781264345} ...\n",
      "The training loss is 5.654529216648547 with std:12.728685451181788. The val loss is 1690.603606751737 with std:16046.611566879494.\n",
      "1690.603606751737 0.0057115864781264345\n",
      "The training loss is 5.985352640319909 with std:10.944765492531907. The val loss is 10.799706587378429 with std:34.47169447677711.\n",
      "10.799706587378429 0.0057115864781264345\n",
      "The training loss is 6.151794219179311 with std:13.027463287547402. The val loss is 7.638110083272155 with std:11.907789066423675.\n",
      "7.638110083272155 0.0057115864781264345\n",
      "The training loss is 5.305487437781697 with std:8.12716520730544. The val loss is 10.872914584525432 with std:30.17285027147075.\n",
      "10.872914584525432 0.0057115864781264345\n",
      "Evaluating for {'lmda': 0.005764488282925874} ...\n",
      "The training loss is 5.656787785352284 with std:12.731494672928903. The val loss is 1674.5354560247356 with std:15893.297140866796.\n",
      "1674.5354560247356 0.005764488282925874\n",
      "The training loss is 5.986886927420831 with std:10.94606328842161. The val loss is 10.808983810880592 with std:34.547338743039155.\n",
      "10.808983810880592 0.005764488282925874\n",
      "The training loss is 6.15355655951505 with std:13.029904784841632. The val loss is 7.639358831185394 with std:11.902894870468872.\n",
      "7.639358831185394 0.005764488282925874\n",
      "The training loss is 5.307223634673626 with std:8.128814921411632. The val loss is 10.871176593943952 with std:30.165596722150525.\n",
      "10.871176593943952 0.005764488282925874\n",
      "Evaluating for {'lmda': 0.005817880074344936} ...\n",
      "The training loss is 5.659043714789922 with std:12.734277037870822. The val loss is 1658.526586373089 with std:15740.5486322457.\n",
      "1658.526586373089 0.005817880074344936\n",
      "The training loss is 5.988413857807578 with std:10.947349558761983. The val loss is 10.818235121934636 with std:34.62279231896317.\n",
      "10.818235121934636 0.005817880074344936\n",
      "The training loss is 6.155312949580939 with std:13.032330299566574. The val loss is 7.640598850394441 with std:11.898019207937368.\n",
      "7.640598850394441 0.005817880074344936\n",
      "The training loss is 5.308955107764112 with std:8.130460894421834. The val loss is 10.86940262803118 with std:30.158200412901056.\n",
      "10.86940262803118 0.005817880074344936\n",
      "Evaluating for {'lmda': 0.005871766390733255} ...\n",
      "The training loss is 5.661296972060254 with std:12.737032475259204. The val loss is 1642.5780537372038 with std:15588.376119001538.\n",
      "1642.5780537372038 0.005871766390733255\n",
      "The training loss is 5.989933454198661 with std:10.948624321299722. The val loss is 10.827460581632524 with std:34.69805505721554.\n",
      "10.827460581632524 0.005871766390733255\n",
      "The training loss is 6.157063382826089 with std:13.034739798401214. The val loss is 7.6418300654073406 with std:11.893161700028436.\n",
      "7.6418300654073406 0.005871766390733255\n",
      "The training loss is 5.310681871389379 with std:8.13210314546035. The val loss is 10.86759251184852 with std:30.150660616407972.\n",
      "10.86759251184852 0.005871766390733255\n",
      "Evaluating for {'lmda': 0.005926151812475552} ...\n",
      "The training loss is 5.663547524779306 with std:12.739760916231338. The val loss is 1626.6908995391389 with std:15436.789540458985.\n",
      "1626.6908995391389 0.005926151812475552\n",
      "The training loss is 5.991445739731021 with std:10.949887594012527. The val loss is 10.836660255811388 with std:34.77312685849909.\n",
      "10.836660255811388 0.005926151812475552\n",
      "The training loss is 6.1588078534002 with std:13.037133248862714. The val loss is 7.643052401273711 with std:11.888321964000026.\n",
      "7.643052401273711 0.005926151812475552\n",
      "The training loss is 5.31240394088262 with std:8.133741694704415. The val loss is 10.865746073155687 with std:30.142976620018796.\n",
      "10.865746073155687 0.005926151812475552\n",
      "Evaluating for {'lmda': 0.005981040962380944} ...\n",
      "The training loss is 5.66579534108668 with std:12.742462293816006. The val loss is 1610.8661505180733 with std:15285.79869571402.\n",
      "1610.8661505180733 0.005981040962380944\n",
      "The training loss is 5.992950737942551 with std:10.951139395089342. The val loss is 10.845834215152145 with std:34.8480076723604.\n",
      "10.845834215152145 0.005981040962380944\n",
      "The training loss is 6.160546356143052 with std:13.03951061931537. The val loss is 7.644265783525339 with std:11.883499613127809.\n",
      "7.644265783525339 0.005981040962380944\n",
      "The training loss is 5.314121332570708 with std:8.135376563385464. The val loss is 10.863863142470604 with std:30.135147726122604.\n",
      "10.863863142470604 0.005981040962380944\n",
      "Evaluating for {'lmda': 0.006036438506075864} ...\n",
      "The training loss is 5.668040389676322 with std:12.745136542961916. The val loss is 1595.1048185824022 with std:15135.413242223152.\n",
      "1595.1048185824022 0.006036438506075864\n",
      "The training loss is 5.994448472786504 with std:10.95237974295057. The val loss is 10.854982534970313 with std:34.92269749536005.\n",
      "10.854982534970313 0.006036438506075864\n",
      "The training loss is 6.1622788866220395 with std:13.04187187900249. The val loss is 7.645470138296676 with std:11.878694257072729.\n",
      "7.645470138296676 0.006036438506075864\n",
      "The training loss is 5.315834063792595 with std:8.137007773821267. The val loss is 10.86194355301519 with std:30.127173251824463.\n",
      "10.86194355301519 0.006036438506075864\n",
      "Evaluating for {'lmda': 0.006092349152400711} ...\n",
      "The training loss is 5.6702826397626 with std:12.747783600477236. The val loss is 1579.4079006249704 with std:14985.642694042368.\n",
      "1579.4079006249704 0.006092349152400711\n",
      "The training loss is 5.995938968615196 with std:10.95360865623754. The val loss is 10.864105295356241 with std:34.99719637202156.\n",
      "10.864105295356241 0.006092349152400711\n",
      "The training loss is 6.164005441101107 with std:13.044216997990764. The val loss is 7.646665392242881 with std:11.87390550179608.\n",
      "7.646665392242881 0.006092349152400711\n",
      "The training loss is 5.31754215288297 with std:8.138635349390464. The val loss is 10.859987140888387 with std:30.119052529908803.\n",
      "10.859987140888387 0.006092349152400711\n",
      "Evaluating for {'lmda': 0.006148777653810023} ...\n",
      "The training loss is 5.6725220611216525 with std:12.750403405079421. The val loss is 1563.7763784106432 with std:14836.496420755475.\n",
      "1563.7763784106432 0.006148777653810023\n",
      "The training loss is 5.997422250182067 with std:10.95482615380014. The val loss is 10.87320258095126 with std:35.07150439314802.\n",
      "10.87320258095126 0.006148777653810023\n",
      "The training loss is 6.165726016551538 with std:13.046545947206539. The val loss is 7.647851472591032 with std:11.869132949771426.\n",
      "7.647851472591032 0.006148777653810023\n",
      "The training loss is 5.3192456191865825 with std:8.1402593145812. The val loss is 10.85799374498411 with std:30.110784908347455.\n",
      "10.85799374498411 0.006148777653810023\n",
      "Evaluating for {'lmda': 0.0062057288067765} ...\n",
      "The training loss is 5.674758624078914 with std:12.752995897373031. The val loss is 1548.2112184020896 with std:14687.983645813163.\n",
      "1548.2112184020896 0.0062057288067765\n",
      "The training loss is 5.998898342647058 with std:10.956032254734332. The val loss is 10.882274481027858 with std:35.14562169609065.\n",
      "10.882274481027858 0.0062057288067765\n",
      "The training loss is 6.167440610671233 with std:13.048858698453687. The val loss is 7.6490283071589005 with std:11.86437620014482.\n",
      "7.6490283071589005 0.0062057288067765\n",
      "The training loss is 5.320944483054019 with std:8.141879694966946. The val loss is 10.855963207099325 with std:30.102369750888244.\n",
      "10.855963207099325 0.0062057288067765\n",
      "Evaluating for {'lmda': 0.006263207452198692} ...\n",
      "The training loss is 5.676992299517963 with std:12.755561019836883. The val loss is 1532.7133716264773 with std:14540.11344526197.\n",
      "1532.7133716264773 0.006263207452198692\n",
      "The training loss is 6.000367271566166 with std:10.957226978343698. The val loss is 10.8913210894105 with std:35.21954846425605.\n",
      "10.8913210894105 0.006263207452198692\n",
      "The training loss is 6.169149221863891 with std:13.051155224383733. The val loss is 7.650195824343979 with std:11.85963484878878.\n",
      "7.650195824343979 0.006263207452198692\n",
      "The training loss is 5.322638765847826 with std:8.143496517241935. The val loss is 10.853895371914733 with std:30.093806437049082.\n",
      "10.853895371914733 0.006263207452198692\n",
      "Evaluating for {'lmda': 0.0063212184758124484} ...\n",
      "The training loss is 5.679223058880531 with std:12.758098716807325. The val loss is 1517.2837735564688 with std:14392.894746611068.\n",
      "1517.2837735564688 0.0063212184758124484\n",
      "The training loss is 6.0018290628893505 with std:10.958410344161635. The val loss is 10.900342504380372 with std:35.29328492614007.\n",
      "10.900342504380372 0.0063212184758124484\n",
      "The training loss is 6.170851849255477 with std:13.053435498502862. The val loss is 7.651353953149671 with std:11.854908488485068.\n",
      "7.651353953149671 0.0063212184758124484\n",
      "The training loss is 5.324328489941787 with std:8.145109809210648. The val loss is 10.851790087134285 with std:30.08509436260857.\n",
      "10.851790087134285 0.0063212184758124484\n",
      "Evaluating for {'lmda': 0.006379766808606282} ...\n",
      "The training loss is 5.681450874187007 with std:12.760608934529348. The val loss is 1501.9233439609031 with std:14246.336327407502.\n",
      "1501.9233439609031 0.006379766808606282\n",
      "The training loss is 6.003283742968165 with std:10.959582371944197. The val loss is 10.909338828779855 with std:35.36683135605662.\n",
      "10.909338828779855 0.006379766808606282\n",
      "The training loss is 6.17254849269533 with std:13.05569949518176. The val loss is 7.652502623200458 with std:11.85019670902894.\n",
      "7.652502623200458 0.006379766808606282\n",
      "The training loss is 5.326013678719135 with std:8.146719599797521. The val loss is 10.84964720338967 with std:30.076232939331998.\n",
      "10.84964720338967 0.006379766808606282\n",
      "Evaluating for {'lmda': 0.006438857427240419} ...\n",
      "The training loss is 5.68367571802394 with std:12.763091621086254. The val loss is 1486.6329867595862 with std:14100.446813853356.\n",
      "1486.6329867595862 0.006438857427240419\n",
      "The training loss is 6.004731338540973 with std:10.96074308167562. The val loss is 10.918310169736413 with std:35.44018807181974.\n",
      "10.918310169736413 0.006438857427240419\n",
      "The training loss is 6.174239152748227 with std:13.057947189652442. The val loss is 7.65364176473241 with std:11.845499097340886.\n",
      "7.65364176473241 0.006438857427240419\n",
      "The training loss is 5.327694356580704 with std:8.148325919077905. The val loss is 10.847466574387315 with std:30.067221595639733.\n",
      "10.847466574387315 0.006438857427240419\n",
      "Evaluating for {'lmda': 0.006498495354469888} ...\n",
      "The training loss is 5.6858975635646765 with std:12.76554672645887. The val loss is 1471.4135899677249 with std:13955.23468027547.\n",
      "1471.4135899677249 0.006498495354469888\n",
      "The training loss is 6.006171876743051 with std:10.961892493576638. The val loss is 10.927256638831716 with std:35.51335543612816.\n",
      "10.927256638831716 0.006498495354469888\n",
      "The training loss is 6.1759238307092215 with std:13.060178558017066. The val loss is 7.654771308613989 with std:11.84081523753067.\n",
      "7.654771308613989 0.006498495354469888\n",
      "The training loss is 5.329370548946471 with std:8.149928798254575. The val loss is 10.845248056885657 with std:30.05805977647077.\n",
      "10.845248056885657 0.006498495354469888\n",
      "Evaluating for {'lmda': 0.0065586856595714355} ...\n",
      "The training loss is 5.688116384550857 with std:12.767974202432953. The val loss is 1456.266025506235 with std:13810.708247318586.\n",
      "1456.266025506235 0.0065586856595714355\n",
      "The training loss is 6.00760538509083 with std:10.963030628077844. The val loss is 10.93617835194865 with std:35.586333855013464.\n",
      "10.93617835194865 0.0065586856595714355\n",
      "The training loss is 6.177602528593279 with std:13.062393577224146. The val loss is 7.655891186354807 with std:11.83614471114818.\n",
      "7.655891186354807 0.0065586856595714355\n",
      "The training loss is 5.331042282241227 with std:8.151528269689114. The val loss is 10.842991510777704 with std:30.04874694374697.\n",
      "10.842991510777704 0.0065586856595714355\n",
      "Evaluating for {'lmda': 0.006619433458774394} ...\n",
      "The training loss is 5.690332155327869 with std:12.770374002692291. The val loss is 1441.1911491383066 with std:13666.875681339468.\n",
      "1441.1911491383066 0.006619433458774394\n",
      "The training loss is 6.00903189149742 with std:10.964157505853873. The val loss is 10.94507542922427 with std:35.65912377751059.\n",
      "10.94507542922427 0.006619433458774394\n",
      "The training loss is 6.1792752491483975 with std:13.064592225116291. The val loss is 7.657001330131734 with std:11.831487097224246.\n",
      "7.657001330131734 0.006619433458774394\n",
      "The training loss is 5.332709583916361 with std:8.153124366894367. The val loss is 10.840696799076945 with std:30.039282576272328.\n",
      "10.840696799076945 0.006619433458774394\n",
      "Evaluating for {'lmda': 0.006680743915695614} ...\n",
      "The training loss is 5.692544850830208 with std:12.77274608275883. The val loss is 1426.1898003419092 with std:13523.744993192953.\n",
      "1426.1898003419092 0.006680743915695614\n",
      "The training loss is 6.0104514242537554 with std:10.965273147805268. The val loss is 10.953947995086033 with std:35.73172569589977.\n",
      "10.953947995086033 0.006680743915695614\n",
      "The training loss is 6.180941995843567 with std:13.066774480362627. The val loss is 7.658101672764993 with std:11.82684197235412.\n",
      "7.658101672764993 0.006680743915695614\n",
      "The training loss is 5.33437248243881 with std:8.154717124559973. The val loss is 10.838363787981377 with std:30.029666170063127.\n",
      "10.838363787981377 0.006680743915695614\n",
      "Evaluating for {'lmda': 0.006742622241778342} ...\n",
      "The training loss is 5.694754446585684 with std:12.775090399987816. The val loss is 1411.2628022202184 with std:13381.324037378568.\n",
      "1411.2628022202184 0.006742622241778342\n",
      "The training loss is 6.011864012038259 with std:10.966377575063243. The val loss is 10.962796178110686 with std:35.80414014432736.\n",
      "10.962796178110686 0.006742622241778342\n",
      "The training loss is 6.182602772885716 with std:13.06894032253099. The val loss is 7.659192147754045 with std:11.822208910872028.\n",
      "7.659192147754045 0.006742622241778342\n",
      "The training loss is 5.33603100728897 with std:8.156306578533268. The val loss is 10.835992346938161 with std:30.019897238651545.\n",
      "10.835992346938161 0.006742622241778342\n",
      "Evaluating for {'lmda': 0.006805073696735207} ...\n",
      "The training loss is 5.696960918723773 with std:12.777406913582366. The val loss is 1396.410961399259 with std:13239.620511064455.\n",
      "1396.410961399259 0.006805073696735207\n",
      "The training loss is 6.013269683905048 with std:10.967470808981181. The val loss is 10.971620110999174 with std:35.87636769857156.\n",
      "10.971620110999174 0.006805073696735207\n",
      "The training loss is 6.184257585201565 with std:13.071089732032657. The val loss is 7.660272689256052 with std:11.817587484864283.\n",
      "7.660272689256052 0.006805073696735207\n",
      "The training loss is 5.337685188968861 with std:8.157892765854058. The val loss is 10.833582348637025 with std:30.009975313076012.\n",
      "10.833582348637025 0.006805073696735207\n",
      "Evaluating for {'lmda': 0.006868103588995308} ...\n",
      "The training loss is 5.699164243988857 with std:12.779695584574378. The val loss is 1381.6350679520604 with std:13098.641953366023.\n",
      "1381.6350679520604 0.006868103588995308\n",
      "The training loss is 6.014668469292653 with std:10.96855287116897. The val loss is 10.980419930583126 with std:35.948408975993964.\n",
      "10.980419930583126 0.006868103588995308\n",
      "The training loss is 6.185906438461894 with std:13.07322269015396. The val loss is 7.6613432321415305 with std:11.812977264446939.\n",
      "7.6613432321415305 0.006868103588995308\n",
      "The training loss is 5.339335058998918 with std:8.159475724740686. The val loss is 10.831133669036516 with std:29.999899942041207.\n",
      "10.831133669036516 0.006868103588995308\n",
      "Evaluating for {'lmda': 0.006931717276155407} ...\n",
      "The training loss is 5.701364399724636 with std:12.781956375811191. The val loss is 1366.9358952773061 with std:12958.395744188896.\n",
      "1366.9358952773061 0.006931717276155407\n",
      "The training loss is 6.016060398017563 with std:10.969623783452434. The val loss is 10.989195777766884 with std:36.020264635017845.\n",
      "10.989195777766884 0.006931717276155407\n",
      "The training loss is 6.1875493390557 with std:13.075339179047065. The val loss is 7.662403711951297 with std:11.808377817727076.\n",
      "7.662403711951297 0.006931717276155407\n",
      "The training loss is 5.340980649913874 with std:8.161055494608274. The val loss is 10.828646187493073 with std:29.989670692450623.\n",
      "10.828646187493073 0.006931717276155407\n",
      "Evaluating for {'lmda': 0.006995920165435375} ...\n",
      "The training loss is 5.7035613638986575 with std:12.784189251985858. The val loss is 1352.3142000782614 with std:12818.88910402868.\n",
      "1352.3142000782614 0.006995920165435375\n",
      "The training loss is 6.017445500265805 with std:10.970683567898744. The val loss is 10.997947797429896 with std:36.091935374146246.\n",
      "10.997947797429896 0.006995920165435375\n",
      "The training loss is 6.18918629411604 with std:13.077439181729007. The val loss is 7.663454064934898 with std:11.803788710961387.\n",
      "7.663454064934898 0.006995920165435375\n",
      "The training loss is 5.342621995270991 with std:8.162632116070727. The val loss is 10.82611978667629 with std:29.979287149116576.\n",
      "10.82611978667629 0.006995920165435375\n",
      "Evaluating for {'lmda': 0.0070607177141377726} ...\n",
      "The training loss is 5.705755115091566 with std:12.78639417958467. The val loss is 1337.7707222299384 with std:12680.12909270603.\n",
      "1337.7707222299384 0.0070607177141377726\n",
      "The training loss is 6.018823806599053 with std:10.971732246807692. The val loss is 11.00667613842727 with std:36.16342193193765.\n",
      "11.00667613842727 0.0070607177141377726\n",
      "The training loss is 6.190817311502316 with std:13.079522682075142. The val loss is 7.664494228048119 with std:11.799209508659287.\n",
      "7.664494228048119 0.0070607177141377726\n",
      "The training loss is 5.344259129642455 with std:8.1642056309507. The val loss is 10.823554352699919 with std:29.9687489153098.\n",
      "10.823554352699919 0.0070607177141377726\n",
      "Evaluating for {'lmda': 0.007126115430111745} ...\n",
      "The training loss is 5.707945632507963 with std:12.788571126907984. The val loss is 1323.306184738308 with std:12542.122608977827.\n",
      "1323.306184738308 0.007126115430111745\n",
      "The training loss is 6.020195347952001 with std:10.972769842723704. The val loss is 11.015380953557104 with std:36.23472508665187.\n",
      "11.015380953557104 0.007126115430111745\n",
      "The training loss is 6.192442399812263 with std:13.081589664842399. The val loss is 7.665524138961235 with std:11.794639773713438.\n",
      "7.665524138961235 0.007126115430111745\n",
      "The training loss is 5.345892088616243 with std:8.165776082275547. The val loss is 10.820949775096825 with std:29.95805561264386.\n",
      "10.820949775096825 0.007126115430111745\n",
      "Evaluating for {'lmda': 0.007192118872221193} ...\n",
      "The training loss is 5.710132895980169 with std:12.79072006406337. The val loss is 1308.9212936880845 with std:12404.87639004065.\n",
      "1308.9212936880845 0.007192118872221193\n",
      "The training loss is 6.0215601556229785 with std:10.973796378427826. The val loss is 11.024062399452703 with std:36.305845655191675.\n",
      "11.024062399452703 0.007192118872221193\n",
      "The training loss is 6.194061568371532 with std:13.083640115642268. The val loss is 7.666543736060955 with std:11.790079067466477.\n",
      "7.666543736060955 0.007192118872221193\n",
      "The training loss is 5.34752090879361 with std:8.167343514293377. The val loss is 10.818305946902099 with std:29.947206881518873.\n",
      "10.818305946902099 0.007192118872221193\n",
      "Evaluating for {'lmda': 0.007258733650817253} ...\n",
      "The training loss is 5.712316885966726 with std:12.792840962948672. The val loss is 1294.6167381668336 with std:12268.397010808316.\n",
      "1294.6167381668336 0.007258733650817253\n",
      "The training loss is 6.022918261280284 with std:10.974811876934165. The val loss is 11.032720636603058 with std:36.376784493297734.\n",
      "11.032720636603058 0.007258733650817253\n",
      "The training loss is 6.195674827245926 with std:13.085674020953293. The val loss is 7.6675529584654445 with std:11.785526949817696.\n",
      "7.6675529584654445 0.007258733650817253\n",
      "The training loss is 5.349145627797172 with std:8.168907972473372. The val loss is 10.815622764679048 with std:29.936202381210833.\n",
      "10.815622764679048 0.007258733650817253\n",
      "Evaluating for {'lmda': 0.00732596542821523} ...\n",
      "The training loss is 5.714497583564063 with std:12.794933797255462. The val loss is 1280.3931902076197 with std:12132.690883365047.\n",
      "1280.3931902076197 0.00732596542821523\n",
      "The training loss is 6.024269696958623 with std:10.975816361514479. The val loss is 11.041355829266339 with std:36.44754249467881.\n",
      "11.041355829266339 0.00732596542821523\n",
      "The training loss is 6.1972821872283825 with std:13.087691368108535. The val loss is 7.668551746008832 with std:11.780982979286954.\n",
      "7.668551746008832 0.00732596542821523\n",
      "The training loss is 5.350766284257843 with std:8.170469503517882. The val loss is 10.812900128509497 with std:29.92504178980819.\n",
      "10.812900128509497 0.00732596542821523\n",
      "Evaluating for {'lmda': 0.007393819919175866} ...\n",
      "The training loss is 5.716674970504522 with std:12.796998542456878. The val loss is 1266.2513047601592 with std:11997.764256692883.\n",
      "1266.2513047601592 0.007393819919175866\n",
      "The training loss is 6.025614495050654 with std:10.976809855664545. The val loss is 11.049968145470062 with std:36.51812059088235.\n",
      "11.049968145470062 0.007393819919175866\n",
      "The training loss is 6.198883659854087 with std:13.089692145335546. The val loss is 7.66954003929065 with std:11.77644671326516.\n",
      "7.66954003929065 0.007393819919175866\n",
      "The training loss is 5.352382917819944 with std:8.172028155352775. The val loss is 10.810137942093872 with std:29.91372480476676.\n",
      "10.810137942093872 0.007393819919175866\n",
      "Evaluating for {'lmda': 0.007462302891391108} ...\n",
      "The training loss is 5.718849029159141 with std:12.799035175809728. The val loss is 1252.1917196365673 with std:11863.623216153974.\n",
      "1252.1917196365673 0.007462302891391108\n",
      "The training loss is 6.026952688312037 with std:10.977792383136887. The val loss is 11.058557756935372 with std:36.58851975070072.\n",
      "11.058557756935372 0.007462302891391108\n",
      "The training loss is 6.200479257382724 with std:13.091676341686304. The val loss is 7.670517779627851 with std:11.771917707901016.\n",
      "7.670517779627851 0.007462302891391108\n",
      "The training loss is 5.353995569142032 with std:8.173583977154028. The val loss is 10.807336112770138 with std:29.902251142848787.\n",
      "10.807336112770138 0.007462302891391108\n",
      "Evaluating for {'lmda': 0.007531420165974375} ...\n",
      "The training loss is 5.72101974254123 with std:12.801043676321218. The val loss is 1238.21505548348 with std:11730.273683227344.\n",
      "1238.21505548348 0.007531420165974375\n",
      "The training loss is 6.028284309856382 with std:10.978763967925902. The val loss is 11.067124839006372 with std:36.65874097935456.\n",
      "11.067124839006372 0.007531420165974375\n",
      "The training loss is 6.202068992812817 with std:13.093643947107534. The val loss is 7.671484909091614 with std:11.767395518316189.\n",
      "7.671484909091614 0.007531420165974375\n",
      "The training loss is 5.35560427988762 with std:8.175137019322007. The val loss is 10.804494551529022 with std:29.890620540349154.\n",
      "10.804494551529022 0.007531420165974375\n",
      "Evaluating for {'lmda': 0.00760117761795533} ...\n",
      "The training loss is 5.72318709431726 with std:12.803024024793832. The val loss is 1224.3219157363837 with std:11597.721415073116.\n",
      "1224.3219157363837 0.00760117761795533\n",
      "The training loss is 6.029609393152185 with std:10.979724634266383. The val loss is 11.075669570674672 with std:36.7287853187464.\n",
      "11.075669570674672 0.00760117761795533\n",
      "The training loss is 6.20365287987246 with std:13.095594952385664. The val loss is 7.67244137050263 with std:11.76287969868717.\n",
      "7.67244137050263 0.00760117761795533\n",
      "The training loss is 5.357209092731551 with std:8.176687333521704. The val loss is 10.801613173072003 with std:29.878832753250467.\n",
      "10.801613173072003 0.00760117761795533\n",
      "Evaluating for {'lmda': 0.007671581176779302} ...\n",
      "The training loss is 5.725351068797838 with std:12.804976203758349. The val loss is 1210.5128866054201 with std:11465.972004399304.\n",
      "1210.5128866054201 0.007671581176779302\n",
      "The training loss is 6.0309279720219955 with std:10.980674406656954. The val loss is 11.084192134464859 with std:36.79865384632484.\n",
      "11.084192134464859 0.007671581176779302\n",
      "The training loss is 6.2052309330252795 with std:13.09752934918654. The val loss is 7.673387107446266 with std:11.758369802331005.\n",
      "7.673387107446266 0.007671581176779302\n",
      "The training loss is 5.358810051353128 with std:8.17823497265512. The val loss is 10.798691895831139 with std:29.866887557381467.\n",
      "10.798691895831139 0.007671581176779302\n",
      "Evaluating for {'lmda': 0.007742636826811269} ...\n",
      "The training loss is 5.727511650947256 with std:12.806900197507835. The val loss is 1196.7885370598199 with std:11335.030879314576.\n",
      "1196.7885370598199 0.007742636826811269\n",
      "The training loss is 6.032240080639939 with std:10.981613309824297. The val loss is 11.092692716442563 with std:36.868347675127744.\n",
      "11.092692716442563 0.007742636826811269\n",
      "The training loss is 6.206803167460842 with std:13.099447130026315. The val loss is 7.674322064256326 with std:11.753865381788174.\n",
      "7.674322064256326 0.007742636826811269\n",
      "The training loss is 5.360407200435751 with std:8.179779990885798. The val loss is 10.795730642012327 with std:29.85478474854084.\n",
      "10.795730642012327 0.007742636826811269\n",
      "Evaluating for {'lmda': 0.007814350607844541} ...\n",
      "The training loss is 5.729668826384314 with std:12.80879599207317. The val loss is 1183.149418807899 with std:11204.90330313788.\n",
      "1183.149418807899 0.007814350607844541\n",
      "The training loss is 6.033545753530109 with std:10.982541368760169. The val loss is 11.101171506149443 with std:36.93786795313498.\n",
      "11.101171506149443 0.007814350607844541\n",
      "The training loss is 6.208369599102087 with std:13.101348288277276. The val loss is 7.675246186032399 with std:11.749365988881337.\n",
      "7.675246186032399 0.007814350607844541\n",
      "The training loss is 5.3620005856603585 with std:8.181322443630544. The val loss is 10.792729337660308 with std:29.842524142847644.\n",
      "10.792729337660308 0.007814350607844541\n",
      "Evaluating for {'lmda': 0.007886728615614156} ...\n",
      "The training loss is 5.731822581392099 with std:12.810663575238477. The val loss is 1169.596066278268 with std:11075.594374221717.\n",
      "1169.596066278268 0.007886728615614156\n",
      "The training loss is 6.034845025564489 with std:10.983458608707346. The val loss is 11.10962869658123 with std:37.00721586297196.\n",
      "11.10962869658123 0.007886728615614156\n",
      "The training loss is 6.209930244604529 with std:13.103232818186003. The val loss is 7.676159418640851 with std:11.74487117485397.\n",
      "7.676159418640851 0.007886728615614156\n",
      "The training loss is 5.363590253718693 with std:8.182862387573229. The val loss is 10.789687912636035 with std:29.83010557661465.\n",
      "10.789687912636035 0.007886728615614156\n",
      "Evaluating for {'lmda': 0.007959777002314986} ...\n",
      "The training loss is 5.733972902903177 with std:12.812502936486712. The val loss is 1156.128996630275 with std:10947.109026052172.\n",
      "1156.128996630275 0.007959777002314986\n",
      "The training loss is 6.03613793195532 with std:10.984365055152077. The val loss is 11.118064484073473 with std:37.07639262093562.\n",
      "11.118064484073473 0.007959777002314986\n",
      "The training loss is 6.211485121344638 with std:13.105100714834293. The val loss is 7.677061708724115 with std:11.740380490473548.\n",
      "7.677061708724115 0.007959777002314986\n",
      "The training loss is 5.365176252284534 with std:8.184399880656684. The val loss is 10.786606300703957 with std:29.81752890672515.\n",
      "10.786606300703957 0.007959777002314986\n",
      "Evaluating for {'lmda': 0.008033501977124734} ...\n",
      "The training loss is 5.736119778524516 with std:12.814314067051084. The val loss is 1142.7487097472977 with std:10819.45202718664.\n",
      "1142.7487097472977 0.008033501977124734\n",
      "The training loss is 6.037424508262951 with std:10.985260733847175. The val loss is 11.126479068356735 with std:37.145399477328795.\n",
      "11.126479068356735 0.008033501977124734\n",
      "The training loss is 6.2130342474308335 with std:13.106951974184634. The val loss is 7.6779530036834 with std:11.735893486010639.\n",
      "7.6779530036834 0.008033501977124734\n",
      "The training loss is 5.3667586300365135 with std:8.185934982090659. The val loss is 10.78348443953763 with std:29.804794010704963.\n",
      "10.78348443953763 0.008033501977124734\n",
      "Evaluating for {'lmda': 0.008107909806731687} ...\n",
      "The training loss is 5.738263196521619 with std:12.816096959862458. The val loss is 1129.4556882500453 with std:10692.627981381953.\n",
      "1129.4556882500453 0.008107909806731687\n",
      "The training loss is 6.038704790384939 with std:10.986145670788197. The val loss is 11.1348726524467 with std:37.21423771552637.\n",
      "11.1348726524467 0.008107909806731687\n",
      "The training loss is 6.214577641696695 with std:13.108786593037165. The val loss is 7.678833251699346 with std:11.731409711408535.\n",
      "7.678833251699346 0.008107909806731687\n",
      "The training loss is 5.368337436638876 with std:8.187467752358106. The val loss is 10.780322270784776 with std:29.791900786881666.\n",
      "10.780322270784776 0.008107909806731687\n",
      "Evaluating for {'lmda': 0.008183006815867389} ...\n",
      "The training loss is 5.740403145833417 with std:12.81785160957313. The val loss is 1116.2503974957524 with std:10566.641327588275.\n",
      "1116.2503974957524 0.008183006815867389\n",
      "The training loss is 6.0399788145580855 with std:10.987019892239053. The val loss is 11.143245442621945 with std:37.282908651605325.\n",
      "11.143245442621945 0.008183006815867389\n",
      "The training loss is 6.2161153236974505 with std:13.110604569051329. The val loss is 7.679702401730489 with std:11.726928716351106.\n",
      "7.679702401730489 0.008183006815867389\n",
      "The training loss is 5.369912722752803 with std:8.18899825321525. The val loss is 10.777119740078682 with std:29.778849154548528.\n",
      "10.777119740078682 0.008183006815867389\n",
      "Evaluating for {'lmda': 0.008258799387844271} ...\n",
      "The training loss is 5.742539616063942 with std:12.81957801252099. The val loss is 1103.133285609266 with std:10441.496340246691.\n",
      "1103.133285609266 0.008258799387844271\n",
      "The training loss is 6.041246617356027 with std:10.987883424719396. The val loss is 11.151597648411332 with std:37.35141363427859.\n",
      "11.151597648411332 0.008258799387844271\n",
      "The training loss is 6.21764731371555 with std:13.11240590074356. The val loss is 7.680560403519534 with std:11.722450050348886.\n",
      "7.680560403519534 0.008258799387844271\n",
      "The training loss is 5.371484540013229 with std:8.190526547688139. The val loss is 10.773876797067226 with std:29.765639054071958.\n",
      "10.773876797067226 0.008258799387844271\n",
      "Evaluating for {'lmda': 0.008335293965098196} ...\n",
      "The training loss is 5.744672597490264 with std:12.821276166747884. The val loss is 1090.1047835047402 with std:10317.197129497727.\n",
      "1090.1047835047402 0.008335293965098196\n",
      "The training loss is 6.042508235681345 with std:10.988736294999173. The val loss is 11.159929482471338 with std:37.41975404365851.\n",
      "11.159929482471338 0.008335293965098196\n",
      "The training loss is 6.219173632754458 with std:13.114190587477264. The val loss is 7.681407207578242 with std:11.717973262751546.\n",
      "7.681407207578242 0.008335293965098196\n",
      "The training loss is 5.373052941044643 with std:8.192052700086345. The val loss is 10.77059339547827 with std:29.752270447069453.\n",
      "10.77059339547827 0.008335293965098196\n",
      "Evaluating for {'lmda': 0.008412497049736118} ...\n",
      "The training loss is 5.746802081064637 with std:12.82294607198293. The val loss is 1077.1653048982148 with std:10193.747641302507.\n",
      "1077.1653048982148 0.008412497049736118\n",
      "The training loss is 6.043763706772491 with std:10.98957853012498. The val loss is 11.168241160637185 with std:37.48793129178901.\n",
      "11.168241160637185 0.008412497049736118\n",
      "The training loss is 6.220694302530697 with std:13.115958629463483. The val loss is 7.6822427652019964 with std:11.71349790290969.\n",
      "7.6822427652019964 0.008412497049736118\n",
      "The training loss is 5.3746179794484705 with std:8.193576775992058. The val loss is 10.767269493156785 with std:29.73874331668027.\n",
      "10.767269493156785 0.008412497049736118\n",
      "Evaluating for {'lmda': 0.008490415204088747} ...\n",
      "The training loss is 5.748928058415692 with std:12.824587729628975. The val loss is 1064.3152463623333 with std:10071.151657966568.\n",
      "1064.3152463623333 0.008490415204088747\n",
      "The training loss is 6.045013068194725 with std:10.99041015739136. The val loss is 11.176532901808528 with std:37.55594682145799.\n",
      "11.176532901808528 0.008490415204088747\n",
      "The training loss is 6.222209345485178 with std:13.117710027769274. The val loss is 7.683067028471655 with std:11.709023520184829.\n",
      "7.683067028471655 0.008490415204088747\n",
      "The training loss is 5.3761797098037025 with std:8.19509884226778. The val loss is 10.763905052047484 with std:29.72505766740233.\n",
      "10.763905052047484 0.008490415204088747\n",
      "Evaluating for {'lmda': 0.008569055051268348} ...\n",
      "The training loss is 5.751050521846157 with std:12.82620114275811. The val loss is 1051.5549873419789 with std:9949.41279828977.\n",
      "1051.5549873419789 0.008569055051268348\n",
      "The training loss is 6.0462563578381765 with std:10.991231204366223. The val loss is 11.184804927955385 with std:37.62380210622545.\n",
      "11.184804927955385 0.008569055051268348\n",
      "The training loss is 6.223718784770283 with std:13.119444784301105. The val loss is 7.683879950245455 with std:11.704549664063624.\n",
      "7.683879950245455 0.008569055051268348\n",
      "The training loss is 5.377738187656742 with std:8.196618967060369. The val loss is 10.760500038281506 with std:29.711213525450315.\n",
      "10.760500038281506 0.008569055051268348\n",
      "Evaluating for {'lmda': 0.008648423275731726} ...\n",
      "The training loss is 5.75316946434066 with std:12.82778631611828. The val loss is 1038.884890220071 with std:9828.534518195926.\n",
      "1038.884890220071 0.008648423275731726\n",
      "The training loss is 6.047493613920034 with std:10.992041698875214. The val loss is 11.193057464065573 with std:37.69149864991157.\n",
      "11.193057464065573 0.008648423275731726\n",
      "The training loss is 6.225222644257519 with std:13.121162901818304. The val loss is 7.684681484180542 with std:11.700075884246377.\n",
      "7.684681484180542 0.008648423275731726\n",
      "The training loss is 5.379293469526968 with std:8.19813721979989. The val loss is 10.75705442220632 with std:29.697210938940863.\n",
      "10.75705442220632 0.008648423275731726\n",
      "Evaluating for {'lmda': 0.00872852662384838} ...\n",
      "The training loss is 5.755284879562447 with std:12.82934325610232. The val loss is 1026.3053003454365 with std:9708.52011099989.\n",
      "1026.3053003454365 0.00872852662384838\n",
      "The training loss is 6.048724874974637 with std:10.992841669013837. The val loss is 11.20129073806216 with std:37.75903798587529.\n",
      "11.20129073806216 0.00872852662384838\n",
      "The training loss is 6.226720948523429 with std:13.122864383919719. The val loss is 7.685471584689535 with std:11.695601730604993.\n",
      "7.685471584689535 0.00872852662384838\n",
      "The training loss is 5.380845612893865 with std:8.199653671198877. The val loss is 10.753568178419593 with std:29.68304997794846.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.753568178419593 0.00872852662384838\n",
      "Evaluating for {'lmda': 0.00880937190447399} ...\n",
      "The training loss is 5.757396761859077 with std:12.830871970762768. The val loss is 1013.8165460997733 with std:9589.372708047495.\n",
      "1013.8165460997733 0.00880937190447399\n",
      "The training loss is 6.049950179862502 with std:10.993631143147812. The val loss is 11.209504980850413 with std:37.82642167727775.\n",
      "11.209504980850413 0.00880937190447399\n",
      "The training loss is 6.228213722860023 with std:13.124549235050493. The val loss is 7.686250206998869 with std:11.691126753398398.\n",
      "7.686250206998869 0.00880937190447399\n",
      "The training loss is 5.382394676199892 with std:8.2011683932512. The val loss is 10.750041285780673 with std:29.6687307345329.\n",
      "10.750041285780673 0.00880937190447399\n",
      "Evaluating for {'lmda': 0.008890965989529159} ...\n",
      "The training loss is 5.759505106257801 with std:12.83237246978842. The val loss is 1001.418938949585 with std:9471.095279212932.\n",
      "1001.418938949585 0.008890965989529159\n",
      "The training loss is 6.051169567752433 with std:10.994410149906706. The val loss is 11.21770042617024 with std:37.893651315783934.\n",
      "11.21770042617024 0.008890965989529159\n",
      "The training loss is 6.229700993260811 with std:13.126217460485986. The val loss is 7.687017307096864 with std:11.686650503241182.\n",
      "7.687017307096864 0.008890965989529159\n",
      "The training loss is 5.383940718841179 with std:8.202681459235114. The val loss is 10.746473727456788 with std:29.65425332301276.\n",
      "10.746473727456788 0.008890965989529159\n",
      "Evaluating for {'lmda': 0.00897331581458352} ...\n",
      "The training loss is 5.7616099084755215 with std:12.833844764516439. The val loss is 989.1127735095956 with std:9353.69063350451.\n",
      "989.1127735095956 0.00897331581458352\n",
      "The training loss is 6.052383078132687 with std:10.99517871818634. The val loss is 11.225877310646057 with std:37.96072852195116.\n",
      "11.225877310646057 0.00897331581458352\n",
      "The training loss is 6.231182786430121 with std:13.127869066360862. The val loss is 7.687772841768147 with std:11.682172531215567.\n",
      "7.687772841768147 0.00897331581458352\n",
      "The training loss is 5.385483801164717 with std:8.20419294372002. The val loss is 10.742865490975591 with std:29.639617880031107.\n",
      "10.742865490975591 0.00897331581458352\n",
      "Evaluating for {'lmda': 0.009056428379445295} ...\n",
      "The training loss is 5.763711164905793 with std:12.835288867888258. The val loss is 976.898327615323 with std:9237.161419759004.\n",
      "976.898327615323 0.009056428379445295\n",
      "The training loss is 6.0535907508038935 with std:10.995936877174753. The val loss is 11.234035873693507 with std:38.027654944329.\n",
      "11.234035873693507 0.009056428379445295\n",
      "The training loss is 6.232659129768488 with std:13.129504059628406. The val loss is 7.688516768566546 with std:11.677692388896746.\n",
      "7.688516768566546 0.009056428379445295\n",
      "The training loss is 5.387023984469421 with std:8.205702922556718. The val loss is 10.739216568254529 with std:29.62482456475239.\n",
      "10.739216568254529 0.009056428379445295\n",
      "Evaluating for {'lmda': 0.009140310748756233} ...\n",
      "The training loss is 5.765808872637908 with std:12.836704794494056. The val loss is 964.775862387694 with std:9121.510127258964.\n",
      "964.775862387694 0.009140310748756233\n",
      "The training loss is 6.054792625871282 with std:10.996684656298793. The val loss is 11.242176357513339 with std:38.094432259411775.\n",
      "11.242176357513339 0.009140310748756233\n",
      "The training loss is 6.234130051381546 with std:13.131122448085994. The val loss is 7.689249045840956 with std:11.673209628466234.\n",
      "7.689249045840956 0.009140310748756233\n",
      "The training loss is 5.388561330990158 with std:8.207211472877443. The val loss is 10.735526955615654 with std:29.609873558889475.\n",
      "10.735526955615654 0.009140310748756233\n",
      "Evaluating for {'lmda': 0.009224970052592174} ...\n",
      "The training loss is 5.767903029438088 with std:12.838092560519206. The val loss is 952.7456223124877 with std:9006.73908649234.\n",
      "952.7456223124877 0.009224970052592174\n",
      "The training loss is 6.055988743751094 with std:10.997422085287091. The val loss is 11.250299007007573 with std:38.161062170768666.\n",
      "11.250299007007573 0.009224970052592174\n",
      "The training loss is 6.235595580068709 with std:13.132724240370543. The val loss is 7.689969632705214 with std:11.66872380271681.\n",
      "7.689969632705214 0.009224970052592174\n",
      "The training loss is 5.3900959039020835 with std:8.208718673098252. The val loss is 10.731796653844052 with std:29.59476506683795.\n",
      "10.731796653844052 0.009224970052592174\n",
      "Evaluating for {'lmda': 0.009310413487069076} ...\n",
      "The training loss is 5.769993633769725 with std:12.8394521837661. The val loss is 940.8078353339406 with std:8892.850470046613.\n",
      "940.8078353339406 0.009310413487069076\n",
      "The training loss is 6.057179145162861 with std:10.99814919412945. The val loss is 11.258404069800616 with std:38.22754640926662.\n",
      "11.258404069800616 0.009310413487069076\n",
      "The training loss is 6.237055745326198 with std:13.134309445939557. The val loss is 7.690678489072731 with std:11.664234465184547.\n",
      "7.690678489072731 0.009310413487069076\n",
      "The training loss is 5.3916277673150494 with std:8.210224602926068. The val loss is 10.728025668193478 with std:29.57949931580502.\n",
      "10.728025668193478 0.009310413487069076\n",
      "Evaluating for {'lmda': 0.009396648314954691} ...\n",
      "The training loss is 5.7720806847762915 with std:12.840783683626782. The val loss is 928.9627129178955 with std:8779.846293212884.\n",
      "928.9627129178955 0.009396648314954691\n",
      "The training loss is 6.058363871126173 with std:10.998866013099843. The val loss is 11.26649179612958 with std:38.29388673203235.\n",
      "11.26649179612958 0.009396648314954691\n",
      "The training loss is 6.238510577338758 with std:13.135878075091455. The val loss is 7.69137557560171 with std:11.659741170086654.\n",
      "7.69137557560171 0.009396648314954691\n",
      "The training loss is 5.393156986264695 with std:8.211729343343398. The val loss is 10.724214008465392 with std:29.56407655601845.\n",
      "10.724214008465392 0.009396648314954691\n",
      "Evaluating for {'lmda': 0.009483681866285927} ...\n",
      "The training loss is 5.7741641822940935 with std:12.84208708109489. The val loss is 917.2104501685635 with std:8667.728415101292.\n",
      "917.2104501685635 0.009483681866285927\n",
      "The training loss is 6.059542962961867 with std:10.99957257274451. The val loss is 11.27456243888228 with std:38.360084922693865.\n",
      "11.27456243888228 0.009483681866285927\n",
      "The training loss is 6.239960106984373 with std:13.137430138949405. The val loss is 7.69206085376518 with std:11.65524347256051.\n",
      "7.69206085376518 0.009483681866285927\n",
      "The training loss is 5.39468362671101 with std:8.213232976617524. The val loss is 10.72036168899917 with std:29.54849706063406.\n",
      "10.72036168899917 0.009483681866285927\n",
      "Evaluating for {'lmda': 0.009571521538991866} ...\n",
      "The training loss is 5.7762441268472156 with std:12.843362398746642. The val loss is 905.5512259084993 with std:8556.498539404643.\n",
      "905.5512259084993 0.009571521538991866\n",
      "The training loss is 6.060716462285055 with std:11.000268903889532. The val loss is 11.282616253483571 with std:38.42614279038522.\n",
      "11.282616253483571 0.009571521538991866\n",
      "The training loss is 6.2414043658232385 with std:13.138965649463836. The val loss is 7.692734285774605 with std:11.65074092852819.\n",
      "7.692734285774605 0.009571521538991866\n",
      "The training loss is 5.39620775552625 with std:8.214735586290804. The val loss is 10.716468728736045 with std:29.53276112605728.\n",
      "10.716468728736045 0.009571521538991866\n",
      "Evaluating for {'lmda': 0.009660174799522647} ...\n",
      "The training loss is 5.778320519651824 with std:12.84460966074591. The val loss is 893.9852027810543 with std:8446.158215378076.\n",
      "893.9852027810543 0.009660174799522647\n",
      "The training loss is 6.061884411007997 with std:11.000955037637961. The val loss is 11.290653497927428 with std:38.49206216999188.\n",
      "11.290653497927428 0.009660174799522647\n",
      "The training loss is 6.2428433861007635 with std:13.140484619416926. The val loss is 7.69339583463204 with std:11.64623309487851.\n",
      "7.69339583463204 0.009660174799522647\n",
      "The training loss is 5.397729440502391 with std:8.21623725718946. The val loss is 10.712535151232455 with std:29.51686907187465.\n",
      "10.712535151232455 0.009660174799522647\n",
      "Evaluating for {'lmda': 0.009749649183484086} ...\n",
      "The training loss is 5.780393362604853 with std:12.84582889280835. The val loss is 882.5125273539567 with std:8336.708838828405.\n",
      "882.5125273539567 0.009749649183484086\n",
      "The training loss is 6.06304685133069 with std:11.001631005381014. The val loss is 11.298674432670852 with std:38.5578449211311.\n",
      "11.298674432670852 0.009749649183484086\n",
      "The training loss is 6.24427720074261 with std:13.141987062409495. The val loss is 7.694045464103697 with std:11.641719529492642.\n",
      "7.694045464103697 0.009749649183484086\n",
      "The training loss is 5.399248750331602 with std:8.217738075413635. The val loss is 10.708560984692264 with std:29.500821240998032.\n",
      "10.708560984692264 0.009749649183484086\n",
      "Evaluating for {'lmda': 0.009839952296278227} ...\n",
      "The training loss is 5.782462658301408 with std:12.847020122222457. The val loss is 871.133330232604 with std:8228.151653196071.\n",
      "871.133330232604 0.009839952296278227\n",
      "The training loss is 6.064203825745489 with std:11.002296838785387. The val loss is 11.306679320674549 with std:38.62349292853444.\n",
      "11.306679320674549 0.009839952296278227\n",
      "The training loss is 6.24570584334554 with std:13.143472992858193. The val loss is 7.694683138707632 with std:11.637199791230112.\n",
      "7.694683138707632 0.009839952296278227\n",
      "The training loss is 5.400765754608905 with std:8.219238128339938. The val loss is 10.704546262041797 with std:29.48461799990832.\n",
      "10.704546262041797 0.009839952296278227\n",
      "Evaluating for {'lmda': 0.009931091813749801} ...\n",
      "The training loss is 5.784528410021807 with std:12.848183377850212. The val loss is 859.8477261664402 with std:8120.487750571432.\n",
      "859.8477261664402 0.009931091813749801\n",
      "The training loss is 6.065355377028944 with std:11.002952569804956. The val loss is 11.314668427258319 with std:38.689008100619624.\n",
      "11.314668427258319 0.009931091813749801\n",
      "The training loss is 6.247129348190289 with std:13.144942426013241. The val loss is 7.695308823748423 with std:11.632673440102884.\n",
      "7.695308823748423 0.009931091813749801\n",
      "The training loss is 5.402280523826869 with std:8.220737504617318. The val loss is 10.7004910208976 with std:29.468259738469538.\n",
      "10.7004910208976 0.009931091813749801\n",
      "Evaluating for {'lmda': 0.010023075482838654} ...\n",
      "The training loss is 5.786590621732617 with std:12.84931869007992. The val loss is 848.6558141720367 with std:8013.718072870649.\n",
      "848.6558141720367 0.010023075482838654\n",
      "The training loss is 6.066501548239636 with std:11.003598230677019. The val loss is 11.322642020196808 with std:38.754392370467194.\n",
      "11.322642020196808 0.010023075482838654\n",
      "The training loss is 6.248547750217562 with std:13.146395377939667. The val loss is 7.695922485269735 with std:11.62814003720908.\n",
      "7.695922485269735 0.010023075482838654\n",
      "The training loss is 5.403793129361832 with std:8.222236294158352. The val loss is 10.69639530365529 with std:29.451746870298958.\n",
      "10.69639530365529 0.010023075482838654\n",
      "Evaluating for {'lmda': 0.010115911122238298} ...\n",
      "The training loss is 5.788649298094116 with std:12.850426090857416. The val loss is 837.5576776463837 with std:7907.843412917689.\n",
      "837.5576776463837 0.010115911122238298\n",
      "The training loss is 6.067642382719764 with std:11.004233853918358. The val loss is 11.330600369561006 with std:38.819647694193804.\n",
      "11.330600369561006 0.010115911122238298\n",
      "The training loss is 6.249961085042039 with std:13.147831865514775. The val loss is 7.696524090085889 with std:11.623599144859183.\n",
      "7.696524090085889 0.010115911122238298\n",
      "The training loss is 5.4053036434770325 with std:8.223734588149314. The val loss is 10.692259157487952 with std:29.43507983262341.\n",
      "10.692259157487952 0.010115911122238298\n",
      "Evaluating for {'lmda': 0.010209606623060466} ...\n",
      "The training loss is 5.79070444444898 with std:12.851505613663061. The val loss is 826.5533844985035 with std:7802.864415701475.\n",
      "826.5533844985035 0.010209606623060466\n",
      "The training loss is 6.0687779240888755 with std:11.004859472346125. The val loss is 11.338543747766396 with std:38.8847760514835.\n",
      "11.338543747766396 0.010209606623060466\n",
      "The training loss is 6.251369388940274 with std:13.149251906437113. The val loss is 7.697113605754722 with std:11.619050326578801.\n",
      "7.697113605754722 0.010209606623060466\n",
      "The training loss is 5.406812139313168 with std:8.225232479044363. The val loss is 10.68808263440438 with std:29.418259086532352.\n",
      "10.68808263440438 0.010209606623060466\n",
      "Evaluating for {'lmda': 0.01030416994950588} ...\n",
      "The training loss is 5.792756066833511 with std:12.852557293517467. The val loss is 815.6429872709679 with std:7698.781579536032.\n",
      "815.6429872709679 0.01030416994950588\n",
      "The training loss is 6.069908216239727 with std:11.005475119052319. The val loss is 11.346472429470596 with std:38.949779444647476.\n",
      "11.346472429470596 0.01030416994950588\n",
      "The training loss is 6.252772698847284 with std:13.150655519213794. The val loss is 7.697691000583215 with std:11.614493147165389.\n",
      "7.697691000583215 0.01030416994950588\n",
      "The training loss is 5.408318690877915 with std:8.22673006054667. The val loss is 10.683865791261315 with std:29.40128511697279.\n",
      "10.683865791261315 0.01030416994950588\n",
      "Evaluating for {'lmda': 0.010399609139541197} ...\n",
      "The training loss is 5.794804171961512 with std:12.853581166936888. The val loss is 804.8265232762885 with std:7595.595257364126.\n",
      "804.8265232762885 0.010399609139541197\n",
      "The training loss is 6.071033303341514 with std:11.006080827421115. The val loss is 11.354386691615485 with std:39.0146598988107.\n",
      "11.354386691615485 0.010399609139541197\n",
      "The training loss is 6.254171052357045 with std:13.15204272316668. The val loss is 7.698256243629609 with std:11.609927172752338.\n",
      "7.698256243629609 0.010399609139541197\n",
      "The training loss is 5.409823373044993 with std:8.228227427628276. The val loss is 10.679608689782613 with std:29.38415843273103.\n",
      "10.679608689782613 0.010399609139541197\n",
      "Evaluating for {'lmda': 0.010495932305582278} ...\n",
      "The training loss is 5.796848767240157 with std:12.854577271983802. The val loss is 794.1040147339897 with std:7493.305658065297.\n",
      "794.1040147339897 0.010495932305582278\n",
      "The training loss is 6.07215322982919 with std:11.006676631130542. The val loss is 11.362286813307845 with std:39.07941946102011.\n",
      "11.362286813307845 0.010495932305582278\n",
      "The training loss is 6.255564487714107 with std:13.153413538429644. The val loss is 7.698809304692943 with std:11.605351970854269.\n",
      "7.698809304692943 0.010495932305582278\n",
      "The training loss is 5.411326261545236 with std:8.229724676509328. The val loss is 10.675311396658284 with std:29.366879566790217.\n",
      "10.675311396658284 0.010495932305582278\n",
      "Evaluating for {'lmda': 0.0105931476351837} ...\n",
      "The training loss is 5.798889860759631 with std:12.855545648217594. The val loss is 783.4754689034594 with std:7391.912847725075.\n",
      "783.4754689034594 0.0105931476351837\n",
      "The training loss is 6.073268040408108 with std:11.007262564145401. The val loss is 11.370173075841757 with std:39.14406020027432.\n",
      "11.370173075841757 0.0105931476351837\n",
      "The training loss is 6.256953043816896 with std:13.15476798594227. The val loss is 7.6993501543033265 with std:11.600767110367537.\n",
      "7.6993501543033265 0.0105931476351837\n",
      "The training loss is 5.412827432961057 with std:8.231221904660366. The val loss is 10.670973983481108 with std:29.3494490760099.\n",
      "10.670973983481108 0.0105931476351837\n",
      "Evaluating for {'lmda': 0.01069126339173476} ...\n",
      "The training loss is 5.8009274612907475 with std:12.856486336700241. The val loss is 772.9408782330362 with std:7291.416751058874.\n",
      "772.9408782330362 0.01069126339173476\n",
      "The training loss is 6.074377780046512 with std:11.007838660726199. The val loss is 11.378045762646316 with std:39.208584207103776.\n",
      "11.378045762646316 0.01069126339173476\n",
      "The training loss is 6.2583367602050615 with std:13.156106087450873. The val loss is 7.699878763726711 with std:11.596172161650953.\n",
      "7.699878763726711 0.01069126339173476\n",
      "The training loss is 5.4143269647169445 with std:8.232719210802363. The val loss is 10.666596526847616 with std:29.331867541482.\n",
      "10.666596526847616 0.01069126339173476\n",
      "Evaluating for {'lmda': 0.010790287915161841} ...\n",
      "The training loss is 5.802961578289675 with std:12.857399379998203. The val loss is 762.5002205043112 with std:7191.8171527898085.\n",
      "762.5002205043112 0.010790287915161841\n",
      "The training loss is 6.075482493974208 with std:11.008404955422323. The val loss is 11.38590515924178 with std:39.27299359310947.\n",
      "11.38590515924178 0.010790287915161841\n",
      "The training loss is 6.259715677065404 with std:13.15742786551485. The val loss is 7.700395104964623 with std:11.591566696573075.\n",
      "7.700395104964623 0.010790287915161841\n",
      "The training loss is 5.41582493507726 with std:8.234216694895137. The val loss is 10.662179108376602 with std:29.31413556852396.\n",
      "10.662179108376602 0.010790287915161841\n",
      "Evaluating for {'lmda': 0.010890229622637299} ...\n",
      "The training loss is 5.804992221893831 with std:12.858284822166626. The val loss is 752.1534589769382 with std:7093.113699031346.\n",
      "752.1534589769382 0.010890229622637299\n",
      "The training loss is 6.076582227680271 with std:11.008961483074954. The val loss is 11.393751553236667 with std:39.33729049095786.\n",
      "11.393751553236667 0.010890229622637299\n",
      "The training loss is 6.261089835219439 with std:13.158733343484078. The val loss is 7.700899150729931 with std:11.58695028850981.\n",
      "7.700899150729931 0.010890229622637299\n",
      "The training loss is 5.41732142313262 with std:8.235714458138109. The val loss is 10.657721814719434 with std:29.296253786638044.\n",
      "10.657721814719434 0.010890229622637299\n",
      "Evaluating for {'lmda': 0.010991097009294973} ...\n",
      "The training loss is 5.807019402916561 with std:12.85914270873285. The val loss is 741.9005425483351 with std:6995.305898813118.\n",
      "741.9005425483351 0.010991097009294973\n",
      "The training loss is 6.077677026908936 with std:11.00950827882522. The val loss is 11.401585234238487 with std:39.40147705351944.\n",
      "11.401585234238487 0.010991097009294973\n",
      "The training loss is 6.262459276126367 with std:13.160022545521976. The val loss is 7.701390874467664 with std:11.582322512448714.\n",
      "7.701390874467664 0.010991097009294973\n",
      "The training loss is 5.418816508799701 with std:8.237212602962703. The val loss is 10.653224737587223 with std:29.278222849550627.\n",
      "10.653224737587223 0.010991097009294973\n",
      "Evaluating for {'lmda': 0.011092898648952227} ...\n",
      "The training loss is 5.809043132851972 with std:12.859973086717053. The val loss is 731.7414059043293 with std:6898.393125518391.\n",
      "731.7414059043293 0.011092898648952227\n",
      "The training loss is 6.078766937658144 with std:11.010045378109899. The val loss is 11.409406493876965 with std:39.465555454066745.\n",
      "11.409406493876965 0.011092898648952227\n",
      "The training loss is 6.263824041874608 with std:13.161295496591222. The val loss is 7.701870250328586 with std:11.57768294497332.\n",
      "7.701870250328586 0.011092898648952227\n",
      "The training loss is 5.420310272808314 with std:8.238711233026287. The val loss is 10.648687973809826 with std:29.260043435377856.\n",
      "10.648687973809826 0.011092898648952227\n",
      "Evaluating for {'lmda': 0.011195643194838782} ...\n",
      "The training loss is 5.811063423872356 with std:12.860776004610631. The val loss is 721.6759696807885 with std:6802.3746184282245.\n",
      "721.6759696807885 0.011195643194838782\n",
      "The training loss is 6.079852006176044 with std:11.010572816658836. The val loss is 11.417215625731286 with std:39.529527885613035.\n",
      "11.417215625731286 0.011195643194838782\n",
      "The training loss is 6.265184175183064 with std:13.16255222245202. The val loss is 7.702337253185713 with std:11.573031164330406.\n",
      "7.702337253185713 0.011195643194838782\n",
      "The training loss is 5.421802796696236 with std:8.24021045321046. The val loss is 10.644111625353393 with std:29.241716246614526.\n",
      "10.644111625353393 0.011195643194838782\n",
      "Evaluating for {'lmda': 0.011299339380332223} ...\n",
      "The training loss is 5.813080288819664 with std:12.86155151235892. The val loss is 711.7041406187501 with std:6707.2494842024535.\n",
      "711.7041406187501 0.011299339380332223\n",
      "The training loss is 6.080932278957577 with std:11.011090630501519. The val loss is 11.425012925326758 with std:39.59339656086722.\n",
      "11.425012925326758 0.011299339380332223\n",
      "The training loss is 6.266539719387942 with std:13.163792749658532. The val loss is 7.702791858593532 with std:11.56836675042416.\n",
      "7.702791858593532 0.011299339380332223\n",
      "The training loss is 5.423294162804306 with std:8.241710369619334. The val loss is 10.639495799351902 with std:29.22324201015471.\n",
      "10.639495799351902 0.011299339380332223\n",
      "Evaluating for {'lmda': 0.011403996019700324} ...\n",
      "The training loss is 5.815093741213882 with std:12.86229966138104. The val loss is 701.8258117399939 with std:6613.016698556074.\n",
      "701.8258117399939 0.011403996019700324\n",
      "The training loss is 6.082007802741503 with std:11.011598855963172. The val loss is 11.432798690070733 with std:39.6571637116056.\n",
      "11.432798690070733 0.011403996019700324\n",
      "The training loss is 6.267890718451795 with std:13.165017105568948. The val loss is 7.703234042831786 with std:11.563689284935107.\n",
      "7.703234042831786 0.011403996019700324\n",
      "The training loss is 5.4247844542616726 with std:8.243211089562294. The val loss is 10.634840608103781 with std:29.204621477211617.\n",
      "10.634840608103781 0.011403996019700324\n",
      "Evaluating for {'lmda': 0.011509622008850324} ...\n",
      "The training loss is 5.817103795244046 with std:12.863020504544082. The val loss is 692.0408625026805 with std:6519.67510774562.\n",
      "692.0408625026805 0.011509622008850324\n",
      "The training loss is 6.083078624510427 with std:11.012097529675463. The val loss is 11.44057321925404 with std:39.720831588719186.\n",
      "11.44057321925404 0.011509622008850324\n",
      "The training loss is 6.269237216949353 with std:13.16622531833153. The val loss is 7.703663782855494 with std:11.5589983512433.\n",
      "7.703663782855494 0.011509622008850324\n",
      "The training loss is 5.426273754986869 with std:8.244712721558258. The val loss is 10.630146169145688 with std:29.18585542351176.\n",
      "10.630146169145688 0.011509622008850324\n",
      "Evaluating for {'lmda': 0.011616226326085019} ...\n",
      "The training loss is 5.819110465765242 with std:12.86371409615331. The val loss is 682.3491589745433 with std:6427.223430222506.\n",
      "682.3491589745433 0.011616226326085019\n",
      "The training loss is 6.084144791483027 with std:11.012586688561747. The val loss is 11.44833681398241 with std:39.78440246154924.\n",
      "11.44833681398241 0.011616226326085019\n",
      "The training loss is 6.270579260067653 with std:13.167417416892324. The val loss is 7.704081056322683 with std:11.554293534595173.\n",
      "7.704081056322683 0.011616226326085019\n",
      "The training loss is 5.427762149670602 with std:8.246215375325828. The val loss is 10.625412605262136 with std:29.166944649208194.\n",
      "10.625412605262136 0.011616226326085019\n",
      "Evaluating for {'lmda': 0.011723818032865985} ...\n",
      "The training loss is 5.821113768299926 with std:12.86438049197092. The val loss is 672.7505540102811 with std:6335.660258327063.\n",
      "672.7505540102811 0.011723818032865985\n",
      "The training loss is 6.0852063511144605 with std:11.01306636984904. The val loss is 11.456089777176292 with std:39.847878617867.\n",
      "11.456089777176292 0.011723818032865985\n",
      "The training loss is 6.271916893601306 with std:13.168593430986943. The val loss is 7.7044858415600075 with std:11.549574422000585.\n",
      "7.7044858415600075 0.011723818032865985\n",
      "The training loss is 5.429249723775784 with std:8.247719161777427. The val loss is 10.620640044516716 with std:29.147889978963434.\n",
      "10.620640044516716 0.011723818032865985\n",
      "Evaluating for {'lmda': 0.011832406274583786} ...\n",
      "The training loss is 5.823113719034909 with std:12.865019749179394. The val loss is 663.2448874162853 with std:6244.984059861208.\n",
      "663.2448874162853 0.011832406274583786\n",
      "The training loss is 6.086263351092478 with std:11.013536611071164. The val loss is 11.46383241352239 with std:39.91126236347604.\n",
      "11.46383241352239 0.011832406274583786\n",
      "The training loss is 6.273250163950193 with std:13.169753391145843. The val loss is 7.704878117583619 with std:11.544840602374554.\n",
      "7.704878117583619 0.011832406274583786\n",
      "The training loss is 5.430736563524454 with std:8.249224193016826. The val loss is 10.615828620253453 with std:29.128692261811832.\n",
      "10.615828620253453 0.011832406274583786\n",
      "Evaluating for {'lmda': 0.011942000281335319} ...\n",
      "The training loss is 5.825110334818784 with std:12.86563192640245. The val loss is 653.8319861404548 with std:6155.193179900733.\n",
      "653.8319861404548 0.011942000281335319\n",
      "The training loss is 6.087315839334288 with std:11.013997450054609. The val loss is 11.471565029454991 with std:39.97455602195466.\n",
      "11.471565029454991 0.011942000281335319\n",
      "The training loss is 6.274579118111281 with std:13.170897328684449. The val loss is 7.705257864076003 with std:11.54009166651161.\n",
      "7.705257864076003 0.011942000281335319\n",
      "The training loss is 5.432222755890325 with std:8.250730582320646. The val loss is 10.610978471179333 with std:29.10935237136264.\n",
      "10.610978471179333 0.011942000281335319\n",
      "Evaluating for {'lmda': 0.012052609368708425} ...\n",
      "The training loss is 5.827103633158061 with std:12.866217083683926. The val loss is 644.5116644376749 with std:6066.285842375434.\n",
      "644.5116644376749 0.012052609368708425\n",
      "The training loss is 6.088363863983149 with std:11.014448924936435. The val loss is 11.479287933092186 with std:40.03776193414352.\n",
      "11.479287933092186 0.012052609368708425\n",
      "The training loss is 6.275903803679602 with std:13.172025275716896. The val loss is 7.705625061388001 with std:11.535327207140138.\n",
      "7.705625061388001 0.012052609368708425\n",
      "The training loss is 5.433708388592216 with std:8.25223844414924. The val loss is 10.606089741326612 with std:29.089871205591592.\n",
      "10.606089741326612 0.012052609368708425\n",
      "Evaluating for {'lmda': 0.01216424293857368} ...\n",
      "The training loss is 5.829093632215584 with std:12.866775282486202. The val loss is 635.2837240554662 with std:5978.260151841314.\n",
      "635.2837240554662 0.01216424293857368\n",
      "The training loss is 6.08940747340792 with std:11.014891074158257. The val loss is 11.487001434257934 with std:40.10088245829692.\n",
      "11.487001434257934 0.01216424293857368\n",
      "The training loss is 6.277224268840398 with std:13.173137265132539. The val loss is 7.705979690529922 with std:11.530546818945195.\n",
      "7.705979690529922 0.01216424293857368\n",
      "The training loss is 5.43519355008317 with std:8.253747894124487. The val loss is 10.601162580112296 with std:29.070249686896496.\n",
      "10.601162580112296 0.01216424293857368\n",
      "Evaluating for {'lmda': 0.012276910479883591} ...\n",
      "The training loss is 5.831080350807701 with std:12.867306585680554. The val loss is 626.1479544222788 with std:5891.11409527874.\n",
      "626.1479544222788 0.012276910479883591\n",
      "The training loss is 6.090446716195468 with std:11.015323936461197. The val loss is 11.494705844405907 with std:40.16391996941124.\n",
      "11.494705844405907 0.012276910479883591\n",
      "The training loss is 6.278540562368706 with std:13.174233330618886. The val loss is 7.706321733169846 with std:11.525750098610668.\n",
      "7.706321733169846 0.012276910479883591\n",
      "The training loss is 5.436678329543146 with std:8.255259049031702. The val loss is 10.596197142374285 with std:29.050488762208083.\n",
      "10.596197142374285 0.012276910479883591\n",
      "Evaluating for {'lmda': 0.012390621569479163} ...\n",
      "The training loss is 5.833063808398046 with std:12.867811057551702. The val loss is 617.1041328267576 with std:5804.845543803559.\n",
      "617.1041328267576 0.012390621569479163\n",
      "The training loss is 6.09148164115038 with std:11.01574755089545. The val loss is 11.502401476605687 with std:40.22687685906637.\n",
      "11.502401476605687 0.012390621569479163\n",
      "The training loss is 6.279852733619915 with std:13.175313506647514. The val loss is 7.706651171619508 with std:11.520936644830176.\n",
      "7.706651171619508 0.012390621569479163\n",
      "The training loss is 5.438162816867907 with std:8.256772026802123. The val loss is 10.591193588370238 with std:29.03058940274536.\n",
      "10.591193588370238 0.012390621569479163\n",
      "Evaluating for {'lmda': 0.012505385872903908} ...\n",
      "The training loss is 5.835044025101651 with std:12.868288763779816. The val loss is 608.1520246063877 with std:5719.4522544685215.\n",
      "608.1520246063877 0.012505385872903908\n",
      "The training loss is 6.092512297293784 with std:11.016161956819598. The val loss is 11.510088645510177 with std:40.289755535147634.\n",
      "11.510088645510177 0.012505385872903908\n",
      "The training loss is 6.281160832529638 with std:13.17637782846759. The val loss is 7.70696798883142 with std:11.516106058333875.\n",
      "7.70696798883142 0.012505385872903908\n",
      "The training loss is 5.439647102665257 with std:8.25828694652107. The val loss is 10.586152083815731 with std:29.010552604164253.\n",
      "10.586152083815731 0.012505385872903908\n",
      "Evaluating for {'lmda': 0.012621213145225473} ...\n",
      "The training loss is 5.837021021673077 with std:12.868739771439738. The val loss is 599.2913833347694 with std:5634.931872050972.\n",
      "599.2913833347694 0.012621213145225473\n",
      "The training loss is 6.093538733855984 with std:11.016567193893865. The val loss is 11.517767667353946 with std:40.35255842183909.\n",
      "11.517767667353946 0.012621213145225473\n",
      "The training loss is 6.282464909611781 with std:13.177426332124389. The val loss is 7.7072721683997845 with std:11.511257941951781.\n",
      "7.7072721683997845 0.012621213145225473\n",
      "The training loss is 5.4411312782397685 with std:8.259803928403697. The val loss is 10.581072799898088 with std:28.990379386377604.\n",
      "10.581072799898088 0.012621213145225473\n",
      "Evaluating for {'lmda': 0.012738113231864785} ...\n",
      "The training loss is 5.8389948195099075 with std:12.869164149003897. The val loss is 590.5219510134601 with std:5551.2819308844455.\n",
      "590.5219510134601 0.012738113231864785\n",
      "The training loss is 6.094561000275899 with std:11.016963302086198. The val loss is 11.525438859871016 with std:40.41528795884507.\n",
      "11.525438859871016 0.012738113231864785\n",
      "The training loss is 6.283765015949026 with std:13.178459054438711. The val loss is 7.707563694552818 with std:11.506391900636148.\n",
      "7.707563694552818 0.012738113231864785\n",
      "The training loss is 5.442615435587126 with std:8.2613230937962. The val loss is 10.575955913302426 with std:28.97007079357281.\n",
      "10.575955913302426 0.012738113231864785\n",
      "Evaluating for {'lmda': 0.012856096069432959} ...\n",
      "The training loss is 5.840965440645579 with std:12.869561966325884. The val loss is 581.8434582617563 with std:5468.499856670665.\n",
      "581.8434582617563 0.012856096069432959\n",
      "The training loss is 6.095579146197003 with std:11.017350321676501. The val loss is 11.533102542292516 with std:40.477946601412754.\n",
      "11.533102542292516 0.012856096069432959\n",
      "The training loss is 6.2850612031891195 with std:13.179476033017183. The val loss is 7.707842552126057 with std:11.501507541421827.\n",
      "7.707842552126057 0.012856096069432959\n",
      "The training loss is 5.444099667381737 with std:8.262844565165793. The val loss is 10.57080160626059 with std:28.949627894271284.\n",
      "10.57080160626059 0.012856096069432959\n",
      "Evaluating for {'lmda': 0.012975171686575875} ...\n",
      "The training loss is 5.842932907747819 with std:12.869933294640342. The val loss is 573.2556245148974 with std:5386.582968371747.\n",
      "573.2556245148974 0.012975171686575875\n",
      "The training loss is 6.096593221465946 with std:11.017728293256324. The val loss is 11.540759035334379 with std:40.54053682018035.\n",
      "11.540759035334379 0.012975171686575875\n",
      "The training loss is 6.286353523546409 with std:13.180477306251055. The val loss is 7.708108726593085 with std:11.496604473572924.\n",
      "7.708108726593085 0.012975171686575875\n",
      "The training loss is 5.4455840669760045 with std:8.264368466090971. The val loss is 10.565610066516875 with std:28.929051781020284.\n",
      "10.565610066516875 0.012975171686575875\n",
      "Evaluating for {'lmda': 0.01309535020482667} ...\n",
      "The training loss is 5.844897244112759 with std:12.870278206551522. The val loss is 564.7581582068552 with std:5305.528479955303.\n",
      "564.7581582068552 0.01309535020482667\n",
      "The training loss is 6.0976032761267005 with std:11.01809725771684. The val loss is 11.548408661150434 with std:40.60306110076194.\n",
      "11.548408661150434 0.01309535020482667\n",
      "The training loss is 6.287642029794194 with std:13.181462913310748. The val loss is 7.708362204026913 with std:11.491682308488686.\n",
      "7.708362204026913 0.01309535020482667\n",
      "The training loss is 5.447068728377631 with std:8.265894921254434. The val loss is 10.560381487389028 with std:28.90834357049093.\n",
      "10.560381487389028 0.01309535020482667\n",
      "Evaluating for {'lmda': 0.013216641839466052} ...\n",
      "The training loss is 5.846858473662925 with std:12.870596776037686. The val loss is 556.350756977945 with std:5225.333502376419.\n",
      "556.350756977945 0.013216641839466052\n",
      "The training loss is 6.0986093604189815 with std:11.018457256264957. The val loss is 11.55605174329811 with std:40.66552194348252.\n",
      "11.55605174329811 0.013216641839466052\n",
      "The training loss is 6.288926775256093 with std:13.182432894145988. The val loss is 7.708602971101728 with std:11.486740659809678.\n",
      "7.708602971101728 0.013216641839466052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 5.448553746249973 with std:8.267424056430304. The val loss is 10.55511606780221 with std:28.887504403490684.\n",
      "10.55511606780221 0.013216641839466052\n",
      "Evaluating for {'lmda': 0.013339056900390587} ...\n",
      "The training loss is 5.848816620944126 with std:12.870889078440273. The val loss is 548.0331078652118 with std:5145.99504539527.\n",
      "548.0331078652118 0.013339056900390587\n",
      "The training loss is 6.099611524775151 with std:11.018808330418855. The val loss is 11.563688606725222 with std:40.727921863175155.\n",
      "11.563688606725222 0.013339056900390587\n",
      "The training loss is 6.290207813811686 with std:13.183387289492686. The val loss is 7.708831015105678 with std:11.48177914343503.\n",
      "7.708831015105678 0.013339056900390587\n",
      "The training loss is 5.450039215898107 with std:8.268955998481324. The val loss is 10.549814012252126 with std:28.866535444603162.\n",
      "10.549814012252126 0.013339056900390587\n",
      "Evaluating for {'lmda': 0.013462605792989104} ...\n",
      "The training loss is 5.850771711117134 with std:12.871155190462664. The val loss is 539.8048875041494 with std:5067.5100195030345.\n",
      "539.8048875041494 0.013462605792989104\n",
      "The training loss is 6.100609819816205 with std:11.019150522005196. The val loss is 11.571319577737935 with std:40.790263389029725.\n",
      "11.571319577737935 0.013462605792989104\n",
      "The training loss is 6.2914851998825085 with std:13.184326140862053. The val loss is 7.709046323905072 with std:11.47679737751673.\n",
      "7.709046323905072 0.013462605792989104\n",
      "The training loss is 5.4515252332585336 with std:8.270490875342736. The val loss is 10.544475530886887 with std:28.845437882403363.\n",
      "10.544475530886887 0.013462605792989104\n",
      "Evaluating for {'lmda': 0.013587299019027098} ...\n",
      "The training loss is 5.852723769958433 with std:12.87139519015229. The val loss is 531.6657623179277 with std:4989.875237728225.\n",
      "531.6657623179277 0.013587299019027098\n",
      "The training loss is 6.101604296348514 with std:11.019483873166253. The val loss is 11.578944983955347 with std:40.85254906403567.\n",
      "11.578944983955347 0.013587299019027098\n",
      "The training loss is 6.292758988435918 with std:13.185249490550794. The val loss is 7.709248885955824 with std:11.471794982485859.\n",
      "7.709248885955824 0.013587299019027098\n",
      "The training loss is 5.453011894892628 with std:8.272028816018933. The val loss is 10.53910083947839 with std:28.824212929152058.\n",
      "10.53910083947839 0.013587299019027098\n",
      "Evaluating for {'lmda': 0.013713147177539449} ...\n",
      "The training loss is 5.854672823853821 with std:12.871609156915127. The val loss is 523.615388729114 with std:4913.087417657932.\n",
      "523.615388729114 0.013713147177539449\n",
      "The training loss is 6.102595005362152 with std:11.019808426351702. The val loss is 11.5865651543032 with std:40.91478144504685.\n",
      "11.5865651543032 0.013713147177539449\n",
      "The training loss is 6.294029234975341 with std:13.186157381633206. The val loss is 7.7094386903001455 with std:11.466771581146386.\n",
      "7.7094386903001455 0.013713147177539449\n",
      "The training loss is 5.454499297971684 with std:8.273569950567225. The val loss is 10.533690159469906 with std:28.80286182080444.\n",
      "10.533690159469906 0.013713147177539449\n",
      "Evaluating for {'lmda': 0.013840160965731315} ...\n",
      "The training loss is 5.856618899793832 with std:12.871797171499146. The val loss is 515.6534133512188 with std:4837.14318326657.\n",
      "515.6534133512188 0.013840160965731315\n",
      "The training loss is 6.103581998025592 with std:11.020124224327407. The val loss is 11.59418041900701 with std:40.97696310267968.\n",
      "11.59418041900701 0.013840160965731315\n",
      "The training loss is 6.295295995539147 with std:13.187049857965077. The val loss is 7.709615726542961 with std:11.461726798608215.\n",
      "7.709615726542961 0.013840160965731315\n",
      "The training loss is 5.455987540269697 with std:8.275114410094906. The val loss is 10.528243717964116 with std:28.781385816790962.\n",
      "10.528243717964116 0.013840160965731315\n",
      "Evaluating for {'lmda': 0.013968351179887397} ...\n",
      "The training loss is 5.858562025369791 with std:12.871959315988162. The val loss is 507.77947319405735 with std:4762.039066876517.\n",
      "507.77947319405735 0.013968351179887397\n",
      "The training loss is 6.10456532568269 with std:11.020431310172437. The val loss is 11.601791109502287 with std:41.03909662052106.\n",
      "11.601791109502287 0.013968351179887397\n",
      "The training loss is 6.296559326695006 with std:13.187926964177958. The val loss is 7.7097799848672075 with std:11.456660262384574.\n",
      "7.7097799848672075 0.013968351179887397\n",
      "The training loss is 5.457476720153747 with std:8.276662326742745. The val loss is 10.522761747777961 with std:28.759786200089273.\n",
      "10.522761747777961 0.013968351179887397\n",
      "Evaluating for {'lmda': 0.01409772871628967} ...\n",
      "The training loss is 5.860502228768406 with std:12.872095673804052. The val loss is 499.9931958597088 with std:4687.7715110283525.\n",
      "499.9931958597088 0.01409772871628967\n",
      "The training loss is 6.105545039851709 with std:11.020729727273062. The val loss is 11.60939755845994 with std:41.10118459535474.\n",
      "11.60939755845994 0.01409772871628967\n",
      "The training loss is 6.297819285538156 with std:13.188788745684676. The val loss is 7.709931456002478 with std:11.451571602345206.\n",
      "7.709931456002478 0.01409772871628967\n",
      "The training loss is 5.4589669365701585 with std:8.278213833678507. The val loss is 10.517244487410018 with std:28.73806427685876.\n",
      "10.517244487410018 0.01409772871628967\n",
      "Evaluating for {'lmda': 0.014228304572143526} ...\n",
      "The training loss is 5.8624395387691575 with std:12.87220632969707. The val loss is 492.2941997503367 with std:4614.336870465466.\n",
      "492.2941997503367 0.014228304572143526\n",
      "The training loss is 6.106521192217423 with std:11.021019519338466. The val loss is 11.617000099747456 with std:41.16322963685494.\n",
      "11.617000099747456 0.014228304572143526\n",
      "The training loss is 6.299075929684401 with std:13.189635248685724. The val loss is 7.710070131247021 with std:11.446460450835902.\n",
      "7.710070131247021 0.014228304572143526\n",
      "The training loss is 5.460458289035036 with std:8.279769065083935. The val loss is 10.511692181094073 with std:28.716221376474813.\n",
      "10.511692181094073 0.014228304572143526\n",
      "Evaluating for {'lmda': 0.014360089846512599} ...\n",
      "The training loss is 5.864373984734501 with std:12.872291369736022. The val loss is 484.68209426556433 with std:4541.731414017887.\n",
      "484.68209426556433 0.014360089846512599\n",
      "The training loss is 6.107493834632765 with std:11.021300730388306. The val loss is 11.624599068391737 with std:41.22523436722665.\n",
      "11.624599068391737 0.014360089846512599\n",
      "The training loss is 6.30032931726753 with std:13.190466520154683. The val loss is 7.710196002439029 with std:11.44132644262595.\n",
      "7.710196002439029 0.014360089846512599\n",
      "The training loss is 5.46195087762529 with std:8.28132815614714. The val loss is 10.506105078793059 with std:28.694258851335377.\n",
      "10.506105078793059 0.014360089846512599\n",
      "Evaluating for {'lmda': 0.014493095741262165} ...\n",
      "The training loss is 5.866305596613073 with std:12.872350881317175. The val loss is 477.15648000843015 with std:4469.951326568551.\n",
      "477.15648000843015 0.014493095741262165\n",
      "The training loss is 6.108463019113379 with std:11.021573404757222. The val loss is 11.632194800579493 with std:41.287201421242216.\n",
      "11.632194800579493 0.014493095741262165\n",
      "The training loss is 6.301579506935184 with std:13.191282607841137. The val loss is 7.710309061953536 with std:11.436169214939598.\n",
      "7.710309061953536 0.014493095741262165\n",
      "The training loss is 5.463444802968228 with std:8.282891243049093. The val loss is 10.50048343621628 with std:28.672178076710537.\n",
      "10.50048343621628 0.014493095741262165\n",
      "Evaluating for {'lmda': 0.0146273335620113} ...\n",
      "The training loss is 5.868234404927088 with std:12.872384953153333. The val loss is 469.71694898473527 with std:4398.992710956446.\n",
      "469.71694898473527 0.0146273335620113\n",
      "The training loss is 6.109428797831565 with std:11.021837587093804. The val loss is 11.639787633575391 with std:41.34913344552251.\n",
      "11.639787633575391 0.0146273335620113\n",
      "The training loss is 6.302826557845179 with std:13.192083560288278. The val loss is 7.710409302714031 with std:11.430988407546398.\n",
      "7.710409302714031 0.0146273335620113\n",
      "The training loss is 5.464940166224353 with std:8.284458462952623. The val loss is 10.494827514837162 with std:28.649980450593308.\n",
      "10.494827514837162 0.0146273335620113\n",
      "Evaluating for {'lmda': 0.014762814719093903} ...\n",
      "The training loss is 5.870160440768781 with std:12.872393675253837. The val loss is 462.36308480541163 with std:4328.851589908469.\n",
      "462.36308480541163 0.014762814719093903\n",
      "The training loss is 6.110391223117482 with std:11.02209332236439. The val loss is 11.647377905779868 with std:41.41103309904735.\n",
      "11.647377905779868 0.014762814719093903\n",
      "The training loss is 6.304070529660678 with std:13.19286942680868. The val loss is 7.710496718155288 with std:11.425783662687982.\n",
      "7.710496718155288 0.014762814719093903\n",
      "The training loss is 5.466437069084229 with std:8.286029953991912. The val loss is 10.489137581877728 with std:28.62766739347756.\n",
      "10.489137581877728 0.014762814719093903\n",
      "Evaluating for {'lmda': 0.014899550728528544} ...\n",
      "The training loss is 5.872083735798877 with std:12.872377138946884. The val loss is 455.09446289384925 with std:4259.523908018649.\n",
      "455.09446289384925 0.014899550728528544\n",
      "The training loss is 6.111350347452908 with std:11.022340655853641. The val loss is 11.654965956609878 with std:41.472903052145355.\n",
      "11.654965956609878 0.014899550728528544\n",
      "The training loss is 6.305311482546053 with std:13.193640257503993. The val loss is 7.710571302255739 with std:11.420554625213988.\n",
      "7.710571302255739 0.014899550728528544\n",
      "The training loss is 5.467935613751016 with std:8.287605855256611. The val loss is 10.483413910368977 with std:28.60524034831192.\n",
      "10.483413910368977 0.014899550728528544\n",
      "Evaluating for {'lmda': 0.015037553212997377} ...\n",
      "The training loss is 5.874004322238721 with std:12.87233543685973. The val loss is 447.9106506825812 with std:4191.00553362582.\n",
      "447.9106506825812 0.015037553212997377\n",
      "The training loss is 6.112306223469359 with std:11.02257963315739. The val loss is 11.662552126558323 with std:41.534745986964595.\n",
      "11.662552126558323 0.015037553212997377\n",
      "The training loss is 6.306549477165017 with std:13.194396103255182. The val loss is 7.710633049488058 with std:11.415300942475852.\n",
      "7.710633049488058 0.015037553212997377\n",
      "The training loss is 5.469435902934583 with std:8.289186306790203. The val loss is 10.477656779090612 with std:28.582700780102712.\n",
      "10.477656779090612 0.015037553212997377\n",
      "Evaluating for {'lmda': 0.015176833902834053} ...\n",
      "The training loss is 5.875922232863906 with std:12.87226866291354. The val loss is 440.81120781980803 with std:4123.2922607851315.\n",
      "440.81120781980803 0.015176833902834053\n",
      "The training loss is 6.11325890394194 with std:11.022810300190207. The val loss is 11.670136757118849 with std:41.596564596924345.\n",
      "11.670136757118849 0.015176833902834053\n",
      "The training loss is 6.307784574674369 with std:13.195137015729177. The val loss is 7.710681954848806 with std:11.410022264469756.\n",
      "7.710681954848806 0.015176833902834053\n",
      "The training loss is 5.470938039834558 with std:8.290771449569938. The val loss is 10.471866472644326 with std:28.56005017597679.\n",
      "10.471866472644326 0.015176833902834053\n",
      "Evaluating for {'lmda': 0.015317404637020799} ...\n",
      "The training loss is 5.877837500997779 with std:12.8721769123168. The val loss is 433.7956863664746 with std:4056.379811149668.\n",
      "433.7956863664746 0.015317404637020799\n",
      "The training loss is 6.114208441789326 with std:11.023032703186631. The val loss is 11.67772019076869 with std:41.658361586487.\n",
      "11.67772019076869 0.015317404637020799\n",
      "The training loss is 6.30901683672049 with std:13.19586304737306. The val loss is 7.710718013839146 with std:11.404718243823185.\n",
      "7.710718013839146 0.015317404637020799\n",
      "The training loss is 5.472442128132526 with std:8.292361425492553. The val loss is 10.466043281421571 with std:28.53729004477314.\n",
      "10.466043281421571 0.015317404637020799\n",
      "Evaluating for {'lmda': 0.015459277364194777} ...\n",
      "The training loss is 5.879750160512228 with std:12.872060281578081. The val loss is 426.8636310066887 with std:3990.263835978732.\n",
      "426.8636310066887 0.015459277364194777\n",
      "The training loss is 6.115154890067784 with std:11.023246888694832. The val loss is 11.685302770959511 with std:41.72013967112771.\n",
      "11.685302770959511 0.015459277364194777\n",
      "The training loss is 6.3102463254372765 with std:13.196574251423398. The val loss is 7.710741222452084 with std:11.399388535770992.\n",
      "7.710741222452084 0.015459277364194777\n",
      "The training loss is 5.473948271978093 with std:8.293956377369787. The val loss is 10.460187501623944 with std:28.514421916940712.\n",
      "10.460187501623944 0.015459277364194777\n",
      "Evaluating for {'lmda': 0.015602464143663687} ...\n",
      "The training loss is 5.881660245812017 with std:12.87191886846962. The val loss is 420.01457923788735 with std:3924.939917953846.\n",
      "420.01457923788735 0.015602464143663687\n",
      "The training loss is 6.116098301968024 with std:11.023452903580278. The val loss is 11.69288484205833 with std:41.78190157683963.\n",
      "11.69288484205833 0.015602464143663687\n",
      "The training loss is 6.31147310343889 with std:13.197270681895539. The val loss is 7.710751577173865 with std:11.394032798248444.\n",
      "7.710751577173865 0.015602464143663687\n",
      "The training loss is 5.475456575980581 with std:8.295556448912995. The val loss is 10.454299435256974 with std:28.491447344270263.\n",
      "10.454299435256974 0.015602464143663687\n",
      "Evaluating for {'lmda': 0.01574697714643086} ...\n",
      "The training loss is 5.883567791839334 with std:12.871752772062244. The val loss is 413.24806158092633 with std:3860.403573183855.\n",
      "413.24806158092633 0.01574697714643086\n",
      "The training loss is 6.117038730813579 with std:11.023650795034769. The val loss is 11.70046674938607 with std:41.84365004048301.\n",
      "11.70046674938607 0.01574697714643086\n",
      "The training loss is 6.312697233818871 with std:13.197952393599563. The val loss is 7.710749074979983 with std:11.388650691877798.\n",
      "7.710749074979983 0.01574697714643086\n",
      "The training loss is 5.476967145189524 with std:8.297161784713534. The val loss is 10.44837939014364 with std:28.468367899680903.\n",
      "10.44837939014364 0.01574697714643086\n",
      "Evaluating for {'lmda': 0.01589282865622978} ...\n",
      "The training loss is 5.885472834057061 with std:12.871562092682383. The val loss is 406.5636017751882 with std:3796.6502530678185.\n",
      "406.5636017751882 0.01589282865622978\n",
      "The training loss is 6.117976230054519 with std:11.023840610556608. The val loss is 11.708048839115946 with std:41.90538780889933.\n",
      "11.708048839115946 0.01589282865622978\n",
      "The training loss is 6.313918780146684 with std:13.198619442128717. The val loss is 7.710733713324003 with std:11.383241880006967.\n",
      "7.710733713324003 0.01589282865622978\n",
      "The training loss is 5.478480085093447 with std:8.29877253024317. The val loss is 10.442427679926643 with std:28.445185177008334.\n",
      "10.442427679926643 0.01589282865622978\n",
      "Evaluating for {'lmda': 0.0160400310705682} ...\n",
      "The training loss is 5.887375408455208 with std:12.871346931940868. The val loss is 399.9607169804539 with std:3733.675346221986.\n",
      "399.9607169804539 0.0160400310705682\n",
      "The training loss is 6.118910853265577 with std:11.024022397969754. The val loss is 11.715631458302422 with std:41.96711763919543.\n",
      "11.715631458302422 0.0160400310705682\n",
      "The training loss is 6.315137806460937 with std:13.19927188386527. The val loss is 7.710705490138186 with std:11.377806028744926.\n",
      "7.710705490138186 0.0160400310705682\n",
      "The training loss is 5.4799955015966955 with std:8.300388831829535. The val loss is 10.436444624063695 with std:28.421900790716506.\n",
      "10.436444624063695 0.0160400310705682\n",
      "Evaluating for {'lmda': 0.016188596901781985} ...\n",
      "The training loss is 5.88927555152982 with std:12.87110739270001. The val loss is 393.43891797618693 with std:3671.4741803824663.\n",
      "393.43891797618693 0.016188596901781985\n",
      "The training loss is 6.119842654142121 with std:11.024196205417361. The val loss is 11.723214954856145 with std:42.028842298538095.\n",
      "11.723214954856145 0.016188596901781985\n",
      "The training loss is 6.316354377267488 with std:13.199909775983775. The val loss is 7.710664403815944 with std:11.372342806946705.\n",
      "7.710664403815944 0.016188596901781985\n",
      "The training loss is 5.481513501016305 with std:8.30201083664709. The val loss is 10.430430547823715 with std:28.398516375642245.\n",
      "10.430430547823715 0.016188596901781985\n",
      "Evaluating for {'lmda': 0.016338538778098613} ...\n",
      "The training loss is 5.891173300291504 with std:12.870843579093213. The val loss is 386.9977093553987 with std:3610.042024255724.\n",
      "386.9977093553987 0.016338538778098613\n",
      "The training loss is 6.1207716864963055 with std:11.024362081357594. The val loss is 11.73079967747529 with std:42.09056456360496.\n",
      "11.73079967747529 0.016338538778098613\n",
      "The training loss is 6.317568557536942 with std:13.200533176449193. The val loss is 7.710610453220273 with std:11.366851886282012.\n",
      "7.710610453220273 0.016338538778098613\n",
      "The training loss is 5.483034190061897 with std:8.303638692704371. The val loss is 10.42438578230875 with std:28.37503358679714.\n",
      "10.42438578230875 0.016338538778098613\n",
      "Evaluating for {'lmda': 0.016489869444710648} ...\n",
      "The training loss is 5.893068692248372 with std:12.870555596500237. The val loss is 380.6365897272568 with std:3549.3740894529715.\n",
      "380.6365897272568 0.016489869444710648\n",
      "The training loss is 6.121698004254092 with std:11.02452007457243. The val loss is 11.738385975679323 with std:42.152287220823894.\n",
      "11.738385975679323 0.016489869444710648\n",
      "The training loss is 6.318780412698288 with std:13.201142144023022. The val loss is 7.710543637672537 with std:11.361332941262415.\n",
      "7.710543637672537 0.016489869444710648\n",
      "The training loss is 5.484557675828002 with std:8.305272548826395. The val loss is 10.418310664419435 with std:28.351454098976063.\n",
      "10.418310664419435 0.016489869444710648\n",
      "Evaluating for {'lmda': 0.016642601764859037} ...\n",
      "The training loss is 5.894961765406547 with std:12.870243551556028. The val loss is 374.3550519112078 with std:3489.46553234324.\n",
      "374.3550519112078 0.016642601764859037\n",
      "The training loss is 6.122621661451478 with std:11.024670234159808. The val loss is 11.745974199744103 with std:42.21401306590259.\n",
      "11.745974199744103 0.016642601764859037\n",
      "The training loss is 6.319990008639713 with std:13.201736738259163. The val loss is 7.710463956942506 with std:11.355785649211768.\n",
      "7.710463956942506 0.016642601764859037\n",
      "The training loss is 5.4860840657842616 with std:8.306912554652323. The val loss is 10.412205536879865 with std:28.32777960660072.\n",
      "10.412205536879865 0.016642601764859037\n",
      "Evaluating for {'lmda': 0.01679674872092653} ...\n",
      "The training loss is 5.896852558257855 with std:12.869907552148215. The val loss is 368.15258313226155 with std:3430.3114559177293.\n",
      "368.15258313226155 0.01679674872092653\n",
      "The training loss is 6.1235427122292805 with std:11.024812609537822. The val loss is 11.753564700703517 with std:42.275744903847766.\n",
      "11.753564700703517 0.01679674872092653\n",
      "The training loss is 6.321197411696903 with std:13.202317019505577. The val loss is 7.710371411250997 with std:11.35020969037747.\n",
      "7.710371411250997 0.01679674872092653\n",
      "The training loss is 5.487613467750528 with std:8.308558860605277. The val loss is 10.406070748213388 with std:28.30401182334065.\n",
      "10.406070748213388 0.01679674872092653\n",
      "Evaluating for {'lmda': 0.016952323415541204} ...\n",
      "The training loss is 5.898741109778061 with std:12.869547707392996. The val loss is 362.0286652123477 with std:3371.9069116166884.\n",
      "362.0286652123477 0.016952323415541204\n",
      "The training loss is 6.124461210832469 with std:11.024947250448687. The val loss is 11.761157830303068 with std:42.337485548557225.\n",
      "11.761157830303068 0.016952323415541204\n",
      "The training loss is 6.322402688659563 with std:13.202883048914014. The val loss is 7.710266001260556 with std:11.344604747887644.\n",
      "7.710266001260556 0.016952323415541204\n",
      "The training loss is 5.489145989899482 with std:8.310211617886889. The val loss is 10.399906652743354 with std:28.280152481804745.\n",
      "10.399906652743354 0.016952323415541204\n",
      "Evaluating for {'lmda': 0.017109339072690143} ...\n",
      "The training loss is 5.900627459417795 with std:12.869164127657449. The val loss is 355.98277477066955 with std:3314.2469012419147.\n",
      "355.98277477066955 0.017109339072690143\n",
      "The training loss is 6.125377211603669 with std:11.025074206947634. The val loss is 11.768753940992095 with std:42.39923782280189.\n",
      "11.768753940992095 0.017109339072690143\n",
      "The training loss is 6.323605906760649 with std:13.203434888433293. The val loss is 7.710147728068477 with std:11.338970507808476.\n",
      "7.710147728068477 0.017109339072690143\n",
      "The training loss is 5.490681740733681 with std:8.311870978468711. The val loss is 10.3937136105962 with std:28.256203333321952.\n",
      "10.3937136105962 0.017109339072690143\n",
      "Evaluating for {'lmda': 0.01726780903884356} ...\n",
      "The training loss is 5.902511647094591 with std:12.868756924534518. The val loss is 350.01438340846715 with std:3257.3263787207625.\n",
      "350.01438340846715 0.01726780903884356\n",
      "The training loss is 6.126290768980881 with std:11.025193529418413. The val loss is 11.776353385901448 with std:42.4610045580518.\n",
      "11.776353385901448 0.01726780903884356\n",
      "The training loss is 6.324807133676036 with std:13.203972600817233. The val loss is 7.710016593204389 with std:11.333306659161204.\n",
      "7.710016593204389 0.01726780903884356\n",
      "The training loss is 5.492220829073056 with std:8.313537095063566. The val loss is 10.387491987674464 with std:28.23216614750284.\n",
      "10.387491987674464 0.01726780903884356\n",
      "Evaluating for {'lmda': 0.01742774678408919} ...\n",
      "The training loss is 5.9043937131910305 with std:12.868326210848064. The val loss is 344.1229579013109 with std:3201.1402519419844.\n",
      "344.1229579013109 0.01742774678408919\n",
      "The training loss is 6.127201937492942 with std:11.025305268557492. The val loss is 11.783956518813016 with std:42.522788594207526.\n",
      "11.783956518813016 0.01742774678408919\n",
      "The training loss is 6.326006437519956 with std:13.204496249620105. The val loss is 7.709872598630538 with std:11.327612893977511.\n",
      "7.709872598630538 0.01742774678408919\n",
      "The training loss is 5.493763364047666 with std:8.315210121123156. The val loss is 10.381242155662358 with std:28.208042712000964.\n",
      "10.381242155662358 0.01742774678408919\n",
      "Evaluating for {'lmda': 0.017589165903277325} ...\n",
      "The training loss is 5.9062736985408595 with std:12.867872100645025. The val loss is 338.30796039271945 with std:3145.6833846039476.\n",
      "338.30796039271945 0.017589165903277325\n",
      "The training loss is 6.128110771756949 with std:11.025409475386448. The val loss is 11.791563694144159 with std:42.58459277956296.\n",
      "11.791563694144159 0.017589165903277325\n",
      "The training loss is 6.327203886841492 with std:13.205005899204297. The val loss is 7.7097157467223125 with std:11.3218889072742.\n",
      "7.7097157467223125 0.017589165903277325\n",
      "The training loss is 5.495309455078743 with std:8.316890210818956. The val loss is 10.374964492006429 with std:28.183834832098146.\n",
      "10.374964492006429 0.017589165903277325\n",
      "Evaluating for {'lmda': 0.017752080117176352} ...\n",
      "The training loss is 5.908151644427382 with std:12.867394709192995. The val loss is 332.568848575254 with std:3090.9505979436944.\n",
      "332.568848575254 0.017752080117176352\n",
      "The training loss is 6.129017326473873 with std:11.025506201246708. The val loss is 11.799175266911648 with std:42.64641997043816.\n",
      "11.799175266911648 0.017752080117176352\n",
      "The training loss is 6.328399550623319 with std:13.20550161474128. The val loss is 7.709546040283761 with std:11.316134397148934.\n",
      "7.709546040283761 0.017752080117176352\n",
      "The training loss is 5.496859211866662 with std:8.318577519025508. The val loss is 10.36865937991496 with std:28.15954433046339.\n",
      "10.36865937991496 0.017752080117176352\n",
      "Evaluating for {'lmda': 0.017916503273639004} ...\n",
      "The training loss is 5.910027592574917 with std:12.866894152976576. The val loss is 326.90507588366404 with std:3036.9366725807226.\n",
      "326.90507588366404 0.017916503273639004\n",
      "The training loss is 6.129921656424052 with std:11.025595497801898. The val loss is 11.806791592717065 with std:42.70827303112052.\n",
      "11.806791592717065 0.017916503273639004\n",
      "The training loss is 6.329593498276216 with std:13.205983462215753. The val loss is 7.709363482526197 with std:11.31034906475074.\n",
      "7.709363482526197 0.017916503273639004\n",
      "The training loss is 5.498412744380971 with std:8.320272201309463. The val loss is 10.362327208314388 with std:28.135173046608372.\n",
      "10.362327208314388 0.017916503273639004\n",
      "Evaluating for {'lmda': 0.018082449348779516} ...\n",
      "The training loss is 5.9119015851412255 with std:12.866370549689437. The val loss is 321.3160916759284 with std:2983.636350245461.\n",
      "321.3160916759284 0.018082449348779516\n",
      "The training loss is 6.130823816464722 with std:11.025677417032167. The val loss is 11.814413027739628 with std:42.770154833825586.\n",
      "11.814413027739628 0.018082449348779516\n",
      "The training loss is 6.3307857996372405 with std:13.20645150842361. The val loss is 7.709168077072214 with std:11.304532614349624.\n",
      "7.709168077072214 0.018082449348779516\n",
      "The training loss is 5.499970162839838 with std:8.321974413908777. The val loss is 10.35596837188123 with std:28.11072283675141.\n",
      "10.35596837188123 0.018082449348779516\n",
      "Evaluating for {'lmda': 0.018249932448161525} ...\n",
      "The training loss is 5.91377366470968 with std:12.865824018233601. The val loss is 315.80134141608784 with std:2931.0443355247176.\n",
      "315.80134141608784 0.018249932448161525\n",
      "The training loss is 6.131723861525177 with std:11.025752011243817. The val loss is 11.822039928678917 with std:42.83206825824784.\n",
      "11.822039928678917 0.018249932448161525\n",
      "The training loss is 6.3319765249641105 with std:13.206905820976. The val loss is 7.7089598279474725 with std:11.298684753336582.\n",
      "7.7089598279474725 0.018249932448161525\n",
      "The training loss is 5.501531577704765 with std:8.323684313724396. The val loss is 10.349583270981624 with std:28.08619557324299.\n",
      "10.349583270981624 0.018249932448161525\n",
      "Evaluating for {'lmda': 0.01841896680799711} ...\n",
      "The training loss is 5.915643874283993 with std:12.86525467870549. The val loss is 310.3602668589784 with std:2879.1552976254584.\n",
      "310.3602668589784 0.01841896680799711\n",
      "The training loss is 6.132621846603103 with std:11.025819333063295. The val loss is 11.829672652756077 with std:42.89401619156199.\n",
      "11.829672652756077 0.01841896680799711\n",
      "The training loss is 6.333165744935398 with std:13.207346468301987. The val loss is 7.708738739582382 with std:11.292805192284808.\n",
      "7.708738739582382 0.01841896680799711\n",
      "The training loss is 5.503097099660365 with std:8.325402058297437. The val loss is 10.343172311678202 with std:28.061593144281005.\n",
      "10.343172311678202 0.01841896680799711\n",
      "Evaluating for {'lmda': 0.018589566796356875} ...\n",
      "The training loss is 5.917512257278503 with std:12.864662652406901. The val loss is 304.9923062313767 with std:2827.9638721039228.\n",
      "304.9923062313767 0.018589566796356875\n",
      "The training loss is 6.133517826762085 with std:11.025879435436627. The val loss is 11.837311557697525 with std:42.95600152833534.\n",
      "11.837311557697525 0.018589566796356875\n",
      "The training loss is 6.334353530646763 with std:13.207773519658337. The val loss is 7.708504816806214 with std:11.286893644965486.\n",
      "7.708504816806214 0.018589566796356875\n",
      "The training loss is 5.504666839603678 with std:8.327127805801116. The val loss is 10.336735905701966 with std:28.03691745347289.\n",
      "10.336735905701966 0.018589566796356875\n",
      "Evaluating for {'lmda': 0.018761746914391204} ...\n",
      "The training loss is 5.919378857512269 with std:12.864048061828147. The val loss is 299.69689440527526 with std:2777.4646625202845.\n",
      "299.69689440527526 0.018761746914391204\n",
      "The training loss is 6.134411857124833 with std:11.025932371631006. The val loss is 11.844957001678846 with std:43.01802717008559.\n",
      "11.844957001678846 0.018761746914391204\n",
      "The training loss is 6.335539953603751 with std:13.208187045117661. The val loss is 7.708258064834375 with std:11.28094982836696.\n",
      "7.708258064834375 0.018761746914391204\n",
      "The training loss is 5.5062409086275075 with std:8.328861715014162. The val loss is 10.330274470445945 with std:28.012170419492467.\n",
      "10.330274470445945 0.018761746914391204\n",
      "Evaluating for {'lmda': 0.018935521797562953} ...\n",
      "The training loss is 5.921243719202513 with std:12.863411030648882. The val loss is 294.4734630796945 with std:2727.652242174198.\n",
      "294.4734630796945 0.018935521797562953\n",
      "The training loss is 6.135303992872688 with std:11.025978195238466. The val loss is 11.852609343351117 with std:43.080096025519246.\n",
      "11.852609343351117 0.018935521797562953\n",
      "The training loss is 6.336725085726332 with std:13.208587115587187. The val loss is 7.707998489282572 with std:11.274973462756062.\n",
      "7.707998489282572 0.018935521797562953\n",
      "The training loss is 5.507819418013286 with std:8.330603945319465. The val loss is 10.32378842891515 with std:27.98735397556442.\n",
      "10.32378842891515 0.018935521797562953\n",
      "Evaluating for {'lmda': 0.01911090621689138} ...\n",
      "The training loss is 5.923106886953888 with std:12.862751683729927. The val loss is 289.3214409548862 with std:2678.5211557680213.\n",
      "289.3214409548862 0.01911090621689138\n",
      "The training loss is 6.136194289239404 with std:11.02601696016966. The val loss is 11.860268941762996 with std:43.14221100989458.\n",
      "11.860268941762996 0.01911090621689138\n",
      "The training loss is 6.337908999339232 with std:13.208973802798042. The val loss is 7.7077260961443 with std:11.26896427168098.\n",
      "7.7077260961443 0.01911090621689138\n",
      "The training loss is 5.509402479207189 with std:8.332354656675335. The val loss is 10.317278209725547 with std:27.96247006910497.\n",
      "10.317278209725547 0.01911090621689138\n",
      "Evaluating for {'lmda': 0.019287915080207777} ...\n",
      "The training loss is 5.92496840575685 with std:12.862070147114876. The val loss is 284.24025390284976 with std:2630.065921034923.\n",
      "284.24025390284976 0.019287915080207777\n",
      "The training loss is 6.137082801508256 with std:11.026048720657794. The val loss is 11.867936156384852 with std:43.204375045277935.\n",
      "11.867936156384852 0.019287915080207777\n",
      "The training loss is 6.339091767172717 with std:13.209347179321203. The val loss is 7.7074408918053 with std:11.262921982035051.\n",
      "7.7074408918053 0.019287915080207777\n",
      "The training loss is 5.5109902038136145 with std:8.33411400960476. The val loss is 10.31074424708028 with std:27.937520661304113.\n",
      "10.31074424708028 0.019287915080207777\n",
      "Evaluating for {'lmda': 0.019466563433422623} ...\n",
      "The training loss is 5.926828320970607 with std:12.861366548011095. The val loss is 279.22932514271247 with std:2582.281030413252.\n",
      "279.22932514271247 0.019466563433422623\n",
      "The training loss is 6.137969585007244 with std:11.026073531256548. The val loss is 11.87561134705854 with std:43.26659106011817.\n",
      "11.87561134705854 0.019466563433422623\n",
      "The training loss is 6.3402734623574615 with std:13.20970731855538. The val loss is 7.707142883024373 with std:11.25684632406016.\n",
      "7.707142883024373 0.019466563433422623\n",
      "The training loss is 5.512582703577411 with std:8.335882165180868. The val loss is 10.304186980728389 with std:27.912507726634246.\n",
      "10.304186980728389 0.019466563433422623\n",
      "Evaluating for {'lmda': 0.019646866461804455} ...\n",
      "The training loss is 5.928686678327147 with std:12.860641014808124. The val loss is 274.2880754065111 with std:2535.1609526292445.\n",
      "274.2880754065111 0.019646866461804455\n",
      "The training loss is 6.138854695107249 with std:11.02609144684602. The val loss is 11.883294873991977 with std:43.32886198925837.\n",
      "11.883294873991977 0.019646866461804455\n",
      "The training loss is 6.341454158425587 with std:13.210054294749213. The val loss is 7.7068320769468865 with std:11.25073703141442.\n",
      "7.7068320769468865 0.019646866461804455\n",
      "The training loss is 5.514180090370681 with std:8.337659285002783. The val loss is 10.297606855945984 with std:27.887433252410148.\n",
      "10.297606855945984 0.019646866461804455\n",
      "Evaluating for {'lmda': 0.019828839491270712} ...\n",
      "The training loss is 5.930543523913747 with std:12.859893677051513. The val loss is 269.4159231105499 with std:2488.7001343333836.\n",
      "269.4159231105499 0.019828839491270712\n",
      "The training loss is 6.139738187214681 with std:11.026102522620539. The val loss is 11.890987097720268 with std:43.391190773625524.\n",
      "11.890987097720268 0.019828839491270712\n",
      "The training loss is 6.342633929302718 with std:13.210388182985577. The val loss is 7.70650848108578 with std:11.244593841173623.\n",
      "7.70650848108578 0.019828839491270712\n",
      "The training loss is 5.515782476178271 with std:8.33944553119022. The val loss is 10.291004323504087 with std:27.86229923835216.\n",
      "10.291004323504087 0.019828839491270712\n",
      "Evaluating for {'lmda': 0.020012497989690372} ...\n",
      "The training loss is 5.932398904170165 with std:12.85912466544617. The val loss is 264.61228451786565 with std:2442.8930016513527.\n",
      "264.61228451786565 0.020012497989690372\n",
      "The training loss is 6.140620116770487 with std:11.026106814103942. The val loss is 11.898688379085756 with std:43.453580360096765.\n",
      "11.898688379085756 0.020012497989690372\n",
      "The training loss is 6.343812849310159 with std:13.210709059200655. The val loss is 7.706172103330743 with std:11.238416493891577.\n",
      "7.706172103330743 0.020012497989690372\n",
      "The training loss is 5.517389973083956 with std:8.341241066358744. The val loss is 10.28437983964057 with std:27.837107696094968.\n",
      "10.28437983964057 0.020012497989690372\n",
      "Evaluating for {'lmda': 0.020197857568198783} ...\n",
      "The training loss is 5.934252865880259 with std:12.858334111855815. The val loss is 259.8765739026788 with std:2397.7339617543357.\n",
      "259.8765739026788 0.020197857568198783\n",
      "The training loss is 6.141500539245349 with std:11.026104377136441. The val loss is 11.906399079235937 with std:43.51603370151187.\n",
      "11.906399079235937 0.020197857568198783\n",
      "The training loss is 6.344990993159578 with std:13.211017000176701. The val loss is 7.705822951942228 with std:11.232204733631844.\n",
      "7.705822951942228 0.020197857568198783\n",
      "The training loss is 5.5190026932542775 with std:8.343046053603777. The val loss is 10.277733866024109 with std:27.811860648714198.\n",
      "10.277733866024109 0.020197857568198783\n",
      "Evaluating for {'lmda': 0.020384933982524632} ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 5.936105456164073 with std:12.857522149288323. The val loss is 255.2082037147084 with std:2353.217404427829.\n",
      "255.2082037147084 0.020384933982524632\n",
      "The training loss is 6.1423795101353535 with std:11.026095267881976. The val loss is 11.914119559573807 with std:43.578553756280236.\n",
      "11.914119559573807 0.020384933982524632\n",
      "The training loss is 6.346168435951356 with std:13.211312083551114. The val loss is 7.705461035547828 with std:11.225958307991332.\n",
      "7.705461035547828 0.020384933982524632\n",
      "The training loss is 5.520620748925777 with std:8.34486065648831. The val loss is 10.271066869722368 with std:27.786560130241945.\n",
      "10.271066869722368 0.020384933982524632\n",
      "Evaluating for {'lmda': 0.020573743134329126} ...\n",
      "The training loss is 5.937956722468054 with std:12.856688911901358. The val loss is 250.6065847345835 with std:2309.337703555707.\n",
      "250.6065847345835 0.020573743134329126\n",
      "The training loss is 6.143257084957005 with std:11.026079542828517. The val loss is 11.921850181746768 with std:43.641143488347566.\n",
      "11.921850181746768 0.020573743134329126\n",
      "The training loss is 6.34734525317079 with std:13.211594387819328. The val loss is 7.705086363144296 with std:11.219676968170944.\n",
      "7.705086363144296 0.020573743134329126\n",
      "The training loss is 5.522244252387553 with std:8.34668503901866. The val loss is 10.264379323169152 with std:27.761208185196.\n",
      "10.264379323169152 0.020573743134329126\n",
      "Evaluating for {'lmda': 0.020764301072557747} ...\n",
      "The training loss is 5.93980671256127 with std:12.855834534988002. The val loss is 246.0711262327903 with std:2266.0892186377578.\n",
      "246.0711262327903 0.020764301072557747\n",
      "The training loss is 6.144133319245954 with std:11.026057258784256. The val loss is 11.929591307620973 with std:43.703805866961076.\n",
      "11.929591307620973 0.020764301072557747\n",
      "The training loss is 6.348521520688983 with std:13.211863992339454. The val loss is 7.704698944092869 with std:11.21336046897785.\n",
      "7.704698944092869 0.020764301072557747\n",
      "The training loss is 5.5238733159709925 with std:8.34851936563412. The val loss is 10.257671704113886 with std:27.735806867996676.\n",
      "10.257671704113886 0.020764301072557747\n",
      "Evaluating for {'lmda': 0.02095662399480433} ...\n",
      "The training loss is 5.941655474523349 with std:12.854959154979001. The val loss is 241.60123612669045 with std:2223.466296289102.\n",
      "241.60123612669045 0.02095662399480433\n",
      "The training loss is 6.145008268549604 with std:11.026028472878005. The val loss is 11.937343299266825 with std:43.76654386665507.\n",
      "11.937343299266825 0.02095662399480433\n",
      "The training loss is 6.3496973147567095 with std:13.212120977329217. The val loss is 7.704298788116178 with std:11.207008568882383.\n",
      "7.704298788116178 0.02095662399480433\n",
      "The training loss is 5.525508052031568 with std:8.350363801187628. The val loss is 10.250944495605626 with std:27.71035824258418.\n",
      "10.250944495605626 0.02095662399480433\n",
      "Evaluating for {'lmda': 0.021150728248687946} ...\n",
      "The training loss is 5.943503056740656 with std:12.854062909435022. The val loss is 237.19632113229093 with std:2181.4632716892897.\n",
      "237.19632113229093 0.021150728248687946\n",
      "The training loss is 6.145881988426768 with std:11.02599324256268. The val loss is 11.945106518933128 with std:43.82936046697618.\n",
      "11.945106518933128 0.021150728248687946\n",
      "The training loss is 6.350872712005279 with std:13.212365423885913. The val loss is 7.703885905302489 with std:11.20062103005823.\n",
      "7.703885905302489 0.021150728248687946\n",
      "The training loss is 5.527148572935624 with std:8.352218510927989. The val loss is 10.244198185924187 with std:27.684864381747733.\n",
      "10.244198185924187 0.021150728248687946\n",
      "Evaluating for {'lmda': 0.021346630333242442} ...\n",
      "The training loss is 5.945349507893891 with std:12.853145937039365. The val loss is 232.85578691445016 with std:2140.0744700167834.\n",
      "232.85578691445016 0.021346630333242442\n",
      "The training loss is 6.1467545344399905 with std:11.025951625612377. The val loss is 11.952881329004523 with std:43.892258652261326.\n",
      "11.952881329004523 0.021346630333242442\n",
      "The training loss is 6.352047789441401 with std:13.21259741397178. The val loss is 7.703460306100656 with std:11.19419761843066.\n",
      "7.703460306100656 0.021346630333242442\n",
      "The training loss is 5.528794991041457 with std:8.354083660478663. The val loss is 10.237433268571541 with std:27.659327366748474.\n",
      "10.237433268571541 0.021346630333242442\n",
      "Evaluating for {'lmda': 0.021544346900318832} ...\n",
      "The training loss is 5.947194876954344 with std:12.852208377596078. The val loss is 228.57903823768697 with std:2099.2942078888927.\n",
      "228.57903823768697 0.021544346900318832\n",
      "The training loss is 6.147625962153322 with std:11.025903680122648. The val loss is 11.960668092009648 with std:43.9552414116559.\n",
      "11.960668092009648 0.021544346900318832\n",
      "The training loss is 6.353222624447046 with std:13.212817030428551. The val loss is 7.703022001317051 with std:11.18773810369294.\n",
      "7.703022001317051 0.021544346900318832\n",
      "The training loss is 5.5304474186927415 with std:8.355959415829512. The val loss is 10.230650242189336 with std:27.633749286649717.\n",
      "10.230650242189336 0.021544346900318832\n",
      "Evaluating for {'lmda': 0.0217438947560008} ...\n",
      "The training loss is 5.949039213173321 with std:12.85125037201858. The val loss is 224.36547911090364 with std:2059.116794743984.\n",
      "224.36547911090364 0.0217438947560008\n",
      "The training loss is 6.148496327128841 with std:11.025849464514746. The val loss is 11.968467170575149 with std:44.018311738805615.\n",
      "11.968467170575149 0.0217438947560008\n",
      "The training loss is 6.354397294777515 with std:13.213024356983635. The val loss is 7.702571002126746 with std:11.181242259397752.\n",
      "7.702571002126746 0.0217438947560008\n",
      "The training loss is 5.5321059681953875 with std:8.357845943311403. The val loss is 10.223849610546413 with std:27.60813223786384.\n",
      "10.223849610546413 0.0217438947560008\n",
      "Evaluating for {'lmda': 0.021945290862033135} ...\n",
      "The training loss is 5.950882566075803 with std:12.850272062340643. The val loss is 220.21451293371885 with std:2019.5365342386046.\n",
      "220.21451293371885 0.021945290862033135\n",
      "The training loss is 6.149365684921238 with std:11.025789037525247. The val loss is 11.976278927414535 with std:44.08147263179738.\n",
      "11.976278927414535 0.021945290862033135\n",
      "The training loss is 6.355571878558126 with std:13.213219478247806. The val loss is 7.7021073200532735 with std:11.174709862931225.\n",
      "7.7021073200532735 0.021945290862033135\n",
      "The training loss is 5.533770751807754 with std:8.35974340958249. The val loss is 10.217031882482642 with std:27.58247832359049.\n",
      "10.217031882482642 0.021945290862033135\n",
      "Evaluating for {'lmda': 0.022148552337263594} ...\n",
      "The training loss is 5.952724985451615 with std:12.84927359168818. The val loss is 216.12554263657975 with std:1980.5477255858532.\n",
      "216.12554263657975 0.022148552337263594\n",
      "The training loss is 6.150234091073923 with std:11.025722458216993. The val loss is 11.984103725295723 with std:44.14472709289543.\n",
      "11.984103725295723 0.022148552337263594\n",
      "The training loss is 6.356746454282144 with std:13.213402479723824. The val loss is 7.701630966987548 with std:11.168140695622062.\n",
      "7.701630966987548 0.022148552337263594\n",
      "The training loss is 5.535441881721857 with std:8.361651981607244. The val loss is 10.210197571845837 with std:27.55678965321093.\n",
      "10.210197571845837 0.022148552337263594\n",
      "Evaluating for {'lmda': 0.022353696459097953} ...\n",
      "The training loss is 5.954566521346536 with std:12.848255104289061. The val loss is 212.09797082350653 with std:1942.1446649183904.\n",
      "212.09797082350653 0.022353696459097953\n",
      "The training loss is 6.151101601115574 with std:11.025649785974807. The val loss is 11.991941927026621 with std:44.20807812850545.\n",
      "11.991941927026621 0.022353696459097953\n",
      "The training loss is 6.357921100811407 with std:13.213573447809106. The val loss is 7.701141955175186 with std:11.161534542738398.\n",
      "7.701141955175186 0.022353696459097953\n",
      "The training loss is 5.537119470050166 with std:8.363571826642119. The val loss is 10.203347197462493 with std:27.531068341785492.\n",
      "10.203347197462493 0.022353696459097953\n",
      "Evaluating for {'lmda': 0.022560740664968604} ...\n",
      "The training loss is 5.956407224058427 with std:12.847216745466739. The val loss is 208.13119990566338 with std:1904.321646564189.\n",
      "208.13119990566338 0.022560740664968604\n",
      "The training loss is 6.151968270555616 with std:11.025571080506182. The val loss is 11.99979389541856 with std:44.27152874884484.\n",
      "11.99979389541856 0.022560740664968604\n",
      "The training loss is 6.359095897370091 with std:13.213732469804242. The val loss is 7.700640297226245 with std:11.154891193583548.\n",
      "7.700640297226245 0.022560740664968604\n",
      "The training loss is 5.538803628812463 with std:8.36550311221761. The val loss is 10.196481283069986 with std:27.505316509418446.\n",
      "10.196481283069986 0.022560740664968604\n",
      "Evaluating for {'lmda': 0.0227697025538168} ...\n",
      "The training loss is 5.958247144124338 with std:12.846158661631021. The val loss is 204.22463224050446 with std:1867.0729643754182.\n",
      "204.22463224050446 0.0227697025538168\n",
      "The training loss is 6.15283415488 with std:11.025486401837108. The val loss is 12.007659993281765 with std:44.33508196799405.\n",
      "12.007659993281765 0.0227697025538168\n",
      "The training loss is 6.360270923547873 with std:13.21387963391091. The val loss is 7.700126006108868 with std:11.148210441496062.\n",
      "7.700126006108868 0.0227697025538168\n",
      "The training loss is 5.540494469916707 with std:8.367446006116262. The val loss is 10.189600357272406 with std:27.47953628075731.\n",
      "10.189600357272406 0.0227697025538168\n",
      "Evaluating for {'lmda': 0.02298059988758851} ...\n",
      "The training loss is 5.960086332316015 with std:12.84508100027283. The val loss is 200.37767026306295 with std:1830.3929129823955.\n",
      "200.37767026306295 0.02298059988758851\n",
      "The training loss is 6.153699309546951 with std:11.025395810316317. The val loss is 12.01554058338295 with std:44.39874080354688.\n",
      "12.01554058338295 0.02298059988758851\n",
      "The training loss is 6.361446259294559 with std:13.214015029243802. The val loss is 7.699599095152907 with std:11.141492083917026.\n",
      "7.699599095152907 0.02298059988758851\n",
      "The training loss is 5.5421921051455145 with std:8.369400676361701. The val loss is 10.182704953489019 with std:27.453729784365215.\n",
      "10.182704953489019 0.02298059988758851\n",
      "Evaluating for {'lmda': 0.02319345059274429} ...\n",
      "The training loss is 5.96192483963016 with std:12.843983909962343. The val loss is 196.58971661635363 with std:1794.2757890391313.\n",
      "196.58971661635363 0.02319345059274429\n",
      "The training loss is 6.15456378998383 with std:11.02529936661899. The val loss is 12.023436028433558 with std:44.462508276554374.\n",
      "12.023436028433558 0.02319345059274429\n",
      "The training loss is 6.362621984919293 with std:13.214138745826745. The val loss is 7.699059578048091 with std:11.134735922434599.\n",
      "7.699059578048091 0.02319345059274429\n",
      "The training loss is 5.543896646141876 with std:8.371367291193286. The val loss is 10.175795609876584 with std:27.4278991521023.\n",
      "10.175795609876584 0.02319345059274429\n",
      "Evaluating for {'lmda': 0.023408272761782933} ...\n",
      "The training loss is 5.963762717281724 with std:12.84286754033597. The val loss is 192.86017428134738 with std:1758.7158924647365.\n",
      "192.86017428134738 0.023408272761782933\n",
      "The training loss is 6.155427651581118 with std:11.025197131736437. The val loss is 12.031346691056179 with std:44.52638741128928.\n",
      "12.031346691056179 0.023408272761782933\n",
      "The training loss is 6.363798181091427 with std:13.21425087461463. The val loss is 7.698507468853804 with std:11.127941762852812.\n",
      "7.698507468853804 0.023408272761782933\n",
      "The training loss is 5.545608204394078 with std:8.373346019054225. The val loss is 10.168872869299902 with std:27.40204651860089.\n",
      "10.168872869299902 0.023408272761782933\n",
      "Evaluating for {'lmda': 0.023625084654779464} ...\n",
      "The training loss is 5.965600016695342 with std:12.841732042101839. The val loss is 189.18844670157654 with std:1723.7075276337243.\n",
      "189.18844670157654 0.023625084654779464\n",
      "The training loss is 6.156290949689817 with std:11.025089166984689. The val loss is 12.039272933763835 with std:44.590381235108346.\n",
      "12.039272933763835 0.023625084654779464\n",
      "The training loss is 6.364974928833828 with std:13.214351507475548. The val loss is 7.697942781988059 with std:11.12110941520691.\n",
      "7.697942781988059 0.023625084654779464\n",
      "The training loss is 5.547326891218322 with std:8.375337028569799. The val loss is 10.161937279246644 with std:27.376174020601503.\n",
      "10.161937279246644 0.023625084654779464\n",
      "Evaluating for {'lmda': 0.023843904700937203} ...\n",
      "The training loss is 5.9674367894981675 with std:12.840577567020093. The val loss is 185.5739379077125 with std:1689.245004565965.\n",
      "185.5739379077125 0.023843904700937203\n",
      "The training loss is 6.157153739616566 with std:11.024975534002238. The val loss is 12.04721511894216 with std:44.654492778329995.\n",
      "12.04721511894216 0.023843904700937203\n",
      "The training loss is 6.366152309527473 with std:13.21444073721118. The val loss is 7.6973655322384085 with std:11.114238693835365.\n",
      "7.6973655322384085 0.023843904700937203\n",
      "The training loss is 5.549052817746718 with std:8.377340488531736. The val loss is 10.154989391783035 with std:27.3502837963949.\n",
      "10.154989391783035 0.023843904700937203\n",
      "Evaluating for {'lmda': 0.024064751500154243} ...\n",
      "The training loss is 5.969273087512651 with std:12.839404267915565. The val loss is 182.01605263794013 with std:1655.3226400766075.\n",
      "182.01605263794013 0.024064751500154243\n",
      "The training loss is 6.158016076619636 with std:11.024856294744309. The val loss is 12.05517360881968 with std:44.718725074038005.\n",
      "12.05517360881968 0.024064751500154243\n",
      "The training loss is 6.3673304049043296 with std:13.21451865756116. The val loss is 7.696775734759754 with std:11.107329417433977.\n",
      "7.696775734759754 0.024064751500154243\n",
      "The training loss is 5.550786094907458 with std:8.379356567878226. The val loss is 10.148029763481162 with std:27.32437798518251.\n",
      "10.148029763481162 0.024064751500154243\n",
      "Evaluating for {'lmda': 0.024287643824604518} ...\n",
      "The training loss is 5.971108962746348 with std:12.838212298651646. The val loss is 178.51419645932006 with std:1621.934758935523.\n",
      "178.51419645932006 0.024287643824604518\n",
      "The training loss is 6.158878015905285 with std:11.024731511495474. The val loss is 12.063148765437381 with std:44.783081157856856.\n",
      "12.063148765437381 0.024287643824604518\n",
      "The training loss is 6.368509297051706 with std:13.214585363204707. The val loss is 7.6961734050785315 with std:11.100381409094602.\n",
      "7.6961734050785315 0.024287643824604518\n",
      "The training loss is 5.552526833415037 with std:8.38138543567651. The val loss is 10.141058955354477 with std:27.298458726469473.\n",
      "10.141058955354477 0.024287643824604518\n",
      "Evaluating for {'lmda': 0.024512600620333384} ...\n",
      "The training loss is 5.9729444673877055 with std:12.837001814140208. The val loss is 175.06777588231535 with std:1589.0756949614242.\n",
      "175.06777588231535 0.024512600620333384\n",
      "The training loss is 6.15973961262163 with std:11.024601246855296. The val loss is 12.071140950624615 with std:44.84756406778145.\n",
      "12.071140950624615 0.024512600620333384\n",
      "The training loss is 6.369689068405375 with std:13.214640949765752. The val loss is 7.695558559098341 with std:11.093394496387564.\n",
      "7.695558559098341 0.024512600620333384\n",
      "The training loss is 5.554275143751326 with std:8.383427261106704. The val loss is 10.134077532797662 with std:27.27252815945527.\n",
      "10.134077532797662 0.024512600620333384\n",
      "Evaluating for {'lmda': 0.024739641008868128} ...\n",
      "The training loss is 5.9747796537945534 with std:12.835772970329623. The val loss is 171.67619847623 with std:1556.7397921248303.\n",
      "171.67619847623 0.024739641008868128\n",
      "The training loss is 6.160600921857367 with std:11.024465563746771. The val loss is 12.079150525997566 with std:44.91217684421607.\n",
      "12.079150525997566 0.024739641008868128\n",
      "The training loss is 6.37086980175285 with std:13.214685513819752. The val loss is 7.694931213091322 with std:11.086368511364922.\n",
      "7.694931213091322 0.024739641008868128\n",
      "The training loss is 5.556031136150668 with std:8.385482213439944. The val loss is 10.127086065511959 with std:27.246588422391856.\n",
      "10.127086065511959 0.024739641008868128\n",
      "Evaluating for {'lmda': 0.024968784288843266} ...\n",
      "The training loss is 5.976614574491016 with std:12.834525924196546. The val loss is 168.3388729814618 with std:1524.9214056205772.\n",
      "168.3388729814618 0.024968784288843266\n",
      "The training loss is 6.161461998634969 with std:11.024324525418757. The val loss is 12.087177852888908 with std:44.97692252941574.\n",
      "12.087177852888908 0.024968784288843266\n",
      "The training loss is 6.372051580228685 with std:13.21471915290012. The val loss is 7.694291383715443 with std:11.07930329067215.\n",
      "7.694291383715443 0.024968784288843266\n",
      "The training loss is 5.557794920585353 with std:8.387550462023409. The val loss is 10.120085127439667 with std:27.220641651982344.\n",
      "10.120085127439667 0.024968784288843266\n",
      "Evaluating for {'lmda': 0.025200049937640922} ...\n",
      "The training loss is 5.978449282155554 with std:12.833260833745143. The val loss is 165.05520941751251 with std:1493.614902899971.\n",
      "165.05520941751251 0.025200049937640922\n",
      "The training loss is 6.162322897907592 with std:11.024178195434475. The val loss is 12.095223292368818 with std:45.041804167700825.\n",
      "12.095223292368818 0.025200049937640922\n",
      "The training loss is 6.373234487317296 with std:13.21474196550431. The val loss is 7.693639088010334 with std:11.072198675571183.\n",
      "7.693639088010334 0.025200049937640922\n",
      "The training loss is 5.5595666067529645 with std:8.389632176263152. The val loss is 10.11307529668269 with std:27.19468998271117.\n",
      "10.11307529668269 0.025200049937640922\n",
      "Evaluating for {'lmda': 0.025433457613046495} ...\n",
      "The training loss is 5.980283829615049 with std:12.831977857996032. The val loss is 161.82461919421993 with std:1462.8146647336189.\n",
      "161.82461919421993 0.025433457613046495\n",
      "The training loss is 6.1631836745549755 with std:11.024026637683425. The val loss is 12.103287205189346 with std:45.106824805039615.\n",
      "12.103287205189346 0.025433457613046495\n",
      "The training loss is 6.3744186068476285 with std:13.214754051094864. The val loss is 7.692974343395927 with std:11.06505451199589.\n",
      "7.692974343395927 0.025433457613046495\n",
      "The training loss is 5.561346304054456 with std:8.39172752559941. The val loss is 10.10605715544894 with std:27.16873554626782.\n",
      "10.10605715544894 0.025433457613046495\n",
      "Evaluating for {'lmda': 0.025669027154919505} ...\n",
      "The training loss is 5.982118269842389 with std:12.830677156991065. The val loss is 158.6465152119975 with std:1432.5150861695065.\n",
      "158.6465152119975 0.025669027154919505\n",
      "The training loss is 6.164044383378883 with std:11.023869916375364. The val loss is 12.111369951761002 with std:45.17198748886553.\n",
      "12.111369951761002 0.025669027154919505\n",
      "The training loss is 6.375604022995711 with std:13.214755510110122. The val loss is 7.692297167688563 with std:11.057870650627377.\n",
      "7.692297167688563 0.025669027154919505\n",
      "The training loss is 5.563134121586844 with std:8.39383667949911. The val loss is 10.099031289954292 with std:27.14278047084527.\n",
      "10.099031289954292 0.025669027154919505\n",
      "Evaluating for {'lmda': 0.025906778586880074} ...\n",
      "The training loss is 5.98395265593778 with std:12.82935889176621. The val loss is 155.52031196957728 with std:1402.7105775626637.\n",
      "155.52031196957728 0.025906778586880074\n",
      "The training loss is 6.164905079098887 with std:11.02370809604032. The val loss is 12.119471892146741 with std:45.237295268095465.\n",
      "12.119471892146741 0.025906778586880074\n",
      "The training loss is 6.376790820281183 with std:13.214746443970231. The val loss is 7.691607579094379 with std:11.050646946946594.\n",
      "7.691607579094379 0.025906778586880074\n",
      "The training loss is 5.564930168122855 with std:8.395959807428648. The val loss is 10.091998290366323 with std:27.116826880563416.\n",
      "10.091998290366323 0.025906778586880074\n",
      "Evaluating for {'lmda': 0.02614673211801092} ...\n",
      "The training loss is 5.985787041133091 with std:12.828023224372311. The val loss is 152.44542566021303 with std:1373.395565494684.\n",
      "152.44542566021303 0.02614673211801092\n",
      "The training loss is 6.165765816348019 with std:11.023541241532147. The val loss is 12.127593386008646 with std:45.30275119268885.\n",
      "12.127593386008646 0.02614673211801092\n",
      "The training loss is 6.3779790835690315 with std:13.214726955080577. The val loss is 7.690905596219688 with std:11.043383261284514.\n",
      "7.690905596219688 0.02614673211801092\n",
      "The training loss is 5.566734552096405 with std:8.39809707883929. The val loss is 10.084958750713442 with std:27.09087689479067.\n",
      "10.084958750713442 0.02614673211801092\n",
      "Evaluating for {'lmda': 0.026388908144575104} ...\n",
      "The training loss is 5.987621478777817 with std:12.82667031784567. The val loss is 149.42127427183698 with std:1344.5644937312616.\n",
      "149.42127427183698 0.026388908144575104\n",
      "The training loss is 6.166626649669191 with std:11.023369418020609. The val loss is 12.135734792606984 with std:45.3683583137095.\n",
      "12.135734792606984 0.026388908144575104\n",
      "The training loss is 6.379168898065253 with std:13.214697146836441. The val loss is 7.690191238070064 with std:11.036079458889576.\n",
      "7.690191238070064 0.026388908144575104\n",
      "The training loss is 5.568547381589964 with std:8.400248663151745. The val loss is 10.07791326881542 with std:27.0649326275254.\n",
      "10.07791326881542 0.026388908144575104\n",
      "Evaluating for {'lmda': 0.02663332725174982} ...\n",
      "The training loss is 5.989456022334519 with std:12.825300336215093. The val loss is 146.4472776832175 with std:1316.2118241412727.\n",
      "146.4472776832175 0.02663332725174982\n",
      "The training loss is 6.167487633509942 with std:11.023192691002569. The val loss is 12.143896470750494 with std:45.434119682924994.\n",
      "12.143896470750494 0.02663332725174982\n",
      "The training loss is 6.380360349321153 with std:13.214657123636123. The val loss is 7.689464524065161 with std:11.02873541000621.\n",
      "7.689464524065161 0.02663332725174982\n",
      "The training loss is 5.570368764318966 with std:8.40241472973261. The val loss is 10.070862446185155 with std:27.038996186696522.\n",
      "10.070862446185155 0.02663332725174982\n",
      "Evaluating for {'lmda': 0.02688001021537606} ...\n",
      "The training loss is 5.991290725369975 with std:12.823913444491724. The val loss is 143.52285775678527 with std:1288.332037584432.\n",
      "143.52285775678527 0.02688001021537606\n",
      "The training loss is 6.168348822218941 with std:11.0230111262929. The val loss is 12.152078778781672 with std:45.50003835273993.\n",
      "12.152078778781672 0.02688001021537606\n",
      "The training loss is 6.381553523226535 with std:13.214606990878785. The val loss is 7.688725474028823 with std:11.02135098989767.\n",
      "7.688725474028823 0.02688001021537606\n",
      "The training loss is 5.57219880761369 with std:8.404595447881947. The val loss is 10.063806887971273 with std:27.013069673624248.\n",
      "10.063806887971273 0.02688001021537606\n",
      "Evaluating for {'lmda': 0.027128978003724658} ...\n",
      "The training loss is 5.9931256415502245 with std:12.822509808668526. The val loss is 140.64743843067788 with std:1260.9196347912646.\n",
      "140.64743843067788 0.027128978003724658\n",
      "The training loss is 6.169210270041626 with std:11.022824790028803. The val loss is 12.160282074553098 with std:45.566117376010276.\n",
      "12.160282074553098 0.027128978003724658\n",
      "The training loss is 6.3827485060148215 with std:13.214546854971953. The val loss is 7.687974108205591 with std:11.013926078929325.\n",
      "7.687974108205591 0.027128978003724658\n",
      "The training loss is 5.574037618406963 with std:8.406790986811254. The val loss is 10.056747202855112 with std:26.987155182291964.\n",
      "10.056747202855112 0.027128978003724658\n",
      "Evaluating for {'lmda': 0.027380251779278577} ...\n",
      "The training loss is 5.994960824633073 with std:12.821089595701158. The val loss is 137.82044580534537 with std:1233.9691371915485.\n",
      "137.82044580534537 0.027380251779278577\n",
      "The training loss is 6.170072031115606 with std:11.022633748666648. The val loss is 12.16850671538238 with std:45.6323598057244.\n",
      "12.16850671538238 0.027380251779278577\n",
      "The training loss is 6.383945384258756 with std:13.21447682334875. The val loss is 7.68721044726966 with std:11.006460562673425.\n",
      "7.68721044726966 0.027380251779278577\n",
      "The training loss is 5.575885303221023 with std:8.409001515630449. The val loss is 10.049684002967233 with std:26.961254798722013.\n",
      "10.049684002967233 0.027380251779278577\n",
      "Evaluating for {'lmda': 0.027633852900531698} ...\n",
      "The training loss is 5.996796328459781 with std:12.819652973513652. The val loss is 135.0413082346118 with std:1207.4750877850079.\n",
      "135.0413082346118 0.027633852900531698\n",
      "The training loss is 6.1709341594667775 with std:11.022438068984496. The val loss is 12.176753058046849 with std:45.69876869498524.\n",
      "12.176753058046849 0.027633852900531698\n",
      "The training loss is 6.3851442448714755 with std:13.21439700445803. The val loss is 7.686434512313468 with std:10.998954331885344.\n",
      "7.686434512313468 0.027633852900531698\n",
      "The training loss is 5.577741968149459 with std:8.41122720332581. The val loss is 10.042617903809496 with std:26.935370600336103.\n",
      "10.042617903809496 0.027633852900531698\n",
      "Evaluating for {'lmda': 0.02788980292380441} ...\n",
      "The training loss is 5.998632206949856 with std:12.818200110986545. The val loss is 132.30945640620507 with std:1181.4320519118392.\n",
      "132.30945640620507 0.02788980292380441\n",
      "The training loss is 6.171796709005131 with std:11.02223781808289. The val loss is 12.18502145873963 with std:45.76534709667609.\n",
      "12.18502145873963 0.02788980292380441\n",
      "The training loss is 6.386345175105503 with std:13.214307507780191. The val loss is 7.685646324872413 with std:10.991407282648018.\n",
      "7.685646324872413 0.02788980292380441\n",
      "The training loss is 5.579607718842562 with std:8.413468218741444. The val loss is 10.035549524157695 with std:26.909504655316216.\n",
      "10.035549524157695 0.02788980292380441\n",
      "Evaluating for {'lmda': 0.0281481236050758} ...\n",
      "The training loss is 6.000468514095042 with std:12.81673117795731. The val loss is 129.62432342545966 with std:1155.8346180533729.\n",
      "129.62432342545966 0.0281481236050758\n",
      "The training loss is 6.172659733519899 with std:11.02203306338028. The val loss is 12.193312273044551 with std:45.83209806329721.\n",
      "12.193312273044551 0.0281481236050758\n",
      "The training loss is 6.387548262552635 with std:13.214208443834051. The val loss is 7.68484590692138 with std:10.983819316403507.\n",
      "7.68484590692138 0.0281481236050758\n",
      "The training loss is 5.581482660495109 with std:8.4157247305686. The val loss is 10.028479485977572 with std:26.883659021983743.\n",
      "10.028479485977572 0.0281481236050758\n",
      "Evaluating for {'lmda': 0.028408836901833025} ...\n",
      "The training loss is 6.0023053039498375 with std:12.815246345202567. The val loss is 126.98534489598862 with std:1130.677398604056.\n",
      "126.98534489598862 0.028408836901833025\n",
      "The training loss is 6.17352328667577 with std:11.02182387261661. The val loss is 12.201625855915637 with std:45.89902464684006.\n",
      "12.201625855915637 0.028408836901833025\n",
      "The training loss is 6.3887535951439265 with std:13.214099924180607. The val loss is 7.684033280884415 with std:10.976190340026783.\n",
      "7.684033280884415 0.028408836901833025\n",
      "The training loss is 5.583366897830095 with std:8.41799690731981. The val loss is 10.02140841432762 with std:26.8578357481051.\n",
      "10.02140841432762 0.028408836901833025\n",
      "Evaluating for {'lmda': 0.028671964974937698} ...\n",
      "The training loss is 6.004142630628906 with std:12.813745784444627. The val loss is 124.39195899729503 with std:1105.9550306141552.\n",
      "124.39195899729503 0.028671964974937698\n",
      "The training loss is 6.174387422009244 with std:11.021610313853925. The val loss is 12.209962561631889 with std:45.966129898399664.\n",
      "12.209962561631889 0.028671964974937698\n",
      "The training loss is 6.38996126114852 with std:13.21398206143113. The val loss is 7.68320846964007 with std:10.968520265892812.\n",
      "7.68320846964007 0.028671964974937698\n",
      "The training loss is 5.585260535084204 with std:8.420284917318172. The val loss is 10.014336937270102 with std:26.83203687030754.\n",
      "10.014336937270102 0.028671964974937698\n",
      "Evaluating for {'lmda': 0.028937530190509504} ...\n",
      "The training loss is 6.005980548297484 with std:12.812229668331307. The val loss is 121.8436065591435 with std:1081.662176501763.\n",
      "121.8436065591435 0.028937530190509504\n",
      "The training loss is 6.175252192923173 with std:11.02139245547508. The val loss is 12.218322743780453 with std:46.03341686811617.\n",
      "12.218322743780453 0.028937530190509504\n",
      "The training loss is 6.3911713491737965 with std:13.213854969251152. The val loss is 7.682371496529034 with std:10.96080901194301.\n",
      "7.682371496529034 0.028937530190509504\n",
      "The training loss is 5.587163675994403 with std:8.422588928674404. The val loss is 10.00726568577527 with std:26.806264413402705.\n",
      "10.00726568577527 0.028937530190509504\n",
      "Evaluating for {'lmda': 0.029205555121827466} ...\n",
      "The training loss is 6.007819111166801 with std:12.810698170444704. The val loss is 119.33973113659879 with std:1057.7935247711232.\n",
      "119.33973113659879 0.029205555121827466\n",
      "The training loss is 6.176117652683524 with std:11.021170366179046. The val loss is 12.226706755232216 with std:46.100888604985144.\n",
      "12.226706755232216 0.029205555121827466\n",
      "The training loss is 6.392383948166395 with std:13.21371876237067. The val loss is 7.681522385362046 with std:10.953056501755068.\n",
      "7.681522385362046 0.029205555121827466\n",
      "The training loss is 5.589076423782362 with std:8.424909109274765. The val loss is 10.000195293625959 with std:26.78052038978289.\n",
      "10.000195293625959 0.029205555121827466\n",
      "Evaluating for {'lmda': 0.029476062551248572} ...\n",
      "The training loss is 6.009658373488409 with std:12.809151465285833. The val loss is 116.87977908113025 with std:1034.3437906935317.\n",
      "116.87977908113025 0.029476062551248572\n",
      "The training loss is 6.1769838544142885 with std:11.020944114990526. The val loss is 12.235114948092493 with std:46.16854815647603.\n",
      "12.235114948092493 0.029476062551248572\n",
      "The training loss is 6.393599147409692 with std:13.21357355659145. The val loss is 7.680661160428824 with std:10.945262664625725.\n",
      "7.680661160428824 0.029476062551248572\n",
      "The training loss is 5.590998881140708 with std:8.42724562676064. The val loss is 9.993126397325357 with std:26.754806798797933.\n",
      "9.993126397325357 0.029476062551248572\n",
      "Evaluating for {'lmda': 0.029749075472144407} ...\n",
      "The training loss is 6.011498389547322 with std:12.807589728267438. The val loss is 114.46319960898501 with std:1011.3077169623388.\n",
      "114.46319960898501 0.029749075472144407\n",
      "The training loss is 6.177850851094087 with std:11.020713771251252. The val loss is 12.24354767368622 with std:46.236398568450205.\n",
      "12.24354767368622 0.029749075472144407\n",
      "The training loss is 6.394817036525377 with std:13.213419468791077. The val loss is 7.679787846503684 with std:10.937427435626034.\n",
      "7.679787846503684 0.029749075472144407\n",
      "The training loss is 5.592931150220187 with std:8.429598648513196. The val loss is 9.986059635986608 with std:26.729125626084418.\n",
      "9.986059635986608 0.029749075472144407\n",
      "Evaluating for {'lmda': 0.030024617090855493} ...\n",
      "The training loss is 6.013339213655569 with std:12.806013135712437. The val loss is 112.08944486904737 with std:988.6800743430804.\n",
      "112.08944486904737 0.030024617090855493\n",
      "The training loss is 6.178718695551591 with std:11.0204794046252. The val loss is 12.252005282526264 with std:46.30444288492046.\n",
      "12.252005282526264 0.030024617090855493\n",
      "The training loss is 6.396037705473898 with std:13.213256616929506. The val loss is 7.678902468853253 with std:10.929550755676011.\n",
      "7.678902468853253 0.030024617090855493\n",
      "The training loss is 5.594873332612048 with std:8.431968341634201. The val loss is 9.978995651242437 with std:26.703478842985753.\n",
      "9.978995651242437 0.030024617090855493\n",
      "Evaluating for {'lmda': 0.030302710828663964} ...\n",
      "The training loss is 6.015180900147539 with std:12.80442186484499. The val loss is 109.75797000814363 with std:966.4556622992975.\n",
      "109.75797000814363 0.030302710828663964\n",
      "The training loss is 6.179587440461091 with std:11.020241085093415. The val loss is 12.260488124278178 with std:46.37268414781177.\n",
      "12.260488124278178 0.030302710828663964\n",
      "The training loss is 6.397261244552093 with std:13.213085120060795. The val loss is 7.678005053251116 with std:10.92163257163328.\n",
      "7.678005053251116 0.030302710828663964\n",
      "The training loss is 5.59682552933829 with std:8.43435487293118. The val loss is 9.971935087150307 with std:26.67786840594373.\n",
      "9.971935087150307 0.030302710828663964\n",
      "Evaluating for {'lmda': 0.030583380323784333} ...\n",
      "The training loss is 6.01702350337496 with std:12.802816093785884. The val loss is 107.46823323260975 with std:944.6293095828371.\n",
      "107.46823323260975 0.030583380323784333\n",
      "The training loss is 6.180457138338633 with std:11.019998882960993. The val loss is 12.268996547725049 with std:46.44112539664504.\n",
      "12.268996547725049 0.030583380323784333\n",
      "The training loss is 6.398487744396364 with std:13.212905098335249. The val loss is 7.677095625977594 with std:10.913672836342226.\n",
      "7.677095625977594 0.030583380323784333\n",
      "The training loss is 5.598787840835047 with std:8.436758408900605. The val loss is 9.964878590074145 with std:26.65229625584557.\n",
      "9.964878590074145 0.030583380323784333\n",
      "Evaluating for {'lmda': 0.03086664943337273} ...\n",
      "The training loss is 6.018867077699066 with std:12.801196001546195. The val loss is 105.21969587056935 with std:923.1958748311174.\n",
      "105.21969587056935 0.03086664943337273\n",
      "The training loss is 6.18132784153765 with std:11.019752868850997. The val loss is 12.277530900754108 with std:46.50976966852314.\n",
      "12.277530900754108 0.03086664943337273\n",
      "The training loss is 6.399717295979437 with std:13.212716673005733. The val loss is 7.676174213832365 with std:10.905671508715672.\n",
      "7.676174213832365 0.03086664943337273\n",
      "The training loss is 5.600760366940824 with std:8.43917911571215. The val loss is 9.95782680858623 with std:26.626764317424737.\n",
      "9.95782680858623 0.03086664943337273\n",
      "Evaluating for {'lmda': 0.03115254223555488} ...\n",
      "The training loss is 6.0207116774875065 with std:12.799561768017131. The val loss is 103.01182243009089 with std:902.1502471250653.\n",
      "103.01182243009089 0.03115254223555488\n",
      "The training loss is 6.182199602245079 with std:11.019503113705545. The val loss is 12.28609153030089 with std:46.57861999764818.\n",
      "12.28609153030089 0.03115254223555488\n",
      "The training loss is 6.400949990614042 with std:13.212519966445273. The val loss is 7.675240844147321 with std:10.897628553826253.\n",
      "7.675240844147321 0.03115254223555488\n",
      "The training loss is 5.602743206880625 with std:8.441617159188967. The val loss is 9.95078039337792 with std:26.60127449872638.\n",
      "9.95078039337792 0.03115254223555488\n",
      "Evaluating for {'lmda': 0.031441083031472646} ...\n",
      "The training loss is 6.022557357109043 with std:12.79791357396826. The val loss is 100.84408065634427 with std:881.4873465376431.\n",
      "100.84408065634427 0.031441083031472646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 6.18307247247625 with std:11.019249688787497. The val loss is 12.294678782331097 with std:46.64767941521602.\n",
      "12.294678782331097 0.031441083031472646\n",
      "The training loss is 6.402185919949978 with std:13.212315102139247. The val loss is 7.674295544794597 with std:10.88954394297072.\n",
      "7.674295544794597 0.031441083031472646\n",
      "The training loss is 5.6047364592549265 with std:8.444072704795659. The val loss is 9.943739997129589 with std:26.575828690404197.\n",
      "9.943739997129589 0.031441083031472646\n",
      "Evaluating for {'lmda': 0.03173229634734976} ...\n",
      "The training loss is 6.0244041709258 with std:12.79625160104112. The val loss is 98.71594158618312 with std:861.2021246580596.\n",
      "98.71594158618312 0.03173229634734976\n",
      "The training loss is 6.183946504072154 with std:11.018992665680996. The val loss is 12.303293001809418 with std:46.7169509491802.\n",
      "12.303293001809418 0.03173229634734976\n",
      "The training loss is 6.403425175975838 with std:13.212102204702633. The val loss is 7.673338344188826 with std:10.881417653728656.\n",
      "7.673338344188826 0.03173229634734976\n",
      "The training loss is 5.6067402220236096 with std:8.446545917618849. The val loss is 9.936706274423887 with std:26.550428765234926.\n",
      "9.936706274423887 0.03173229634734976\n",
      "Evaluating for {'lmda': 0.03202620693657652} ...\n",
      "The training loss is 6.026252173292928 with std:12.794576031743748. The val loss is 96.62687960105114 with std:841.289565099929.\n",
      "96.62687960105114 0.03202620693657652\n",
      "The training loss is 6.184821748694225 with std:11.018732116287156. The val loss is 12.311934532665465 with std:46.78643762400938.\n",
      "12.311934532665465 0.03202620693657652\n",
      "The training loss is 6.404667851019553 with std:13.21188139988811. The val loss is 7.672369271310378 with std:10.873249670075294.\n",
      "7.672369271310378 0.03202620693657652\n",
      "The training loss is 5.6087545924962985 with std:8.449036962352451. The val loss is 9.929679881625216 with std:26.525076577480373.\n",
      "9.929679881625216 0.03202620693657652\n",
      "Evaluating for {'lmda': 0.03232283978181379} ...\n",
      "The training loss is 6.028101418548846 with std:12.792887049438471. The val loss is 94.57637247842015 with std:821.7446839957786.\n",
      "94.57637247842015 0.03232283978181379\n",
      "The training loss is 6.185698257820764 with std:11.018468112827676. The val loss is 12.320603717753137 with std:46.85614246034138.\n",
      "12.320603717753137 0.03232283978181379\n",
      "The training loss is 6.405914037747525 with std:13.211652814582942. The val loss is 7.6713883557002625 with std:10.865039982415118.\n",
      "7.6713883557002625 0.03232283978181379\n",
      "The training loss is 5.610779667316609 with std:8.451546003283052. The val loss is 9.922661476774275 with std:26.49977396233126.\n",
      "9.922661476774275 0.03232283978181379\n",
      "Evaluating for {'lmda': 0.0326222200971167} ...\n",
      "The training loss is 6.029951961014175 with std:12.791184838343916. The val loss is 92.56390144002248 with std:802.5625304609767.\n",
      "92.56390144002248 0.0326222200971167\n",
      "The training loss is 6.186576082741528 with std:11.01820072784439. The val loss is 12.32930089882369 with std:46.92606847483098.\n",
      "12.32930089882369 0.0326222200971167\n",
      "The training loss is 6.407163829165433 with std:13.211416576829736. The val loss is 7.670395627486076 with std:10.85678858770657.\n",
      "7.670395627486076 0.0326222200971167\n",
      "The training loss is 5.612815542448577 with std:8.454073204271175. The val loss is 9.915651719480088 with std:26.474522735338777.\n",
      "9.915651719480088 0.0326222200971167\n",
      "Evaluating for {'lmda': 0.03292437333007769} ...\n",
      "The training loss is 6.031803854986132 with std:12.789469583523946. The val loss is 90.5889511994409 with std:783.7381870516872.\n",
      "90.5889511994409 0.03292437333007769\n",
      "The training loss is 6.187455274555817 with std:11.01793003419859. The val loss is 12.338026416493545 with std:46.99621867986259.\n",
      "12.338026416493545 0.03292437333007769\n",
      "The training loss is 6.408417318619673 with std:13.211172815826558. The val loss is 7.669391117383015 with std:10.84849548950788.\n",
      "7.669391117383015 0.03292437333007769\n",
      "The training loss is 5.614862313167832 with std:8.456618728739793. The val loss is 9.90865127080824 with std:26.449324691877205.\n",
      "9.90865127080824 0.03292437333007769\n",
      "Evaluating for {'lmda': 0.033229325163989715} ...\n",
      "The training loss is 6.033657154733067 with std:12.787741470883525. The val loss is 88.65101000713354 with std:765.2667701986336.\n",
      "88.65101000713354 0.033229325163989715\n",
      "The training loss is 6.188335884166193 with std:11.017656105069703. The val loss is 12.346780610200712 with std:47.0665960832438.\n",
      "12.346780610200712 0.033229325163989715\n",
      "The training loss is 6.409674599795135 with std:13.21092166193612. The val loss is 7.668374856705727 with std:10.840160698069749.\n",
      "7.668374856705727 0.033229325163989715\n",
      "The training loss is 5.616920074044538 with std:8.459182739655178. The val loss is 9.901660793166903 with std:26.42418160658309.\n",
      "9.901660793166903 0.033229325163989715\n",
      "Evaluating for {'lmda': 0.033537101520029304} ...\n",
      "The training loss is 6.035511914493345 with std:12.786000687164256. The val loss is 86.74956969428514 with std:747.1434306297226.\n",
      "86.74956969428514 0.033537101520029304\n",
      "The training loss is 6.189217962276153 with std:11.017379013956887. The val loss is 12.355563818186655 with std:47.13720368805176.\n",
      "12.355563818186655 0.033537101520029304\n",
      "The training loss is 6.410935766718788 with std:13.2106632466911. The val loss is 7.667346877381723 with std:10.831784230399363.\n",
      "7.667346877381723 0.033537101520029304\n",
      "The training loss is 5.618988918934085 with std:8.461765399515055. The val loss is 9.894680950194525 with std:26.399095232806904.\n",
      "9.894680950194525 0.033537101520029304\n",
      "Evaluating for {'lmda': 0.03384772855945981} ...\n",
      "The training loss is 6.037368188467247 with std:12.784247419932598. The val loss is 84.88412571402243 with std:729.3633537677285.\n",
      "84.88412571402243 0.03384772855945981\n",
      "The training loss is 6.190101559384402 with std:11.017098834679247. The val loss is 12.364376377443955 with std:47.2080444922463.\n",
      "12.364376377443955 0.03384772855945981\n",
      "The training loss is 6.412200913756983 with std:13.210397702804544. The val loss is 7.66630721196126 with std:10.823366110362944.\n",
      "7.66630721196126 0.03384772855945981\n",
      "The training loss is 5.621068940962591 with std:8.464366870330279. The val loss is 9.887712406648026 with std:26.3740673021135.\n",
      "9.887712406648026 0.03384772855945981\n",
      "Evaluating for {'lmda': 0.03416123268585528} ...\n",
      "The training loss is 6.039226030816287 with std:12.78248185758403. The val loss is 83.0541771810731 with std:711.9217601134427.\n",
      "83.0541771810731 0.03416123268585528\n",
      "The training loss is 6.190986725782208 with std:11.016815641373649. The val loss is 12.37321862369104 with std:47.279121488457214.\n",
      "12.37321862369104 0.03416123268585528\n",
      "The training loss is 6.41347013561904 with std:13.210125164181049. The val loss is 7.665255893633371 with std:10.81490636876075.\n",
      "7.665255893633371 0.03416123268585528\n",
      "The training loss is 5.623160232516186 with std:8.466987313611854. The val loss is 9.880755828286546 with std:26.34909952376079.\n",
      "9.880755828286546 0.03416123268585528\n",
      "Evaluating for {'lmda': 0.03447764054734464} ...\n",
      "The training loss is 6.0410854956584465 with std:12.780704189328278. The val loss is 81.25922690981204 with std:694.8139056134635.\n",
      "81.25922690981204 0.03447764054734464\n",
      "The training loss is 6.1918735115484695 with std:11.016529508497264. The val loss is 12.38209089134124 with std:47.350437663758434.\n",
      "12.38209089134124 0.03447764054734464\n",
      "The training loss is 6.414743527354482 with std:13.209845765914555. The val loss is 7.664192956230435 with std:10.80640504339202.\n",
      "7.664192956230435 0.03447764054734464\n",
      "The training loss is 5.6252628852298505 with std:8.469626890358207. The val loss is 9.873811881751703 with std:26.324193584170644.\n",
      "9.873811881751703 0.03447764054734464\n",
      "Evaluating for {'lmda': 0.034796979038876914} ...\n",
      "The training loss is 6.042946637064488 with std:12.778914605187637. The val loss is 79.49878145150184 with std:678.0350820206373.\n",
      "79.49878145150184 0.034796979038876914\n",
      "The training loss is 6.192761966545876 with std:11.016240510823932. The val loss is 12.390993513445705 with std:47.421995999197385.\n",
      "12.390993513445705 0.034796979038876914\n",
      "The training loss is 6.4160211843561745 with std:13.209559644304793. The val loss is 7.663118434245278 with std:10.797862179149973.\n",
      "7.663118434245278 0.034796979038876914\n",
      "The training loss is 5.627376989973403 with std:8.472285761038147. The val loss is 9.866881234458933 with std:26.299351146455386.\n",
      "9.866881234458933 0.034796979038876914\n",
      "Evaluating for {'lmda': 0.03511927530450729} ...\n",
      "The training loss is 6.044809509054623 with std:12.777113295992487. The val loss is 77.7723511265581 with std:661.5806172072803.\n",
      "77.7723511265581 0.03511927530450729\n",
      "The training loss is 6.193652140417076 with std:11.015948723448595. The val loss is 12.399926821684945 with std:47.49379946977595.\n",
      "12.399926821684945 0.03511927530450729\n",
      "The training loss is 6.417303202359202 with std:13.209266936862205. The val loss is 7.662032362839764 with std:10.789277828096225.\n",
      "7.662032362839764 0.03511927530450729\n",
      "The training loss is 5.629502636840728 with std:8.474964085575893. The val loss is 9.859964554474622 with std:26.27457384993114.\n",
      "9.859964554474622 0.03511927530450729\n",
      "Evaluating for {'lmda': 0.035444556739704335} ...\n",
      "The training loss is 6.046674165595848 with std:12.775300453373683. The val loss is 76.07945006014053 with std:645.4458755103011.\n",
      "76.07945006014053 0.035444556739704335\n",
      "The training loss is 6.194544082580371 with std:11.015654221783134. The val loss is 12.408891146309667 with std:47.56585104393332.\n",
      "12.408891146309667 0.035444556739704335\n",
      "The training loss is 6.418589677441719 with std:13.20896778231564. The val loss is 7.6609347778657835 with std:10.780652049567312.\n",
      "7.6609347778657835 0.035444556739704335\n",
      "The training loss is 5.631639915139538 with std:8.47766202333842. The val loss is 9.8530625104037 with std:26.24986330965718.\n",
      "9.8530625104037 0.035444556739704335\n",
      "Evaluating for {'lmda': 0.03577285099367873} ...\n",
      "The training loss is 6.048540660597615 with std:12.773476269755571. The val loss is 74.41959621092282 with std:629.6262580115839.\n",
      "74.41959621092282 0.03577285099367873\n",
      "The training loss is 6.195437842225895 with std:11.015357081557008. The val loss is 12.417886816119196 with std:47.6381536834156.\n",
      "12.417886816119196 0.03577285099367873\n",
      "The training loss is 6.419880706026076 with std:13.208662320620535. The val loss is 7.659825715864433 with std:10.771984910211723.\n",
      "7.659825715864433 0.03577285099367873\n",
      "The training loss is 5.6337889133792 with std:8.480379733124433. The val loss is 9.846175771260665 with std:26.22522111594262.\n",
      "9.846175771260665 0.03577285099367873\n",
      "Evaluating for {'lmda': 0.03610418597173337} ...\n",
      "The training loss is 6.050409047911527 with std:12.771640938357823. The val loss is 72.79231140240786 with std:614.1172028428332.\n",
      "72.79231140240786 0.03610418597173337\n",
      "The training loss is 6.19633346831191 with std:11.015057378823027. The val loss is 12.42691415841264 with std:47.710710342857.\n",
      "12.42691415841264 0.03610418597173337\n",
      "The training loss is 6.421176384879059 with std:13.20835069296615. The val loss is 7.65870521409115 with std:10.763276484111834.\n",
      "7.65870521409115 0.03610418597173337\n",
      "The training loss is 5.635949719259382 with std:8.483117373146037. The val loss is 9.83930500636706 with std:26.200648833948247.\n",
      "9.83930500636706 0.03610418597173337\n",
      "Evaluating for {'lmda': 0.03643858983763545} ...\n",
      "The training loss is 6.05227938132563 with std:12.769794653182492. The val loss is 71.1971213500386 with std:598.9141854507885.\n",
      "71.1971213500386 0.03643858983763545\n",
      "The training loss is 6.197231009560274 with std:11.014755189946072. The val loss is 12.435973498954842 with std:47.783523969532965.\n",
      "12.435973498954842 0.03643858983763545\n",
      "The training loss is 6.4224768111111645 with std:13.208033041786043. The val loss is 7.657573310523383 with std:10.754526852856591.\n",
      "7.657573310523383 0.03643858983763545\n",
      "The training loss is 5.638122419660675 with std:8.485875101018276. The val loss is 9.832450885211223 with std:26.176148003212557.\n",
      "9.832450885211223 0.03643858983763545\n",
      "Evaluating for {'lmda': 0.03677609101601033} ...\n",
      "The training loss is 6.05415171456418 with std:12.767937609011494. The val loss is 69.6335556869335 with std:584.0127188494181.\n",
      "69.6335556869335 0.03677609101601033\n",
      "The training loss is 6.1981305144529975 with std:11.014450591611388. The val loss is 12.445065161945289 with std:47.85659750309587.\n",
      "12.445065161945289 0.03677609101601033\n",
      "The training loss is 6.4237820821794385 with std:13.207709510762422. The val loss is 7.656430043872702 with std:10.745736105613714.\n",
      "7.656430043872702 0.03677609101601033\n",
      "The training loss is 5.640307100631623 with std:8.488653073743318. The val loss is 9.82561407735199 with std:26.151720137277227.\n",
      "9.82561407735199 0.03677609101601033\n",
      "Evaluating for {'lmda': 0.03711671819475765} ...\n",
      "The training loss is 6.05602610128383 with std:12.766070001407279. The val loss is 68.10114798967503 with std:569.4083538730775.\n",
      "68.10114798967503 0.03711671819475765\n",
      "The training loss is 6.199032031227814 with std:11.0141436608226. The val loss is 12.454189469970366 with std:47.929933875210395.\n",
      "12.454189469970366 0.03711671819475765\n",
      "The training loss is 6.425092295885824 with std:13.207380244837777. The val loss is 7.655275453601098 with std:10.736904339222368.\n",
      "7.655275453601098 0.03711671819475765\n",
      "The training loss is 5.642503847380788 with std:8.49145144770154. The val loss is 9.818795252270201 with std:26.127366723209896.\n",
      "9.818795252270201 0.03711671819475765\n",
      "Evaluating for {'lmda': 0.03746050032748993} ...\n",
      "The training loss is 6.057902595072748 with std:12.764192026697426. The val loss is 66.59943580103187 with std:555.0966794007629.\n",
      "66.59943580103187 0.03746050032748993\n",
      "The training loss is 6.199935607875197 with std:11.0138344749021. The val loss is 12.463346743962951 with std:48.00353600919802.\n",
      "12.463346743962951 0.03746050032748993\n",
      "The training loss is 6.426407550379686 with std:13.207045390220962. The val loss is 7.654109579936631 with std:10.728031658282955.\n",
      "7.654109579936631 0.03746050032748993\n",
      "The training loss is 5.644712744265176 with std:8.494270378635889. The val loss is 9.811995079276628 with std:26.103089221314576.\n",
      "9.811995079276628 0.03746050032748993\n",
      "Evaluating for {'lmda': 0.03780746663599349} ...\n",
      "The training loss is 6.059781249447838 with std:12.762303881978209. The val loss is 65.1279606515552 with std:541.0733225699719.\n",
      "65.1279606515552 0.03780746663599349\n",
      "The training loss is 6.2008412921331875 with std:11.013523111488883. The val loss is 12.47253730317172 with std:48.07740681981113.\n",
      "12.47253730317172 0.03780746663599349\n",
      "The training loss is 6.427727944155975 with std:13.206705094392284. The val loss is 7.652932463877811 with std:10.719118175218918.\n",
      "7.652932463877811 0.03780746663599349\n",
      "The training loss is 5.646933874780318 with std:8.497110021639577. The val loss is 9.805214227365912 with std:26.078889064657986.\n",
      "9.805214227365912 0.03780746663599349\n",
      "Evaluating for {'lmda': 0.03815764661271248} ...\n",
      "The training loss is 6.061662117853299 with std:12.76040576510768. The val loss is 63.68626807984481 with std:527.3339489782553.\n",
      "63.68626807984481 0.03815764661271248\n",
      "The training loss is 6.2017491314847355 with std:11.013209648538327. The val loss is 12.481761465116335 with std:48.15154921286004.\n",
      "12.481761465116335 0.03815764661271248\n",
      "The training loss is 6.429053576057771 with std:13.206359506120975. The val loss is 7.651744147220519 with std:10.710164010390613.\n",
      "7.651744147220519 0.03815764661271248\n",
      "The training loss is 5.649167321551429 with std:8.499970531145996. The val loss is 9.798453365108157 with std:26.054767658760742.\n",
      "9.798453365108157 0.03815764661271248\n",
      "Evaluating for {'lmda': 0.03851107002325571} ...\n",
      "The training loss is 6.063545253658362 with std:12.758497874700828. The val loss is 62.27390765140382 with std:513.8742628717831.\n",
      "62.27390765140382 0.03851107002325571\n",
      "The training loss is 6.202659173153283 with std:11.01289416432492. The val loss is 12.49101954554912 with std:48.22596608490906.\n",
      "12.49101954554912 0.03851107002325571\n",
      "The training loss is 6.4303845452761745 with std:13.206008775463081. The val loss is 7.650544672561982 with std:10.701169292151896.\n",
      "7.650544672561982 0.03851107002325571\n",
      "The training loss is 5.651413166321243 with std:8.502852060911708. The val loss is 9.791713160527443 with std:26.03072638125903.\n",
      "9.791713160527443 0.03851107002325571\n",
      "Evaluating for {'lmda': 0.038867766908926654} ...\n",
      "The training loss is 6.065430710157308 with std:12.756580410124249. The val loss is 60.89043297571345 with std:500.69000731732854.\n",
      "60.89043297571345 0.038867766908926654\n",
      "The training loss is 6.203571464098836 with std:11.01257673744045. The val loss is 12.500311858410402 with std:48.3006603228965.\n",
      "12.500311858410402 0.038867766908926654\n",
      "The training loss is 6.431720951349913 with std:13.20565305377593. The val loss is 7.649334083321272 with std:10.692134156951633.\n",
      "7.649334083321272 0.038867766908926654\n",
      "The training loss is 5.653671489944731 with std:8.505754764010447. The val loss is 9.784994280968617 with std:26.006766581529483.\n",
      "9.784994280968617 0.038867766908926654\n",
      "Evaluating for {'lmda': 0.03922776758927719} ...\n",
      "The training loss is 6.0673185405667756 with std:12.754653571497155. The val loss is 59.535401722238106 with std:487.77696436452726.\n",
      "59.535401722238106 0.03922776758927719\n",
      "The training loss is 6.2044860510149045 with std:11.012257446791308. The val loss is 12.509638715796626 with std:48.375634803871364.\n",
      "12.509638715796626 0.03922776758927719\n",
      "The training loss is 6.433062894167439 with std:13.205292493723219. The val loss is 7.648112423749127 with std:10.683058749404815.\n",
      "7.648112423749127 0.03922776758927719\n",
      "The training loss is 5.655942372375542 with std:8.508678792818877. The val loss is 9.778297392988328 with std:25.982889580428537.\n",
      "9.778297392988328 0.03922776758927719\n",
      "Evaluating for {'lmda': 0.03959110266468459} ...\n",
      "The training loss is 6.069208798025535 with std:12.752717559680526. The val loss is 58.208375635185035 with std:475.1309551968434.\n",
      "58.208375635185035 0.03959110266468459\n",
      "The training loss is 6.20540298032346 with std:11.011936371604154. The val loss is 12.519000427901746 with std:48.45089239452903.\n",
      "12.519000427901746 0.03959110266468459\n",
      "The training loss is 6.43441047396515 with std:13.204927249286007. The val loss is 7.646879738946626 with std:10.673943222388882.\n",
      "7.646879738946626 0.03959110266468459\n",
      "The training loss is 5.658225892662028 with std:8.511624299006094. The val loss is 9.771623162212878 with std:25.959096669921305.\n",
      "9.771623162212878 0.03959110266468459\n",
      "Evaluating for {'lmda': 0.039957803018952694} ...\n",
      "The training loss is 6.071101535592966 with std:12.750772576275002. The val loss is 56.9089205457739 with std:462.747840259191.\n",
      "56.9089205457739 0.039957803018952694\n",
      "The training loss is 6.20632229817291 with std:11.011613591419216. The val loss is 12.52839730300286 with std:48.52643595106851.\n",
      "12.52839730300286 0.039957803018952694\n",
      "The training loss is 6.4357637913308325 with std:13.204557475768466. The val loss is 7.645636074878253 with std:10.664787737116681.\n",
      "7.645636074878253 0.039957803018952694\n",
      "The training loss is 5.660522128932625 with std:8.51459143352015. The val loss is 9.764972253242487 with std:25.935389112897123.\n",
      "9.764972253242487 0.039957803018952694\n",
      "Evaluating for {'lmda': 0.040327899821937074} ...\n",
      "The training loss is 6.0729968062492 with std:12.748818823623125. The val loss is 55.63660638522824 with std:450.62351939312657.\n",
      "55.63660638522824 0.040327899821937074\n",
      "The training loss is 6.207244050432238 with std:11.011289186095638. The val loss is 12.53782964738384 with std:48.602268318567255.\n",
      "12.53782964738384 0.040327899821937074\n",
      "The training loss is 6.437122947200052 with std:13.204183329807323. The val loss is 7.644381478386135 with std:10.655592463228867.\n",
      "7.644381478386135 0.040327899821937074\n",
      "The training loss is 5.662831158393414 with std:8.517580346580615. The val loss is 9.758345329502303 with std:25.91176814281715.\n",
      "9.758345329502303 0.040327899821937074\n",
      "Evaluating for {'lmda': 0.040701424532194365} ...\n",
      "The training loss is 6.074894662894842 with std:12.746856504799958. The val loss is 54.391007194204555 with std:438.7539319385784.\n",
      "54.391007194204555 0.040701424532194365\n",
      "The training loss is 6.2081682826901154 with std:11.010963235808132. The val loss is 12.547297765327496 with std:48.678392330910185.\n",
      "12.547297765327496 0.040701424532194365\n",
      "The training loss is 6.438488042861133 with std:13.203804969381538. The val loss is 7.6431159972042435 with std:10.646357578860465.\n",
      "7.6431159972042435 0.040701424532194365\n",
      "The training loss is 5.665153057315253 with std:8.520591187664559. The val loss is 9.751743053132522 with std:25.888234963499972.\n",
      "9.751743053132522 0.040701424532194365\n",
      "Evaluating for {'lmda': 0.04107840889965647} ...\n",
      "The training loss is 6.076795158348334 with std:12.744885823608607. The val loss is 53.17170113295725 with std:427.1350568433581.\n",
      "53.17170113295725 0.04107840889965647\n",
      "The training loss is 6.209095040248136 with std:11.010635821046732. The val loss is 12.556801959043593 with std:48.75481081019323.\n",
      "12.556801959043593 0.04107840889965647\n",
      "The training loss is 6.439859179951561 with std:13.20342255381703. The val loss is 7.641839679978028 with std:10.63708327074681.\n",
      "7.641839679978028 0.04107840889965647\n",
      "The training loss is 5.667487901029114 with std:8.523624105501172. The val loss is 9.7451660848602 with std:25.864790748896215.\n",
      "9.7451660848602 0.04107840889965647\n",
      "Evaluating for {'lmda': 0.04145888496832911} ...\n",
      "The training loss is 6.0786983453494905 with std:12.742906984583424. The val loss is 51.97827048846692 with std:415.7629127442747.\n",
      "51.97827048846692 0.04145888496832911\n",
      "The training loss is 6.210024368119676 with std:11.010307022618653. The val loss is 12.566342528648594 with std:48.83152656653589.\n",
      "12.566342528648594 0.04145888496832911\n",
      "The training loss is 6.441236460460397 with std:13.203036243799161. The val loss is 7.640552576274891 with std:10.627769734289076.\n",
      "7.640552576274891 0.04145888496832911\n",
      "The training loss is 5.66983576391581 with std:8.526679248057158. The val loss is 9.738615083883262 with std:25.841436642872228.\n",
      "9.738615083883262 0.04145888496832911\n",
      "Evaluating for {'lmda': 0.041842885079015825} ...\n",
      "The training loss is 6.0806042765556025 with std:12.7409201929812. The val loss is 50.81030168173729 with std:404.63355805067755.\n",
      "50.81030168173729 0.041842885079015825\n",
      "The training loss is 6.210956311024555 with std:11.00997692164509. The val loss is 12.575919772104466 with std:48.90854239758572.\n",
      "12.575919772104466 0.041842885079015825\n",
      "The training loss is 6.442619986727507 with std:13.202646201379336. The val loss is 7.639254736601209 with std:10.618417173640943.\n",
      "7.639254736601209 0.041842885079015825\n",
      "The training loss is 5.672196719399319 with std:8.529756762526587. The val loss is 9.732090707736663 with std:25.818173758998544.\n",
      "9.732090707736663 0.041842885079015825\n",
      "Evaluating for {'lmda': 0.042230441872066746} ...\n",
      "The training loss is 6.082513004545136 with std:12.738925654782454. The val loss is 49.667385273099896 with std:393.74309100974125.\n",
      "49.667385273099896 0.042230441872066746\n",
      "The training loss is 6.211890913386868 with std:11.00964559956415. The val loss is 12.585533985186146 with std:48.985861088218044.\n",
      "12.585533985186146 0.042230441872066746\n",
      "The training loss is 6.444009861444166 with std:13.202252589980235. The val loss is 7.6379462124177815 with std:10.609025801792678.\n",
      "7.6379462124177815 0.042230441872066746\n",
      "The training loss is 5.6745708399403165 with std:8.532856795325557. The val loss is 9.725593612179738 with std:25.795003180392175.\n",
      "9.725593612179738 0.042230441872066746\n",
      "Evaluating for {'lmda': 0.04262158829015323} ...\n",
      "The training loss is 6.084424581816173 with std:12.736923576684644. The val loss is 48.54911596656152 with std:383.0876497634713.\n",
      "48.54911596656152 0.04262158829015323\n",
      "The training loss is 6.212828219330303 with std:11.00931313812804. The val loss is 12.595185461428681 with std:49.06348541009928.\n",
      "12.595185461428681 0.04262158829015323\n",
      "The training loss is 6.445406187653927 with std:13.201855574411619. The val loss is 7.636627056155642 with std:10.599595840644717.\n",
      "7.636627056155642 0.04262158829015323\n",
      "The training loss is 5.676958197030087 with std:8.535979492080594. The val loss is 9.719124451062761 with std:25.771925959513883.\n",
      "9.719124451062761 0.04262158829015323\n",
      "Evaluating for {'lmda': 0.04301635758106795} ...\n",
      "The training loss is 6.086339060787622 with std:12.734914166104902. The val loss is 47.455092613179126 with std:372.6634123973015.\n",
      "47.455092613179126 0.04301635758106795\n",
      "The training loss is 6.2137682726748835 with std:11.008979619404966. The val loss is 12.604874492093353 with std:49.14141812139463.\n",
      "12.604874492093353 0.04301635758106795\n",
      "The training loss is 6.446809068750917 with std:13.201455320871169. The val loss is 7.63529732123256 with std:10.59012752110718.\n",
      "7.63529732123256 0.04301635758106795\n",
      "The training loss is 5.679358861179048 with std:8.539124997616767. The val loss is 9.712683876215609 with std:25.748943118092257.\n",
      "9.712683876215609 0.04301635758106795\n",
      "Evaluating for {'lmda': 0.04341478330055092} ...\n",
      "The training loss is 6.088256493799621 with std:12.732897631174941. The val loss is 46.384918212974135 with std:362.466596975736.\n",
      "46.384918212974135 0.04341478330055092\n",
      "The training loss is 6.214711116934238 with std:11.008645125778031. The val loss is 12.614601366107623 with std:49.219661966231705.\n",
      "12.614601366107623 0.04341478330055092\n",
      "The training loss is 6.448218608481236 with std:13.201051996959652. The val loss is 7.633957062066933 with std:10.580621083158402.\n",
      "7.633957062066933 0.04341478330055092\n",
      "The training loss is 5.68177290191544 with std:8.542293455954503. The val loss is 9.706272537320428 with std:25.72605564694157.\n",
      "9.706272537320428 0.04341478330055092\n",
      "Evaluating for {'lmda': 0.043816899315141926} ...\n",
      "The training loss is 6.090176933114914 with std:12.730874180737317. The val loss is 45.338199916005 with std:352.49346157096323.\n",
      "45.338199916005 0.043816899315141926\n",
      "The training loss is 6.215656795310738 with std:11.008309739941362. The val loss is 12.624366370037066 with std:49.29821967450469.\n",
      "12.624366370037066 0.043816899315141926\n",
      "The training loss is 6.449634910943271 with std:13.200645771682389. The val loss is 7.63260633409303 with std:10.5710767759256.\n",
      "7.63260633409303 0.043816899315141926\n",
      "The training loss is 5.684200387776973 with std:8.545485010296769. The val loss is 9.699891081784234 with std:25.70326450588228.\n",
      "9.699891081784234 0.043816899315141926\n",
      "Evaluating for {'lmda': 0.044222739805058996} ...\n",
      "The training loss is 6.092100430919592 with std:12.728844024349769. The val loss is 44.31454902209567 with std:342.74030427972866.\n",
      "44.31454902209567 0.044222739805058996\n",
      "The training loss is 6.216605350693569 with std:11.007973544908259. The val loss is 12.634169788023213 with std:49.377093961273026.\n",
      "12.634169788023213 0.044222739805058996\n",
      "The training loss is 6.4510580805861535 with std:13.200236815465368. The val loss is 7.631245193779526 with std:10.561494857785707.\n",
      "7.631245193779526 0.044222739805058996\n",
      "The training loss is 5.6866413863048315 with std:8.548699803022844. The val loss is 9.693540154627566 with std:25.68057062363026.\n",
      "9.693540154627566 0.044222739805058996\n",
      "Evaluating for {'lmda': 0.04463233926710395} ...\n",
      "The training loss is 6.094027039325462 with std:12.726807372275719. The val loss is 43.31358097999361 with std:333.2034632358824.\n",
      "43.31358097999361 0.04463233926710395\n",
      "The training loss is 6.217556825653823 with std:11.007636624001771. The val loss is 12.64401190174334 with std:49.45628752646965.\n",
      "12.64401190174334 0.04463233926710395\n",
      "The training loss is 6.452488222210607 with std:13.19982530015551. The val loss is 7.629873698642876 with std:10.55187559641447.\n",
      "7.629873698642876 0.04463233926710395\n",
      "The training loss is 5.6890959640369765 with std:8.55193797567933. The val loss is 9.687220398356377 with std:25.657974897766305.\n",
      "9.687220398356377 0.04463233926710395\n",
      "Evaluating for {'lmda': 0.04504573251759458} ...\n",
      "The training loss is 6.095956810369985 with std:12.724764435490584. The val loss is 42.33491538497354 with std:323.87931660941973.\n",
      "42.33491538497354 0.04504573251759458\n",
      "The training loss is 6.218511262442342 with std:11.007299060860618. The val loss is 12.653892990374633 with std:49.53580305453749.\n",
      "12.653892990374633 0.04504573251759458\n",
      "The training loss is 6.453925440968263 with std:13.199411399034885. The val loss is 7.628491907262959 with std:10.54221926888402.\n",
      "7.628491907262959 0.04504573251759458\n",
      "The training loss is 5.6915641865044995 with std:8.555199668972055. The val loss is 9.680932452842415 with std:25.63547819463862.\n",
      "9.680932452842415 0.04504573251759458\n",
      "Evaluating for {'lmda': 0.04546295469532399} ...\n",
      "The training loss is 6.097889796018618 with std:12.722715425677418. The val loss is 41.37817597552072 with std:314.76428259796734.\n",
      "41.37817597552072 0.04546295469532399\n",
      "The training loss is 6.219468702985274 with std:11.00696093943561. The val loss is 12.663813330525823 with std:49.61564321387372.\n",
      "12.663813330525823 0.04546295469532399\n",
      "The training loss is 6.455369842361873 with std:13.19899528682978. The val loss is 7.627099879304033 with std:10.532526161735936.\n",
      "7.627099879304033 0.04546295469532399\n",
      "The training loss is 5.694046118224127 with std:8.558485022759477. The val loss is 9.674676955207923 with std:25.613081349378778.\n",
      "9.674676955207923 0.04546295469532399\n",
      "Evaluating for {'lmda': 0.04588404126454759} ...\n",
      "The training loss is 6.099826048167387 with std:12.720660555225457. The val loss is 40.44299062957396 with std:305.85481941550165.\n",
      "40.44299062957396 0.04588404126454759\n",
      "The training loss is 6.220429188881942 with std:11.006622343990776. The val loss is 12.673773196210808 with std:49.69581065656315.\n",
      "12.673773196210808 0.04588404126454759\n",
      "The training loss is 6.4568215322440325 with std:13.198577139714692. The val loss is 7.62569767552154 with std:10.522796571046266.\n",
      "7.62569767552154 0.04588404126454759\n",
      "The training loss is 5.69654182269581 with std:8.561794176046286. The val loss is 9.668454539695249 with std:25.590785165832777.\n",
      "9.668454539695249 0.04588404126454759\n",
      "Evaluating for {'lmda': 0.04630902801799739} ...\n",
      "The training loss is 6.101765618643252 with std:12.718600037227544. The val loss is 39.528991358853254 with std:297.14742526434725.\n",
      "39.528991358853254 0.04630902801799739\n",
      "The training loss is 6.221392761400305 with std:11.006283359102571. The val loss is 12.683772858786536 with std:49.7763080178541.\n",
      "12.683772858786536 0.04630902801799739\n",
      "The training loss is 6.4582806168183104 with std:13.198157135324582. The val loss is 7.624285357788127 with std:10.513030802512173.\n",
      "7.624285357788127 0.04630902801799739\n",
      "The training loss is 5.699051362395366 with std:8.565127266973242. The val loss is 9.662265837560513 with std:25.568590416619802.\n",
      "9.662265837560513 0.04630902801799739\n",
      "Evaluating for {'lmda': 0.04673795107992461} ...\n",
      "The training loss is 6.103708559208718 with std:12.716534085487238. The val loss is 38.6358143033421 with std:288.6386383101527.\n",
      "38.6358143033421 0.04673795107992461\n",
      "The training loss is 6.2223594614743885 with std:11.005944069658552. The val loss is 12.693812586908695 with std:49.85713791574439.\n",
      "12.693812586908695 0.04673795107992461\n",
      "The training loss is 6.459747202636252 with std:13.197735452759952. The val loss is 7.622862989101304 with std:10.50322917151664.\n",
      "7.622862989101304 0.04673795107992461\n",
      "The training loss is 5.701574798770943 with std:8.568484432812179. The val loss is 9.656111476950043 with std:25.546497843121813.\n",
      "9.656111476950043 0.04673795107992461\n",
      "Evaluating for {'lmda': 0.04717084690917017} ...\n",
      "The training loss is 6.105654921561356 with std:12.714462914513177. The val loss is 37.76309972397549 with std:280.32503664153023.\n",
      "37.76309972397549 0.04717084690917017\n",
      "The training loss is 6.223329329700844 with std:11.005604560859828. The val loss is 12.703892646488727 with std:49.9383029506165.\n",
      "12.703892646488727 0.04717084690917017\n",
      "The training loss is 6.4612213965988845 with std:13.197312272600984. The val loss is 7.621430633603476 with std:10.49339200320459.\n",
      "7.621430633603476 0.04717084690917017\n",
      "The training loss is 5.704112192239607 with std:8.57186580996218. The val loss is 9.649992082775574 with std:25.52450815553686.\n",
      "9.649992082775574 0.04717084690917017\n",
      "Evaluating for {'lmda': 0.04760775230226368} ...\n",
      "The training loss is 6.107604757338051 with std:12.712386739517438. The val loss is 36.91049199491501 with std:272.20323822762793.\n",
      "36.91049199491501 0.04760775230226368\n",
      "The training loss is 6.224302406335881 with std:11.005264918216486. The val loss is 12.714013300640401 with std:50.01980570474827.\n",
      "12.714013300640401 0.04760775230226368\n",
      "The training loss is 6.4627033059548085 with std:13.196887776910394. The val loss is 7.619988356598441 with std:10.48351963256016.\n",
      "7.619988356598441 0.04760775230226368\n",
      "The training loss is 5.706663602182025 with std:8.575271533939647. The val loss is 9.643908276608418 with std:25.5026220329377.\n",
      "9.643908276608418 0.04760775230226368\n",
      "Evaluating for {'lmda': 0.04804870439655132} ...\n",
      "The training loss is 6.109558118117878 with std:12.710305776423187. The val loss is 36.077639594620315 with std:264.2699008660853.\n",
      "36.077639594620315 0.04804870439655132\n",
      "The training loss is 6.225278731291726 with std:11.004925227551114. The val loss is 12.724174809628563 with std:50.101648741860735.\n",
      "12.724174809628563 0.04804870439655132\n",
      "The training loss is 6.464193038299728 with std:13.19646214924346. The val loss is 7.618536224563875 with std:10.47361240445682.\n",
      "7.618536224563875 0.04804870439655132\n",
      "The training loss is 5.709229086938405 with std:8.578701739371562. The val loss is 9.637860676558526 with std:25.480840123357638.\n",
      "9.637860676558526 0.04804870439655132\n",
      "Evaluating for {'lmda': 0.048493740673352353} ...\n",
      "The training loss is 6.1115150554239355 with std:12.7082202418586. The val loss is 35.26419509650471 with std:256.5217221291651.\n",
      "35.26419509650471 0.048493740673352353\n",
      "The training loss is 6.226258344133894 with std:11.004585574995923. The val loss is 12.734377430825015 with std:50.18383460671269.\n",
      "12.734377430825015 0.048493740673352353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 6.4656907015754745 with std:13.196035574658287. The val loss is 7.617074305171906 with std:10.463670673754153.\n",
      "7.617074305171906 0.048493740673352353\n",
      "The training loss is 5.711808703806775 with std:8.582156559995195. The val loss is 9.63184989715672 with std:25.459163043886335.\n",
      "9.63184989715672 0.048493740673352353\n",
      "Evaluating for {'lmda': 0.048942898961145294} ...\n",
      "The training loss is 6.113475620727165 with std:12.706130353160482. The val loss is 34.469815158489276 with std:248.95543930165053.\n",
      "34.469815158489276 0.048942898961145294\n",
      "The training loss is 6.227241284077501 with std:11.004246046992355. The val loss is 12.744621418656928 with std:50.266365824653214.\n",
      "12.744621418656928 0.048942898961145294\n",
      "The training loss is 6.467196404068915 with std:13.19560823972174. The val loss is 7.615602667296526 with std:10.453694805327286.\n",
      "7.615602667296526 0.048942898961145294\n",
      "The training loss is 5.714402509037687 with std:8.585636128645532. The val loss is 9.625876549244767 with std:25.437591380775533.\n",
      "9.625876549244767 0.048942898961145294\n",
      "Evaluating for {'lmda': 0.04939621743878321} ...\n",
      "The training loss is 6.1154398654486295 with std:12.704036328378626. The val loss is 33.69416051191373 with std:241.56782931499245.\n",
      "33.69416051191373 0.04939621743878321\n",
      "The training loss is 6.22822758998444 with std:11.00390673029332. The val loss is 12.754907024550723 with std:50.349244901088596.\n",
      "12.754907024550723 0.04939621743878321\n",
      "The training loss is 6.468710254410068 with std:13.19518033252204. The val loss is 7.614121381041783 with std:10.443685174180276.\n",
      "7.614121381041783 0.04939621743878321\n",
      "The training loss is 5.717010557831515 with std:8.589140577254502. The val loss is 9.619941239858885 with std:25.416125689596438.\n",
      "9.619941239858885 0.04939621743878321\n",
      "Evaluating for {'lmda': 0.049853734638738934} ...\n",
      "The training loss is 6.117407840962798 with std:12.701938386270125. The val loss is 32.936895948970104 with std:234.35570866991364.\n",
      "32.936895948970104 0.049853734638738934\n",
      "The training loss is 6.229217300360504 with std:11.00356771195748. The val loss is 12.765234496894864 with std:50.43247432116305.\n",
      "12.765234496894864 0.049853734638738934\n",
      "The training loss is 6.470232361571722 with std:13.194752042671258. The val loss is 7.612630517746202 with std:10.433642165469026.\n",
      "7.612630517746202 0.049853734638738934\n",
      "The training loss is 5.719632904336852 with std:8.592670036845812. The val loss is 9.61404457211871 with std:25.394766495372615.\n",
      "9.61404457211871 0.049853734638738934\n",
      "Evaluating for {'lmda': 0.05031548945038054} ...\n",
      "The training loss is 6.119379598603091 with std:12.699836746308891. The val loss is 32.19769031097483 with std:227.31593336978904.\n",
      "32.19769031097483 0.05031548945038054\n",
      "The training loss is 6.230210453351906 with std:11.003229079354128. The val loss is 12.77560408097527 with std:50.51605654915711.\n",
      "12.77560408097527 0.05031548945038054\n",
      "The training loss is 6.471762834867512 with std:13.194323561318695. The val loss is 7.611130150003039 with std:10.423566174582056.\n",
      "7.611130150003039 0.05031548945038054\n",
      "The training loss is 5.722269601646014 with std:8.596224637527035. The val loss is 9.608187145111541 with std:25.373514292754567.\n",
      "9.608187145111541 0.05031548945038054\n",
      "Evaluating for {'lmda': 0.05078152112327673} ...\n",
      "The training loss is 6.12135518966221 with std:12.69773162868161. The val loss is 31.476216474130577 with std:220.44539883307252.\n",
      "31.476216474130577 0.05078152112327673\n",
      "The training loss is 6.23120708674265 with std:11.002890920159734. The val loss is 12.786016018932836 with std:50.599994028112704.\n",
      "12.786016018932836 0.05078152112327673\n",
      "The training loss is 6.473301783950603 with std:13.193895081157617. The val loss is 7.609620351676685 with std:10.413457607202558.\n",
      "7.609620351676685 0.05078152112327673\n",
      "The training loss is 5.7249207017951385 with std:8.599804508490662. The val loss is 9.60236955378836 with std:25.352369546221816.\n",
      "9.60236955378836 0.05078152112327673\n",
      "Evaluating for {'lmda': 0.0512518692705333} ...\n",
      "The training loss is 6.123334665398739 with std:12.695623254298159. The val loss is 30.772151335420876 with std:213.74103981008773.\n",
      "30.772151335420876 0.0512518692705333\n",
      "The training loss is 6.232207237951372 with std:11.002553322358825. The val loss is 12.796470549707955 with std:50.68428917930769.\n",
      "12.796470549707955 0.0512518692705333\n",
      "The training loss is 6.474849318811613 with std:13.193466796432414. The val loss is 7.608101197916243 with std:10.403316879362443.\n",
      "7.608101197916243 0.0512518692705333\n",
      "The training loss is 5.727586255759398 with std:8.603409778002174. The val loss is 9.596592388849755 with std:25.33133269028024.\n",
      "9.596592388849755 0.0512518692705333\n",
      "Evaluating for {'lmda': 0.05172657387216019} ...\n",
      "The training loss is 6.1253180770386475 with std:12.69351184478376. The val loss is 30.085175798098422 with std:207.19983029941207.\n",
      "30.085175798098422 0.05172657387216019\n",
      "The training loss is 6.233210944027981 with std:11.002216374238994. The val loss is 12.806967908996086 with std:50.76894440185815.\n",
      "12.806967908996086 0.05172657387216019\n",
      "The training loss is 6.47640554977701 with std:13.193038902950207. The val loss is 7.606572765168229 with std:10.393144417493593.\n",
      "7.606572765168229 0.05172657387216019\n",
      "The training loss is 5.730266313452705 with std:8.607040573402406. The val loss is 9.590856236637826 with std:25.310404129688.\n",
      "9.590856236637826 0.05172657387216019\n",
      "Evaluating for {'lmda': 0.052205675278469754} ...\n",
      "The training loss is 6.1273054757812355 with std:12.691397622489824. The val loss is 29.414974755820445 with std:200.81878345493772.\n",
      "29.414974755820445 0.052205675278469754\n",
      "The training loss is 6.2342182416511465 with std:11.001880164398518. The val loss is 12.81750832917902 with std:50.85396207208585.\n",
      "12.81750832917902 0.052205675278469754\n",
      "The training loss is 6.477970587507048 with std:13.192611598085731. The val loss is 7.605035131198227 with std:10.38294065850076.\n",
      "7.605035131198227 0.052205675278469754\n",
      "The training loss is 5.732960923725565 with std:8.610697021100268. The val loss is 9.585161679029795 with std:25.289584239700297.\n",
      "9.585161679029795 0.052205675278469754\n",
      "Evaluating for {'lmda': 0.052689214213506745} ...\n",
      "The training loss is 6.129296912802045 with std:12.689280810496363. The val loss is 28.761237076615686 with std:194.59495149526418.\n",
      "28.761237076615686 0.052689214213506745\n",
      "The training loss is 6.235229167125808 with std:11.001544781737243. The val loss is 12.828092039300298 with std:50.939344543271176.\n",
      "12.828092039300298 0.052689214213506745\n",
      "The training loss is 6.479544542993139 with std:13.19218508079064. The val loss is 7.603488375100852 with std:10.372706049802568.\n",
      "7.603488375100852 0.052689214213506745\n",
      "The training loss is 5.735670134363814 with std:8.614379246569873. The val loss is 9.579509293330409 with std:25.268873366319053.\n",
      "9.579509293330409 0.052689214213506745\n",
      "Evaluating for {'lmda': 0.05317723177850969} ...\n",
      "The training loss is 6.1312924392579315 with std:12.687161632611335. The val loss is 28.123655585594065 with std:188.52542560529852.\n",
      "28.123655585594065 0.05317723177850969\n",
      "The training loss is 6.236243756378916 with std:11.001210315461838. The val loss is 12.838719264982329 with std:51.02509414490849.\n",
      "12.838719264982329 0.05317723177850969\n",
      "The training loss is 6.481127527556806 with std:13.191759551604322. The val loss is 7.601932577315613 with std:10.362441049383216.\n",
      "7.601932577315613 0.05317723177850969\n",
      "The training loss is 5.738393992087382 with std:8.618087374347235. The val loss is 9.573899652169018 with std:25.24827182658593.\n",
      "9.573899652169018 0.05317723177850969\n",
      "Evaluating for {'lmda': 0.05366976945540476} ...\n",
      "The training loss is 6.133292106290047 with std:12.685040313380041. The val loss is 27.50192704839288 with std:182.60733584928204.\n",
      "27.50192704839288 0.05366976945540476\n",
      "The training loss is 6.237262044958053 with std:11.000876855081135. The val loss is 12.849390228393023 with std:51.111213182369.\n",
      "12.849390228393023 0.05366976945540476\n",
      "The training loss is 6.482719652845329 with std:13.191335212659613. The val loss is 7.60036781964054 with std:10.35214612584351.\n",
      "7.60036781964054 0.05366976945540476\n",
      "The training loss is 5.741132542548957 with std:8.621821528026441. The val loss is 9.568333323396343 with std:25.22777990887087.\n",
      "9.568333323396343 0.05366976945540476\n",
      "Evaluating for {'lmda': 0.05416686911033152} ...\n",
      "The training loss is 6.135295965030367 with std:12.682917078086158. The val loss is 26.895752152164164 with std:176.837851065348.\n",
      "26.895752152164164 0.05416686911033152\n",
      "The training loss is 6.238284068027757 with std:11.000544490408366. The val loss is 12.86010514819296 with std:51.19770393642627.\n",
      "12.86010514819296 0.05416686911033152\n",
      "The training loss is 6.484321030831011 with std:13.19091226769169. The val loss is 7.598794185250227 with std:10.34182175845495.\n",
      "7.598794185250227 0.05416686911033152\n",
      "The training loss is 5.743885830333152 with std:8.6255818302569. The val loss is 9.562810869979506 with std:25.207397873172454.\n",
      "9.562810869979506 0.05416686911033152\n",
      "Evaluating for {'lmda': 0.054668572997201806} ...\n",
      "The training loss is 6.137304066605195 with std:12.680792152756322. The val loss is 26.304835487458096 with std:171.21417877359335.\n",
      "26.304835487458096 0.054668572997201806\n",
      "The training loss is 6.239309860367015 with std:11.000213311558115. The val loss is 12.870864239470839 with std:51.28456866263476.\n",
      "12.870864239470839 0.054668572997201806\n",
      "The training loss is 6.485931773807173 with std:13.190490922042859. The val loss is 7.597211758704579 with std:10.331468437186707.\n",
      "7.597211758704579 0.054668572997201806\n",
      "The training loss is 5.746653898958164 with std:8.629368402742067. The val loss is 9.557332849898055 with std:25.187125951430062.\n",
      "9.557332849898055 0.054668572997201806\n",
      "Evaluating for {'lmda': 0.05517492376129126} ...\n",
      "The training loss is 6.139316462138214 with std:12.678665764166073. The val loss is 25.72888552876431 with std:165.73356507710977.\n",
      "25.72888552876431 0.05517492376129126\n",
      "The training loss is 6.240339456365796 with std:10.999883408947335. The val loss is 12.881667713694663 with std:51.37180959089027.\n",
      "12.881667713694663 0.05517492376129126\n",
      "The training loss is 6.4875519943853135 with std:13.19007138268089. The val loss is 7.595620625968585 with std:10.321086662773114.\n",
      "7.595620625968585 0.05517492376129126\n",
      "The training loss is 5.749436790870704 with std:8.63318136623201. The val loss is 9.551899816056267 with std:25.166964347929568.\n",
      "9.551899816056267 0.05517492376129126\n",
      "Evaluating for {'lmda': 0.05568596444286412} ...\n",
      "The training loss is 6.141333202759467 with std:12.676538139845292. The val loss is 25.1676146151078 with std:160.39329456922425.\n",
      "25.1676146151078 0.05568596444286412\n",
      "The training loss is 6.241372890023962 with std:10.999554873293182. The val loss is 12.892515778669328 with std:51.45942892499691.\n",
      "12.892515778669328 0.05568596444286412\n",
      "The training loss is 6.489181805492572 with std:13.189653858195056. The val loss is 7.594020874423026 with std:10.31067694673412.\n",
      "7.594020874423026 0.05568596444286412\n",
      "The training loss is 5.752234547451433 with std:8.637020840526493. The val loss is 9.546512316168226 with std:25.14691323955904.\n",
      "9.546512316168226 0.05568596444286412\n",
      "Evaluating for {'lmda': 0.05620173848083188} ...\n",
      "The training loss is 6.143354339605718 with std:12.6744095080815. The val loss is 24.62073892934481 with std:155.1906902349784.\n",
      "24.62073892934481 0.05620173848083188\n",
      "The training loss is 6.242410194946351 with std:10.999227795611555. The val loss is 12.90340863846391 with std:51.54742884201187.\n",
      "12.90340863846391 0.05620173848083188\n",
      "The training loss is 6.490821320367693 with std:13.189238558812768. The val loss is 7.592412592876502 with std:10.300239811417322.\n",
      "7.592412592876502 0.05620173848083188\n",
      "The training loss is 5.755047209009328 with std:8.640886944467772. The val loss is 9.541170892677261 with std:25.126972776286735.\n",
      "9.541170892677261 0.05620173848083188\n",
      "Evaluating for {'lmda': 0.05672228971644543} ...\n",
      "The training loss is 6.145379923828953 with std:12.672280097928814. The val loss is 24.087978477758107 with std:150.12311336206133.\n",
      "24.087978477758107 0.05672228971644543\n",
      "The training loss is 6.243451404341813 with std:10.998902267219387. The val loss is 12.914346493372015 with std:51.6358114918461.\n",
      "12.914346493372015 0.05672228971644543\n",
      "The training loss is 6.492470652557644 with std:13.18882569640155. The val loss is 7.59079587158296 with std:10.28977579003838.\n",
      "7.59079587158296 0.05672228971644543\n",
      "The training loss is 5.757874814788556 with std:8.644779795943741. The val loss is 9.535876082645396 with std:25.107143081463192.\n",
      "9.535876082645396 0.05672228971644543\n",
      "Evaluating for {'lmda': 0.05724766239702179} ...\n",
      "The training loss is 6.147410006599181 with std:12.670150139211714. The val loss is 23.56905706838453 with std:145.18796344721.\n",
      "23.56905706838453 0.05724766239702179\n",
      "The training loss is 6.244496551019602 with std:10.998578379730331. The val loss is 12.925329539849502 with std:51.7245789966764.\n",
      "12.925329539849502 0.05724766239702179\n",
      "The training loss is 6.494129915914383 with std:13.18841548448321. The val loss is 7.589170802252992 with std:10.279285426715584.\n",
      "7.589170802252992 0.05724766239702179\n",
      "The training loss is 5.760717402962909 with std:8.64869951188061. The val loss is 9.530628417671 with std:25.08742425227443.\n",
      "9.530628417671 0.05724766239702179\n",
      "Evaluating for {'lmda': 0.05777790117970504} ...\n",
      "The training loss is 6.149444639111151 with std:12.668019862531903. The val loss is 23.063702289711387 with std:140.3826781137191.\n",
      "23.063702289711387 0.05777790117970504\n",
      "The training loss is 6.2455456673870815 with std:10.998256225054638. The val loss is 12.936357970467952 with std:51.81373345049986.\n",
      "12.936357970467952 0.05777790117970504\n",
      "The training loss is 6.495799224590083 with std:13.188008138237112. The val loss is 7.587537478064629 with std:10.268769276486362.\n",
      "7.587537478064629 0.05777790117970504\n",
      "The training loss is 5.76357501063891 with std:8.652646208243953. The val loss is 9.52542842379003 with std:25.06781636013396.\n",
      "9.52542842379003 0.05777790117970504\n",
      "Evaluating for {'lmda': 0.058313051135262216} ...\n",
      "The training loss is 6.15148387258815 with std:12.66588949927611. The val loss is 22.571645488149194 with std:135.70473302591904.\n",
      "22.571645488149194 0.058313051135262216\n",
      "The training loss is 6.246598785447 with std:10.9979358953992. The val loss is 12.947431973847408 with std:51.90327691849293.\n",
      "12.947431973847408 0.058313051135262216\n",
      "The training loss is 6.497478693033756 with std:13.18760387450995. The val loss is 7.585895993678745 with std:10.258227905344988.\n",
      "7.585895993678745 0.058313051135262216\n",
      "The training loss is 5.766447673858257 with std:8.65662000003643. The val loss is 9.520276621386227 with std:25.048319451123188.\n",
      "9.520276621386227 0.058313051135262216\n",
      "Evaluating for {'lmda': 0.058853157751914506} ...\n",
      "The training loss is 6.153527758289024 with std:12.663759281621985. The val loss is 22.09262174552652 with std:131.15164181269077.\n",
      "22.09262174552652 0.058853157751914506\n",
      "The training loss is 6.247655936794872 with std:10.997617483265392. The val loss is 12.958551734618483 with std:51.99321143663092.\n",
      "12.958551734618483 0.058853157751914506\n",
      "The training loss is 6.499168435986371 with std:13.18720291182267. The val loss is 7.584246445250366 with std:10.247661890267043.\n",
      "7.584246445250366 0.058853157751914506\n",
      "The training loss is 5.769335427595586 with std:8.660621001296144. The val loss is 9.515173525102737 with std:25.0289335464263.\n",
      "9.515173525102737 0.058853157751914506\n",
      "Evaluating for {'lmda': 0.05939826693920356} ...\n",
      "The training loss is 6.155576347512521 with std:12.661629442544394. The val loss is 21.626369855845578 with std:126.72095599364428.\n",
      "21.626369855845578 0.05939826693920356\n",
      "The training loss is 6.248717152616414 with std:10.997301081448212. The val loss is 12.969717433351022 with std:52.08353901102923.\n",
      "12.969717433351022 0.05939826693920356\n",
      "The training loss is 6.500868568476954 with std:13.186805470380305. The val loss is 7.582588930441426 with std:10.23707181922726.\n",
      "7.582588930441426 0.05939826693920356\n",
      "The training loss is 5.7722383057621585 with std:8.66464932509512. The val loss is 9.510119643753203 with std:25.00965864278971.\n",
      "9.510119643753203 0.05939826693920356\n",
      "Evaluating for {'lmda': 0.05994842503189409} ...\n",
      "The training loss is 6.157629691602612 with std:12.659500215827327. The val loss is 21.17263230210025 with std:122.41026491610921.\n",
      "21.17263230210025 0.05994842503189409\n",
      "The training loss is 6.249782463684992 with std:10.996986783035242. The val loss is 12.98092924651069 with std:52.17426161748492.\n",
      "12.98092924651069 0.05994842503189409\n",
      "The training loss is 6.502579205816823 with std:13.186411772076584. The val loss is 7.580923548428735 with std:10.226458291208605.\n",
      "7.580923548428735 0.05994842503189409\n",
      "The training loss is 5.775156341206817 with std:8.668705083538974. The val loss is 9.505115480236238 with std:24.99049471299557.\n",
      "9.505115480236238 0.05994842503189409\n",
      "Evaluating for {'lmda': 0.06050367879391221} ...\n",
      "The training loss is 6.159687841954446 with std:12.657371836068373. The val loss is 20.731155232131815 with std:118.21719569395749.\n",
      "20.731155232131815 0.06050367879391221\n",
      "The training loss is 6.250851900359255 with std:10.996674681406084. The val loss is 12.99218734640003 with std:52.26538120092902.\n",
      "12.99218734640003 0.06050367879391221\n",
      "The training loss is 6.504300463595382 with std:13.18602204050561. The val loss is 7.579250399922622 with std:10.215821916243295.\n",
      "7.579250399922622 0.06050367879391221\n",
      "The training loss is 5.778089565716406 with std:8.672788387763894. The val loss is 9.500161531450095 with std:24.971441706327127.\n",
      "9.500161531450095 0.06050367879391221\n",
      "Evaluating for {'lmda': 0.061064075422320396} ...\n",
      "The training loss is 6.16175085002046 with std:12.65524453868593. The val loss is 20.301688434403168 with std:114.13941315731789.\n",
      "20.301688434403168 0.061064075422320396\n",
      "The training loss is 6.2519254925800425 with std:10.99636487023046. The val loss is 13.003491901097238 with std:52.35689967482495.\n",
      "13.003491901097238 0.061064075422320396\n",
      "The training loss is 6.506032457674815 with std:13.185636500965636. The val loss is 7.577569587170088 with std:10.205163315393754.\n",
      "7.577569587170088 0.061064075422320396\n",
      "The training loss is 5.78103801001871 with std:8.676899347937377. The val loss is 9.495258288209762 with std:24.95249954908416.\n",
      "9.495258288209762 0.061064075422320396\n",
      "Evaluating for {'lmda': 0.06162966255132942} ...\n",
      "The training loss is 6.163818767314433 with std:12.65311855993216. The val loss is 19.883985313545146 with std:110.17461981252698.\n",
      "19.883985313545146 0.06162966255132942\n",
      "The training loss is 6.2530032698685485 with std:10.996057443466809. The val loss is 13.014843074412338 with std:52.44881892073751.\n",
      "13.014843074412338 0.06162966255132942\n",
      "The training loss is 6.50777530418408 with std:13.185255380468053. The val loss is 7.5758812139700265 with std:10.194483120777448.\n",
      "7.5758812139700265 0.06162966255132942\n",
      "The training loss is 5.784001703784186 with std:8.681038073256543. The val loss is 9.490406235165958 with std:24.933668145052575.\n",
      "9.490406235165958 0.06162966255132942\n",
      "Evaluating for {'lmda': 0.062200488256347115} ...\n",
      "The training loss is 6.165891645418899 with std:12.650994136897648. The val loss is 19.47780286514946 with std:106.32055580818397.\n",
      "19.47780286514946 0.062200488256347115\n",
      "The training loss is 6.254085261323719 with std:10.995752495360636. The val loss is 13.026241025821319 with std:52.54114078768786.\n",
      "13.026241025821319 0.062200488256347115\n",
      "The training loss is 6.509529119514185 with std:13.184878907746196. The val loss is 7.574185385684184 with std:10.183781975575753.\n",
      "7.574185385684184 0.062200488256347115\n",
      "The training loss is 5.78698067562715 with std:8.685204671946444. The val loss is 9.48560585072426 with std:24.914947376052165.\n",
      "9.48560585072426 0.062200488256347115\n",
      "Evaluating for {'lmda': 0.062776601058065} ...\n",
      "The training loss is 6.167969535990245 with std:12.648871507523634. The val loss is 19.082901650597513 with std:102.57499891545764.\n",
      "19.082901650597513 0.062776601058065\n",
      "The training loss is 6.255171495619421 with std:10.995450120444419. The val loss is 13.037685910420318 with std:52.63386709169946.\n",
      "13.037685910420318 0.062776601058065\n",
      "The training loss is 6.51129402031143 with std:13.184507313261431. The val loss is 7.5724822092429145 with std:10.17306053401941.\n",
      "7.5724822092429145 0.062776601058065\n",
      "The training loss is 5.789974953109479 with std:8.689399251262916. The val loss is 9.480857606968696 with std:24.89633710244247.\n",
      "9.480857606968696 0.062776601058065\n",
      "Evaluating for {'lmda': 0.0633580499265825} ...\n",
      "The training loss is 6.170052490763657 with std:12.646750910609516. The val loss is 18.699045771379723 with std:98.93576451842955.\n",
      "18.699045771379723 0.0633580499265825\n",
      "The training loss is 6.256262001002367 with std:10.995150413535512. The val loss is 13.049177878855911 with std:52.72699961511402.\n",
      "13.049177878855911 0.0633580499265825\n",
      "The training loss is 6.513070123470454 with std:13.18414082920864. The val loss is 7.570771793159029 with std:10.16231946140377.\n",
      "7.570771793159029 0.0633580499265825\n",
      "The training loss is 5.7929845627407595 with std:8.69362191748483. The val loss is 9.476161969585366 with std:24.877837163661518.\n",
      "9.476161969585366 0.0633580499265825\n",
      "Evaluating for {'lmda': 0.0639448842855694} ...\n",
      "The training loss is 6.172140561559705 with std:12.644632585825844. The val loss is 18.326002843195322 with std:95.4007056178684.\n",
      "18.326002843195322 0.0639448842855694\n",
      "The training loss is 6.257356805289709 with std:10.994853469734265. The val loss is 13.060717077287427 with std:52.82054010622424.\n",
      "13.060717077287427 0.0639448842855694\n",
      "The training loss is 6.514857546130272 with std:13.183779689527828. The val loss is 7.569054247534947 with std:10.15155943407182.\n",
      "7.569054247534947 0.0639448842855694\n",
      "The training loss is 5.796009529983014 with std:8.697872775922153. The val loss is 9.471519397788306 with std:24.859447378768056.\n",
      "9.471519397788306 0.0639448842855694\n",
      "Evaluating for {'lmda': 0.06453715401646702} ...\n",
      "The training loss is 6.174233800290169 with std:12.642516773719274. The val loss is 17.96354396965956 with std:91.96771284767492.\n",
      "17.96354396965956 0.06453715401646702\n",
      "The training loss is 6.258455935866799 with std:10.994559384424209. The val loss is 13.072303647312923 with std:52.914490278524404.\n",
      "13.072303647312923 0.06453715401646702\n",
      "The training loss is 6.516656405665408 with std:13.183424129907927. The val loss is 7.567329684069722 with std:10.140781139403117.\n",
      "7.567329684069722 0.06453715401646702\n",
      "The training loss is 5.799049879252764 with std:8.702151930910587. The val loss is 9.466930344239957 with std:24.841167546978443.\n",
      "9.466930344239957 0.06453715401646702\n",
      "Evaluating for {'lmda': 0.06513490946272803} ...\n",
      "The training loss is 6.176332258963349 with std:12.640403715727519. The val loss is 17.611443715739938 with std:88.63471450588929.\n",
      "17.611443715739938 0.06513490946272803\n",
      "The training loss is 6.259559419683893 with std:10.9942682532669. The val loss is 13.083937725930308 with std:53.00885181033262.\n",
      "13.083937725930308 0.06513490946272803\n",
      "The training loss is 6.518466819679102 with std:13.183074387794003. The val loss is 7.565598216071293 with std:10.129985275811833.\n",
      "7.565598216071293 0.06513490946272803\n",
      "The training loss is 5.802105633921652 with std:8.706459485810013. The val loss is 9.462395254995421 with std:24.822997448277185.\n",
      "9.462395254995421 0.06513490946272803\n",
      "Evaluating for {'lmda': 0.06573820143409585} ...\n",
      "The training loss is 6.178435989690254 with std:12.638293654189214. The val loss is 17.26948008108237 with std:85.3996766023829.\n",
      "17.26948008108237 0.06573820143409585\n",
      "The training loss is 6.26066728325515 with std:10.993980172204282. The val loss is 13.095619445469557 with std:53.103626344118005.\n",
      "13.095619445469557 0.06573820143409585\n",
      "The training loss is 6.5202889059967815 with std:13.18273070239625. The val loss is 7.563859958463008 with std:10.119172552726024.\n",
      "7.563859958463008 0.06573820143409585\n",
      "The training loss is 5.805176816322147 with std:8.71079554300716. The val loss is 9.457914569415914 with std:24.80493684391482.\n",
      "9.457914569415914 0.06573820143409585\n",
      "Evaluating for {'lmda': 0.06634708121092348} ...\n",
      "The training loss is 6.180545044690097 with std:12.636186832352957. The val loss is 16.937434472908073 with std:82.26060292088226.\n",
      "16.937434472908073 0.06634708121092348\n",
      "The training loss is 6.261779552655515 with std:10.99369523745513. The val loss is 13.107348933540441 with std:53.19881548597762.\n",
      "13.107348933540441 0.06634708121092348\n",
      "The training loss is 6.5221227826588635 with std:13.18239331469359. The val loss is 7.562115027787933 with std:10.108343690554891.\n",
      "7.562115027787933 0.06634708121092348\n",
      "The training loss is 5.808263447749278 with std:8.715160203913813. The val loss is 9.453488720113707 with std:24.786985477046684.\n",
      "9.453488720113707 0.06634708121092348\n",
      "Evaluating for {'lmda': 0.06696160054853219} ...\n",
      "The training loss is 6.182659476296478 with std:12.634083494391234. The val loss is 16.615091678640024 with std:79.21553509723327.\n",
      "16.615091678640024 0.06696160054853219\n",
      "The training loss is 6.2628962535183454 with std:10.993413545513139. The val loss is 13.119126312980793 with std:53.29442080508883.\n",
      "13.119126312980793 0.06696160054853219\n",
      "The training loss is 6.523968567911384 with std:13.182062467443286. The val loss is 7.560363542218993 with std:10.09749942067494.\n",
      "7.560363542218993 0.06696160054853219\n",
      "The training loss is 5.811365548463351 with std:8.719553568967022. The val loss is 9.44911813288003 with std:24.769143073265457.\n",
      "9.44911813288003 0.06696160054853219\n",
      "Evaluating for {'lmda': 0.0675818116816111} ...\n",
      "The training loss is 6.184779336962486 with std:12.631983885410637. The val loss is 16.302239838527857 with std:76.26255271669909.\n",
      "16.302239838527857 0.0675818116816111\n",
      "The training loss is 6.264017411033855 with std:10.993135193146335. The val loss is 13.130951701794967 with std:53.390443833134306.\n",
      "13.130951701794967 0.0675818116816111\n",
      "The training loss is 6.525826380199659 with std:13.181738405186193. The val loss is 7.558605621565659 with std:10.086640485401286.\n",
      "7.558605621565659 0.0675818116816111\n",
      "The training loss is 5.814483137693582 with std:8.723975737628777. The val loss is 9.44480322662198 with std:24.751409341225614.\n",
      "9.44480322662198 0.0675818116816111\n",
      "Evaluating for {'lmda': 0.06820776732865685} ...\n",
      "The training loss is 6.1869046792667755 with std:12.629888251462976. The val loss is 15.998670417710896 with std:73.39977342506548.\n",
      "15.998670417710896 0.06820776732865685\n",
      "The training loss is 6.265143049945628 with std:10.992860277393623. The val loss is 13.142825213100176 with std:53.48688606373398.\n",
      "13.142825213100176 0.06820776732865685\n",
      "The training loss is 6.527696338158748 with std:13.181421374253482. The val loss is 7.556841387277235 with std:10.075767637947227.\n",
      "7.556841387277235 0.06820776732865685\n",
      "The training loss is 5.817616233641316 with std:8.728426808384771. The val loss is 9.440544413304423 with std:24.73378397322163.\n",
      "9.440544413304423 0.06820776732865685\n",
      "Evaluating for {'lmda': 0.06883952069645496} ...\n",
      "The training loss is 6.189035555919856 with std:12.627796839560386. The val loss is 15.704178178481099 with std:70.62535306029594.\n",
      "15.704178178481099 0.06883952069645496\n",
      "The training loss is 6.266273194549786 with std:10.992588895564145. The val loss is 13.154746955074893 with std:53.58374895190693.\n",
      "13.154746955074893 0.06883952069645496\n",
      "The training loss is 6.529578560605604 with std:13.181111622773978. The val loss is 7.555070962451662 with std:10.064881642392828.\n",
      "7.555070962451662 0.06883952069645496\n",
      "The training loss is 5.820764853482839 with std:8.732906878745741. The val loss is 9.436342097885968 with std:24.71626664579823.\n",
      "9.436342097885968 0.06883952069645496\n",
      "Evaluating for {'lmda': 0.06947712548460236} ...\n",
      "The training loss is 6.1911720197691515 with std:12.625709897683546. The val loss is 15.418561151968575 with std:67.93748579688157.\n",
      "15.418561151968575 0.06947712548460236\n",
      "The training loss is 6.267407868691466 with std:10.992321145234952. The val loss is 13.166717030895864 with std:53.681033913473996.\n",
      "13.166717030895864 0.06947712548460236\n",
      "The training loss is 6.531473166529674 with std:13.180809400676413. The val loss is 7.553294471836715 with std:10.05398327363355.\n",
      "7.553294471836715 0.06947712548460236\n",
      "The training loss is 5.823929013374201 with std:8.73741604524661. The val loss is 9.432196678261224 with std:24.698857020337112.\n",
      "9.432196678261224 0.06947712548460236\n",
      "Evaluating for {'lmda': 0.07012063589007181} ...\n",
      "The training loss is 6.193314123804229 with std:12.623627674798888. The val loss is 15.141620609823267 with std:65.33440430681894.\n",
      "15.141620609823267 0.07012063589007181\n",
      "The training loss is 6.268547095763788 with std:10.992057124249294. The val loss is 13.178735538693825 with std:53.77874232456134.\n",
      "13.178735538693825 0.07012063589007181\n",
      "The training loss is 6.533380275083432 with std:13.180514959701995. The val loss is 7.551512041838476 with std:10.043073317348759.\n",
      "7.551512041838476 0.07012063589007181\n",
      "The training loss is 5.827108728453245 with std:8.741954403446153. The val loss is 9.428108545204852 with std:24.681554743680877.\n",
      "9.428108545204852 0.07012063589007181\n",
      "Evaluating for {'lmda': 0.0707701066118189} ...\n",
      "The training loss is 6.195461921163342 with std:12.62155042086774. The val loss is 14.8731610357381 with std:62.81437993389911.\n",
      "14.8731610357381 0.0707701066118189\n",
      "The training loss is 6.269690898704493 with std:10.991796930713981. The val loss is 13.190802571482703 with std:53.87687552090995.\n",
      "13.190802571482703 0.0707701066118189\n",
      "The training loss is 6.535300005573193 with std:13.180228553404119. The val loss is 7.549723800520771 with std:10.03215256993054.\n",
      "7.549723800520771 0.0707701066118189\n",
      "The training loss is 5.830304012843139 with std:8.746522047925378. The val loss is 9.424078082315337 with std:24.664359448742555.\n",
      "9.424078082315337 0.0707701066118189\n",
      "Evaluating for {'lmda': 0.07142559285543126} ...\n",
      "The training loss is 6.197615465138263 with std:12.619478386861195. The val loss is 14.612990096785689 with std:60.375722878239465.\n",
      "14.612990096785689 0.07142559285543126\n",
      "The training loss is 6.2708392999946465 with std:10.99154066299645. The val loss is 13.202918217122102 with std:53.97543479746513.\n",
      "13.202918217122102 0.07142559285543126\n",
      "The training loss is 6.537232477449634 with std:13.179950437158396. The val loss is 7.547929877613816 with std:10.021221838449529.\n",
      "7.547929877613816 0.07142559285543126\n",
      "The training loss is 5.833514879657693 with std:8.751119072290123. The val loss is 9.420105665966148 with std:24.647270755122804.\n",
      "9.420105665966148 0.07142559285543126\n",
      "Evaluating for {'lmda': 0.07208715033782136} ...\n",
      "The training loss is 6.199774809180693 with std:12.617411824775205. The val loss is 14.360918614551453 with std:58.016782387061134.\n",
      "14.360918614551453 0.07208715033782136\n",
      "The training loss is 6.271992321656144 with std:10.99128841972639. The val loss is 13.215082558244664 with std:54.0744214076496.\n",
      "13.215082558244664 0.07208715033782136\n",
      "The training loss is 6.539177810296428 with std:13.179680868165923. The val loss is 7.546130404511278 with std:10.010281940574721.\n",
      "7.546130404511278 0.07208715033782136\n",
      "The training loss is 5.836741341003343 with std:8.755745569169049. The val loss is 9.416191665248556 with std:24.630288269721877.\n",
      "9.416191665248556 0.07208715033782136\n",
      "Evaluating for {'lmda': 0.07275483529196229} ...\n",
      "The training loss is 6.2019400069059625 with std:12.615350987640593. The val loss is 14.116760536075846 with std:55.7359469467306.\n",
      "14.116760536075846 0.07275483529196229\n",
      "The training loss is 6.273149985249433 with std:10.99104029978745. The val loss is 13.22729567222005 with std:54.17383656297098.\n",
      "13.22729567222005 0.07275483529196229\n",
      "The training loss is 6.5411361238209915 with std:13.179420105459343. The val loss is 7.544325514276107 with std:9.999333704520133.\n",
      "7.544325514276107 0.07275483529196229\n",
      "The training loss is 5.839983407982849 with std:8.760401630212199. The val loss is 9.412336441927941 with std:24.61341158736805.\n",
      "9.412336441927941 0.07275483529196229\n",
      "Evaluating for {'lmda': 0.07342870447166762} ...\n",
      "The training loss is 6.204111112100549 with std:12.613296129540158. The val loss is 13.880332904906295 with std:53.53164447198161.\n",
      "13.880332904906295 0.07342870447166762\n",
      "The training loss is 6.274312311871639 with std:10.99079640232072. The val loss is 13.2395576310815 with std:54.273681432289486.\n",
      "13.2395576310815 0.07342870447166762\n",
      "The training loss is 6.54310753784352 with std:13.179168409908423. The val loss is 7.542515341640611 with std:9.988377968966498.\n",
      "7.542515341640611 0.07342870447166762\n",
      "The training loss is 5.843241090700871 with std:8.76508734609325. The val loss is 9.408540350393112 with std:24.596640291432582.\n",
      "9.408540350393112 0.07342870447166762\n",
      "Evaluating for {'lmda': 0.0741088151564157} ...\n",
      "The training loss is 6.20628817872486 with std:12.611247505620126. The val loss is 13.651455831753033 with std:51.40234247928541.\n",
      "13.651455831753033 0.0741088151564157\n",
      "The training loss is 6.275479322154595 with std:10.990556826718644. The val loss is 13.251868501488767 with std:54.373957141394165.\n",
      "13.251868501488767 0.0741088151564157\n",
      "The training loss is 6.545092172284629 with std:13.17892604422618. The val loss is 7.540700023009824 with std:9.977415583001562.\n",
      "7.540700023009824 0.0741088151564157\n",
      "The training loss is 5.846514398265903 with std:8.769802806506794. The val loss is 9.404803737611838 with std:24.57997395445607.\n",
      "9.404803737611838 0.0741088151564157\n",
      "Evaluating for {'lmda': 0.07479522515621821} ...\n",
      "The training loss is 6.208471260921055 with std:12.609205372109162. The val loss is 13.429952465132496 with std:49.34654823668914.\n",
      "13.429952465132496 0.07479522515621821\n",
      "The training loss is 6.276651036261671 with std:10.990321672623542. The val loss is 13.264228344663625 with std:54.47466477236354.\n",
      "13.264228344663625 0.07479522515621821\n",
      "The training loss is 6.547090147155121 with std:13.178693272970895. The val loss is 7.538879696458008 with std:9.96644740601757.\n",
      "7.538879696458008 0.07479522515621821\n",
      "The training loss is 5.849803338795133 with std:8.774548100169904. The val loss is 9.401126943086902 with std:24.563412138779405.\n",
      "9.401126943086902 0.07479522515621821\n",
      "Evaluating for {'lmda': 0.07548799281653436} ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 6.2106604130161145 with std:12.607169986325678. The val loss is 13.215648961949636 with std:47.362808876235206.\n",
      "13.215648961949636 0.07548799281653436\n",
      "The training loss is 6.277827473887611 with std:10.990091039925522. The val loss is 13.276637216335162 with std:54.575805362974776.\n",
      "13.276637216335162 0.07548799281653436\n",
      "The training loss is 6.549101582544937 with std:13.178470362553957. The val loss is 7.537054501730542 with std:9.955474307640916.\n",
      "7.537054501730542 0.07548799281653436\n",
      "The training loss is 5.853107919417396 with std:8.779323314818114. The val loss is 9.39751029881458 with std:24.54695439715821.\n",
      "9.39751029881458 0.07548799281653436\n",
      "Evaluating for {'lmda': 0.07618717702322995} ...\n",
      "The training loss is 6.2128556895280065 with std:12.605141606699412. The val loss is 13.00837445791618 with std:45.44971145094358.\n",
      "13.00837445791618 0.07618717702322995\n",
      "The training loss is 6.279008654254589 with std:10.98986502876005. The val loss is 13.289095166693878 with std:54.67737990623292.\n",
      "13.289095166693878 0.07618717702322995\n",
      "The training loss is 6.551126598608182 with std:13.178257581242036. The val loss is 7.535224580242897 with std:9.944497167638957.\n",
      "7.535224580242897 0.07618717702322995\n",
      "The training loss is 5.856428146278391 with std:8.784128537208261. The val loss is 9.39395412924347 with std:24.530600273380184.\n",
      "9.39395412924347 0.07618717702322995\n",
      "Evaluating for {'lmda': 0.0768928372075831} ...\n",
      "The training loss is 6.215057145168912 with std:12.603120492783138. The val loss is 12.807961037943256 with std:43.60588291696751.\n",
      "12.807961037943256 0.0768928372075831\n",
      "The training loss is 6.280194596111154 with std:10.98964373950435. The val loss is 13.301602240329508 with std:54.77938934970953.\n",
      "13.301602240329508 0.0768928372075831\n",
      "The training loss is 6.553165315553711 with std:13.178055199164492. The val loss is 7.533390075080073 with std:9.93351687582578.\n",
      "7.533390075080073 0.0768928372075831\n",
      "The training loss is 5.859764024542979 with std:8.78896385311606. The val loss is 9.390458751237107 with std:24.514349302915427.\n",
      "9.390458751237107 0.0768928372075831\n",
      "Evaluating for {'lmda': 0.07760503335133571} ...\n",
      "The training loss is 6.217264834852367 with std:12.601106905267663. The val loss is 12.61424370636416 with std:41.82999001437392.\n",
      "12.61424370636416 0.07760503335133571\n",
      "The training loss is 6.28138531772985 with std:10.9894272727755. The val loss is 13.314158476187714 with std:54.88183459508787.\n",
      "13.314158476187714 0.07760503335133571\n",
      "The training loss is 6.555217853630358 with std:13.177863488313722. The val loss is 7.531551130991978 with std:9.92253433195866.\n",
      "7.531551130991978 0.07760503335133571\n",
      "The training loss is 5.863115558400643 with std:8.793829347335192. The val loss is 9.387024474037386 with std:24.49820101350512.\n",
      "9.387024474037386 0.07760503335133571\n",
      "Evaluating for {'lmda': 0.07832382599179197} ...\n",
      "The training loss is 6.219478813696133 with std:12.599101105998558. The val loss is 12.427060357234277 with std:40.12073901864679.\n",
      "12.427060357234277 0.07832382599179197\n",
      "The training loss is 6.2825808369057485 with std:10.989215729426459. The val loss is 13.326763907510173 with std:54.984716497518825.\n",
      "13.326763907510173 0.07832382599179197\n",
      "The training loss is 6.5572843331150095 with std:13.177682722551566. The val loss is 7.529707894391851 with std:9.911550445627167.\n",
      "7.529707894391851 0.07832382599179197\n",
      "The training loss is 5.866482751067948 with std:8.79872510367661. The val loss is 9.383651599229212 with std:24.482154925798966.\n",
      "9.383651599229212 0.07832382599179197\n",
      "Evaluating for {'lmda': 0.07904927622696424} ...\n",
      "The training loss is 6.22169913702705 with std:12.59710335798883. The val loss is 12.246251744343613 with std:38.47687532366305.\n",
      "12.246251744343613 0.07904927622696424\n",
      "The training loss is 6.283781170953552 with std:10.989009210545035. The val loss is 13.339418561785248 with std:55.08803586510728.\n",
      "13.339418561785248 0.07904927622696424\n",
      "The training loss is 6.5593648742988435 with std:13.177513177612196. The val loss is 7.527860513351819 with std:9.900566136148818.\n",
      "7.527860513351819 0.07904927622696424\n",
      "The training loss is 5.869865604793894 with std:8.8036512049665. The val loss is 9.380340420709746 with std:24.466210553971543.\n",
      "9.380340420709746 0.07904927622696424\n",
      "Evaluating for {'lmda': 0.07978144572076624} ...\n",
      "The training loss is 6.223925860385685 with std:12.595113925439405. The val loss is 12.07166145147511 with std:36.89718281868832.\n",
      "12.07166145147511 0.07978144572076624\n",
      "The training loss is 6.284986336706222 with std:10.98880781744966. The val loss is 13.352122460693723 with std:55.191793458327076.\n",
      "13.352122460693723 0.07978144572076624\n",
      "The training loss is 6.561459597472367 with std:13.177355131106596. The val loss is 7.526009137603198 with std:9.88958233246099.\n",
      "7.526009137603198 0.07978144572076624\n",
      "The training loss is 5.873264120862221 with std:8.808607733047095. The val loss is 9.377091224651451 with std:24.450367406325345.\n",
      "9.377091224651451 0.07978144572076624\n",
      "Evaluating for {'lmda': 0.08052039670825474} ...\n",
      "The training loss is 6.226159039530594 with std:12.59313307375024. The val loss is 11.903135862373443 with std:35.38048300556139.\n",
      "11.903135862373443 0.08052039670825474\n",
      "The training loss is 6.286196350512921 with std:10.98861165168592. The val loss is 13.364875620062534 with std:55.295989989525204.\n",
      "13.364875620062534 0.08052039670825474\n",
      "The training loss is 6.563568622912949 with std:13.177208862524528. The val loss is 7.524153918522808 with std:9.878599972972212.\n",
      "7.524153918522808 0.08052039670825474\n",
      "The training loss is 5.876678299596344 with std:8.81359476877271. The val loss is 9.373904289481459 with std:24.434624985914432.\n",
      "9.373904289481459 0.08052039670825474\n",
      "Evaluating for {'lmda': 0.08126619200091946} ...\n",
      "The training loss is 6.228398730441851 with std:12.591161069537218. The val loss is 11.740524130729614 with std:33.925633802657885.\n",
      "11.740524130729614 0.08126619200091946\n",
      "The training loss is 6.287411228236743 with std:10.988420815023895. The val loss is 13.37767804980636 with std:55.40062612230204.\n",
      "13.37767804980636 0.08126619200091946\n",
      "The training loss is 6.565692070868751 with std:13.177074653237772. The val loss is 7.52229500913434 with std:9.867620005457235.\n",
      "7.52229500913434 0.08126619200091946\n",
      "The training loss is 5.880108140362457 with std:8.818612392009351. The val loss is 9.37077988584663 with std:24.418982791148693.\n",
      "9.37077988584663 0.08126619200091946\n",
      "Evaluating for {'lmda': 0.08201889499202203} ...\n",
      "The training loss is 6.2306449893255005 with std:12.589198180649934. The val loss is 11.583678150199932 with std:32.531527972417024.\n",
      "11.583678150199932 0.08201889499202203\n",
      "The training loss is 6.2886309852535085 with std:10.98823540945645. The val loss is 13.390529753884831 with std:55.50570247102443.\n",
      "13.390529753884831 0.08201889499202203\n",
      "The training loss is 6.5678300615455605 with std:13.17695278650358. The val loss is 7.520432564099147 with std:9.856643386911925.\n",
      "7.520432564099147 0.08201889499202203\n",
      "The training loss is 5.883553641574602 with std:8.823660681634149. The val loss is 9.367718276589631 with std:24.403440316378866.\n",
      "9.367718276589631 0.08201889499202203\n",
      "Evaluating for {'lmda': 0.08277856966198477} ...\n",
      "The training loss is 6.232897872616851 with std:12.587244676185897. The val loss is 11.432452524426957 with std:31.19709110211424.\n",
      "11.432452524426957 0.08277856966198477\n",
      "The training loss is 6.289855636448808 with std:10.988055537193604. The val loss is 13.403430730246258 with std:55.61121960024804.\n",
      "13.403430730246258 0.08277856966198477\n",
      "The training loss is 6.5699827150892345 with std:13.176843547466174. The val loss is 7.518566739711704 with std:9.84567108342541.\n",
      "7.518566739711704 0.08277856966198477\n",
      "The training loss is 5.8870148006963054 with std:8.828739715530851. The val loss is 9.364719716726931 with std:24.387997052524007.\n",
      "9.364719716726931 0.08277856966198477\n",
      "Evaluating for {'lmda': 0.08354528058382867} ...\n",
      "The training loss is 6.235157436985511 with std:12.585300826507709. The val loss is 11.28670453685478 with std:29.92127906044941.\n",
      "11.28670453685478 0.08354528058382867\n",
      "The training loss is 6.291085196216951 with std:10.987881300660543. The val loss is 13.41638097077937 with std:55.71717802416262.\n",
      "13.41638097077937 0.08354528058382867\n",
      "The training loss is 6.5721501515728145 with std:13.176747223160833. The val loss is 7.516697693886028 with std:9.83470407001499.\n",
      "7.516697693886028 0.08354528058382867\n",
      "The training loss is 5.890491614246092 with std:8.833849570589692. The val loss is 9.361784453424308 with std:24.372652487639918.\n",
      "9.361784453424308 0.08354528058382867\n",
      "Evaluating for {'lmda': 0.08431909292866255} ...\n",
      "The training loss is 6.237423739335672 with std:12.583366903258863. The val loss is 11.146294120852922 with std:28.703074855042747.\n",
      "11.146294120852922 0.08431909292866255\n",
      "The training loss is 6.292319678458613 with std:10.987712802493405. The val loss is 13.42938046126923 with std:55.82357820612974.\n",
      "13.42938046126923 0.08431909292866255\n",
      "The training loss is 6.574332490977557 with std:13.176664102512627. The val loss is 7.514825586154892 with std:9.82374333050117.\n",
      "7.514825586154892 0.08431909292866255\n",
      "The training loss is 5.893984077799966 with std:8.83899032270428. The val loss is 9.358912725976415 with std:24.357406107519303.\n",
      "9.358912725976415 0.08431909292866255\n",
      "Evaluating for {'lmda': 0.08510007247122246} ...\n",
      "The training loss is 6.2396968368132155 with std:12.58144317937951. The val loss is 11.01108382942707 with std:27.54148480462376.\n",
      "11.01108382942707 0.08510007247122246\n",
      "The training loss is 6.293559096579362 with std:10.987550145537407. The val loss is 13.442429181341298 with std:55.93042055805681.\n",
      "13.442429181341298 0.08510007247122246\n",
      "The training loss is 6.576529853179436 with std:13.17659447634026. The val loss is 7.512950577651618 with std:9.812789857321206.\n",
      "7.512950577651618 0.08510007247122246\n",
      "The training loss is 5.897492185996569 with std:8.844162046770599. The val loss is 9.356104765787007 with std:24.342257396254478.\n",
      "9.356104765787007 0.08510007247122246\n",
      "Evaluating for {'lmda': 0.0858882855954625} ...\n",
      "The training loss is 6.241976786805571 with std:12.579529929124332. The val loss is 10.880938805429857 with std:26.435533957576165.\n",
      "10.880938805429857 0.0858882855954625\n",
      "The training loss is 6.294803463487202 with std:10.987393432840506. The val loss is 13.455527104416714 with std:56.03770543991292.\n",
      "13.455527104416714 0.0858882855954625\n",
      "The training loss is 6.5787423579305395 with std:13.176538637357368. The val loss is 7.511072831107608 with std:9.801844651394736.\n",
      "7.511072831107608 0.0858882855954625\n",
      "The training loss is 5.901015932537843 with std:8.849364816681778. The val loss is 9.353360796353774 with std:24.327205836844453.\n",
      "9.353360796353774 0.0858882855954625\n",
      "Evaluating for {'lmda': 0.08668379930019779} ...\n",
      "The training loss is 6.244263646946749 with std:12.577627428077502. The val loss is 10.755726751310569 with std:25.384260682294148.\n",
      "10.755726751310569 0.08668379930019779\n",
      "The training loss is 6.296052791591533 with std:10.987242767652406. The val loss is 13.468674197665065 with std:56.145433159193146.\n",
      "13.468674197665065 0.08668379930019779\n",
      "The training loss is 6.580970124842834 with std:13.17649688017296. The val loss is 7.509192510837165 with std:9.790908721938926.\n",
      "7.509192510837165 0.08668379930019779\n",
      "The training loss is 5.904555310195984 with std:8.854598705327303. The val loss is 9.350681033249074 with std:24.312250911724703.\n",
      "9.350681033249074 0.08668379930019779\n",
      "Evaluating for {'lmda': 0.0874866812047991} ...\n",
      "The training loss is 6.246557475118326 with std:12.57573595316908. The val loss is 10.635317899232971 with std:24.386710387471545.\n",
      "10.635317899232971 0.0874866812047991\n",
      "The training loss is 6.297307092800167 with std:10.9870982534192. The val loss is 13.481870421957096 with std:56.2536039704216.\n",
      "13.481870421957096 0.0874866812047991\n",
      "The training loss is 6.583213273370355 with std:13.176469501292852. The val loss is 7.507309782726027 with std:9.779983086296848.\n",
      "7.507309782726027 0.0874866812047991\n",
      "The training loss is 5.908110310813474 with std:8.859863784588567. The val loss is 9.348065684109327 with std:24.297392103352028.\n",
      "9.348065684109327 0.0874866812047991\n",
      "Evaluating for {'lmda': 0.08829699955494087} ...\n",
      "The training loss is 6.248858329453535 with std:12.573855782692313. The val loss is 10.519584980977237 with std:23.44192834572638.\n",
      "10.519584980977237 0.08829699955494087\n",
      "The training loss is 6.298566378519006 with std:10.986959993780392. The val loss is 13.495115731818066 with std:56.36221807458184.\n",
      "13.495115731818066 0.08829699955494087\n",
      "The training loss is 6.5854719227917595 with std:13.176456799118403. The val loss is 7.505424814220478 with std:9.769068769760507.\n",
      "7.505424814220478 0.08829699955494087\n",
      "The training loss is 5.911680925309591 with std:8.865160125338432. The val loss is 9.345514948619316 with std:24.282628894744242.\n",
      "9.345514948619316 0.08829699955494087\n",
      "Evaluating for {'lmda': 0.08911482322840202} ...\n",
      "The training loss is 6.251166268337707 with std:12.571987196320384. The val loss is 10.408403198120157 with std:22.548951640888465.\n",
      "10.408403198120157 0.08911482322840202\n",
      "The training loss is 6.299830659649403 with std:10.986828092564341. The val loss is 13.508410075384313 with std:56.471275618669274.\n",
      "13.508410075384313 0.08911482322840202\n",
      "The training loss is 6.587746192191881 with std:13.176459073947544. The val loss is 7.50353777431193 with std:9.758166805381594.\n",
      "7.50353777431193 0.08911482322840202\n",
      "The training loss is 5.915267143680463 with std:8.870487797432915. The val loss is 9.343029018504668 with std:24.267960770038897.\n",
      "9.343029018504668 0.08911482322840202\n",
      "Evaluating for {'lmda': 0.08994022174092044} ...\n",
      "The training loss is 6.2534813504119455 with std:12.57013047511957. The val loss is 10.301650191995872 with std:21.70680030126122.\n",
      "10.301650191995872 0.08994022174092044\n",
      "The training loss is 6.301099946585958 with std:10.986702653784914. The val loss is 13.521753394353208 with std:56.58077669511416.\n",
      "13.521753394353208 0.08994022174092044\n",
      "The training loss is 6.590036200443181 with std:13.176476627974786. The val loss is 7.501648833524661 with std:9.747278233781548.\n",
      "7.501648833524661 0.08994022174092044\n",
      "The training loss is 5.918868955005084 with std:8.875846869712591. The val loss is 9.340608077519544 with std:24.253387215011195.\n",
      "9.340608077519544 0.08994022174092044\n",
      "Evaluating for {'lmda': 0.09077326525210229} ...\n",
      "The training loss is 6.255803634573633 with std:12.568285901570581. The val loss is 10.199206013862282 with std:20.914467750637026.\n",
      "10.199206013862282 0.09077326525210229\n",
      "The training loss is 6.30237424921607 with std:10.986583781636025. The val loss is 13.535145623946482 with std:56.69072134135092.\n",
      "13.535145623946482 0.09077326525210229\n",
      "The training loss is 6.5923420661874355 with std:13.176509765289653. The val loss is 7.499758163898954 with std:9.73640410295025.\n",
      "7.499758163898954 0.09077326525210229\n",
      "The training loss is 5.922486347446172 with std:8.881237409994618. The val loss is 9.338252301438597 with std:24.23890771761249.\n",
      "9.338252301438597 0.09077326525210229\n",
      "Evaluating for {'lmda': 0.09161402457138516} ...\n",
      "The training loss is 6.2581331799792155 with std:12.5664537595814. The val loss is 10.100953095124986 with std:20.170910776587974.\n",
      "10.100953095124986 0.09161402457138516\n",
      "The training loss is 6.303653576916998 with std:10.98647158048956. The val loss is 13.54858669286031 with std:56.8011095392757.\n",
      "13.54858669286031 0.09161402457138516\n",
      "The training loss is 6.5946639078154785 with std:13.176558791877257. The val loss is 7.49786593897713 with std:9.725545468046134.\n",
      "7.49786593897713 0.09161402457138516\n",
      "The training loss is 5.926119308253942 with std:8.886659485070345. The val loss is 9.335961858054524 with std:24.22452176848424.\n",
      "9.335961858054524 0.09161402457138516\n",
      "Evaluating for {'lmda': 0.09246257116405739} ...\n",
      "The training loss is 6.260470046044129 with std:12.564634334503065. The val loss is 10.006776217551861 with std:19.47503929591156.\n",
      "10.006776217551861 0.09246257116405739\n",
      "The training loss is 6.3049379385548185 with std:10.986366154889096. The val loss is 13.562076523222002 with std:56.911941214730156.\n",
      "13.562076523222002 0.09246257116405739\n",
      "The training loss is 6.597001843448131 with std:13.176624015613363. The val loss is 7.495972333784721 with std:9.714703391179254.\n",
      "7.495972333784721 0.09246257116405739\n",
      "The training loss is 5.9297678237684845 with std:8.89211316069954. The val loss is 9.333736907169692 with std:24.21022886147345.\n",
      "9.333736907169692 0.09246257116405739\n",
      "Evaluating for {'lmda': 0.0933189771573324} ...\n",
      "The training loss is 6.26281429244525 with std:12.56282791314935. The val loss is 9.916562483536861 with std:18.825706279048728.\n",
      "9.916562483536861 0.0933189771573324\n",
      "The training loss is 6.30622734248258 with std:10.98626760954686. The val loss is 13.57561503055556 with std:57.02321623710159.\n",
      "13.57561503055556 0.0933189771573324\n",
      "The training loss is 6.599355990916691 with std:13.176705746266876. The val loss is 7.494077524814404 with std:9.703878941202008.\n",
      "7.494077524814404 0.0933189771573324\n",
      "The training loss is 5.9334318794245515 with std:8.89759850161002. The val loss is 9.33157760059069 with std:24.196028494109623.\n",
      "9.33157760059069 0.0933189771573324\n",
      "Evaluating for {'lmda': 0.09418331534647952} ...\n",
      "The training loss is 6.265165979120888 with std:12.561034783808179. The val loss is 9.830201286529784 with std:18.2216982703593.\n",
      "9.830201286529784 0.09418331534647952\n",
      "The training loss is 6.307521796539103 with std:10.986176049340264. The val loss is 13.589202123732196 with std:57.13493441875121.\n",
      "13.589202123732196 0.09418331534647952\n",
      "The training loss is 6.601726467741781 with std:13.176804295493405. The val loss is 7.4921816900088745 with std:9.693073193484796.\n",
      "7.4921816900088745 0.09418331534647952\n",
      "The training loss is 5.937111459751447 with std:8.903115571486113. The val loss is 9.329484082133895 with std:24.181920168126258.\n",
      "9.329484082133895 0.09418331534647952\n",
      "Evaluating for {'lmda': 0.09505565920101196} ...\n",
      "The training loss is 6.26752516627159 with std:12.559255236259663. The val loss is 9.747584281444803 with std:17.661726997448696.\n",
      "9.747584281444803 0.09505565920101196\n",
      "The training loss is 6.3088213080463 with std:10.986091579303471. The val loss is 13.602837704935554 with std:57.24709551462365.\n",
      "13.602837704935554 0.09505565920101196\n",
      "The training loss is 6.6041133911126835 with std:13.176919976835993. The val loss is 7.490285008738353 with std:9.682287229682853.\n",
      "7.490285008738353 0.09505565920101196\n",
      "The training loss is 5.940806548376382 with std:8.908664432967564. The val loss is 9.327456487619068 with std:24.1679033899149.\n",
      "9.327456487619068 0.09505565920101196\n",
      "Evaluating for {'lmda': 0.09593608287093142} ...\n",
      "The training loss is 6.2698919143602945 with std:12.55748956179461. The val loss is 9.668605355227568 with std:17.144422597131086.\n",
      "9.668605355227568 0.09593608287093142\n",
      "The training loss is 6.310125883808618 with std:10.98601430462778. The val loss is 13.616521669616702 with std:57.359699221699024.\n",
      "13.616521669616702 0.09593608287093142\n",
      "The training loss is 6.606516877866631 with std:13.177053105720132. The val loss is 7.488387661783556 with std:9.671522137510676.\n",
      "7.488387661783556 0.09593608287093142\n",
      "The training loss is 5.944517128027406 with std:8.914245147642475. The val loss is 9.325494944872574 with std:24.153977671002313.\n",
      "9.325494944872574 0.09593608287093142\n",
      "Evaluating for {'lmda': 0.09682466119303124} ...\n",
      "The training loss is 6.27226628411293 with std:12.555738053224971. The val loss is 9.593160597398501 with std:16.668328977884823.\n",
      "9.593160597398501 0.09682466119303124\n",
      "The training loss is 6.311435530111117 with std:10.985944330652664. The val loss is 13.63025390646025 with std:57.47274517859184.\n",
      "13.63025390646025 0.09682466119303124\n",
      "The training loss is 6.6089370444670825 with std:13.177203999449572. The val loss is 7.486489831312226 with std:9.660779010490934.\n",
      "7.486489831312226 0.09682466119303124\n",
      "The training loss is 5.948243180535947 with std:8.919857776043106. The val loss is 9.323599573727009 with std:24.140142528497137.\n",
      "9.323599573727009 0.09682466119303124\n",
      "Evaluating for {'lmda': 0.09772146969725724} ...\n",
      "The training loss is 6.274648336517865 with std:12.554001004902124. The val loss is 9.521148270830743 with std:16.231901791457467.\n",
      "9.521148270830743 0.09772146969725724\n",
      "The training loss is 6.312750252718081 with std:10.985881762864631. The val loss is 13.644034297349057 with std:57.58623296512302.\n",
      "13.644034297349057 0.09772146969725724\n",
      "The training loss is 6.61137400698251 with std:13.177372977204847. The val loss is 7.484591700859452 with std:9.650058947715108.\n",
      "7.484591700859452 0.09772146969725724\n",
      "The training loss is 5.951984686837515 with std:8.925502377637937. The val loss is 9.321770486030502 with std:24.126397485553834.\n",
      "9.321770486030502 0.09772146969725724\n",
      "Evaluating for {'lmda': 0.0986265846131282} ...\n",
      "The training loss is 6.277038132824945 with std:12.552278712732626. The val loss is 9.452468782511307 with std:15.833509385037326.\n",
      "9.452468782511307 0.0986265846131282\n",
      "The training loss is 6.314070056871104 with std:10.985826706889812. The val loss is 13.657862717315261 with std:57.70016210176537.\n",
      "13.657862717315261 0.0986265846131282\n",
      "The training loss is 6.613827881063204 with std:13.177560360034807. The val loss is 7.482693455305008 with std:9.639363053592115.\n",
      "7.482693455305008 0.0986265846131282\n",
      "The training loss is 5.955741626975706 with std:8.931179010826177. The val loss is 9.320007785647409 with std:24.11274207178648.\n",
      "9.320007785647409 0.0986265846131282\n",
      "Evaluating for {'lmda': 0.09954008287621524} ...\n",
      "The training loss is 6.279435734546183 with std:12.550571474191013. The val loss is 9.38702465441723 with std:15.47143696434839.\n",
      "9.38702465441723 0.09954008287621524\n",
      "The training loss is 6.315394947288003 with std:10.98577926848919. The val loss is 13.671739034520305 with std:57.81453204934857.\n",
      "13.671739034520305 0.09954008287621524\n",
      "The training loss is 6.616298781919513 with std:13.177766470855754. The val loss is 7.480795280847177 with std:9.628692437582718.\n",
      "7.480795280847177 0.09954008287621524\n",
      "The training loss is 5.959513980102426 with std:8.936887732931503. The val loss is 9.318311568464994 with std:24.09917582370659.\n",
      "9.318311568464994 0.09954008287621524\n",
      "Evaluating for {'lmda': 0.10046204213468131} ...\n",
      "The training loss is 6.281841203452815 with std:12.548879588336398. The val loss is 9.324720494564918 with std:15.143894018338814.\n",
      "9.324720494564918 0.10046204213468131\n",
      "The training loss is 6.316724928161425 with std:10.985739553556355. The val loss is 13.685663110204901 with std:57.92934220849571.\n",
      "13.685663110204901 0.10046204213468131\n",
      "The training loss is 6.618786824298678 with std:13.177991634443499. The val loss is 7.478897364982408 with std:9.618048213944164.\n",
      "7.478897364982408 0.10046204213468131\n",
      "The training loss is 5.963301724480377 with std:8.942628600193705. The val loss is 9.316681922405257 with std:24.085698285127176.\n",
      "9.316681922405257 0.10046204213468131\n",
      "Evaluating for {'lmda': 0.10139254075588153} ...\n",
      "The training loss is 6.284254601575337 with std:12.547203355825921. The val loss is 9.265462968112697 with std:14.849024855703167.\n",
      "9.265462968112697 0.10139254075588153\n",
      "The training loss is 6.31806000315715 with std:10.985707668108931. The val loss is 13.69963479866628 with std:58.04459191931132.\n",
      "13.69963479866628 0.10139254075588153\n",
      "The training loss is 6.621292122461835 with std:13.178236177427026. The val loss is 7.476999896475687 with std:9.607431501449947.\n",
      "7.476999896475687 0.10139254075588153\n",
      "The training loss is 5.96710483748457 with std:8.948401667762539. The val loss is 9.315118927428356 with std:24.072309007560605.\n",
      "9.315118927428356 0.10139254075588153\n",
      "Evaluating for {'lmda': 0.10233165783302449} ...\n",
      "The training loss is 6.2866759912019265 with std:12.545543078930677. The val loss is 9.20916076855688 with std:14.584921905643329.\n",
      "9.20916076855688 0.10233165783302449\n",
      "The training loss is 6.319400175413143 with std:10.9856837182852. The val loss is 13.713653947220399 with std:58.16028046090779.\n",
      "13.713653947220399 0.10233165783302449\n",
      "The training loss is 6.62381479016058 with std:13.178500428283956. The val loss is 7.475103065339398 with std:9.596843423126318.\n",
      "7.475103065339398 0.10233165783302449\n",
      "The training loss is 5.9709232956044875 with std:8.954206989690539. The val loss is 9.313622655547595 with std:24.059007550603784.\n",
      "9.313622655547595 0.10233165783302449\n",
      "Evaluating for {'lmda': 0.10327947319189515} ...\n",
      "The training loss is 6.289105434875343 with std:12.543899061546165. The val loss is 9.15572458911185 with std:14.349641258014016.\n",
      "9.15572458911185 0.10327947319189515\n",
      "The training loss is 6.320745447537692 with std:10.98566781033842. The val loss is 13.72772039617084 with std:58.276407051035214.\n",
      "13.72772039617084 0.10327947319189515\n",
      "The training loss is 6.626354940612349 with std:13.178784717331109. The val loss is 7.473207062799983 with std:9.58628510595384.\n",
      "7.473207062799983 0.10327947319189515\n",
      "The training loss is 5.97475707444398 with std:8.960044618923684. The val loss is 9.31219317083902 with std:24.045793482319567.\n",
      "9.31219317083902 0.10327947319189515\n",
      "Evaluating for {'lmda': 0.10423606739764012} ...\n",
      "The training loss is 6.291542995392356 with std:12.542271609210259. The val loss is 9.105067094178736 with std:14.141219781270708.\n",
      "9.105067094178736 0.10423606739764012\n",
      "The training loss is 6.322095821608606 with std:10.985660050632008. The val loss is 13.74183397877526 with std:58.39297084564336.\n",
      "13.74183397877526 0.10423606739764012\n",
      "The training loss is 6.628912686476914 with std:13.17908937671901. The val loss is 7.471312081276626 with std:9.575757680598159.\n",
      "7.471312081276626 0.10423606739764012\n",
      "The training loss is 5.978606148724123 with std:8.965914607294657. The val loss is 9.310830529454492 with std:24.03266637958992.\n",
      "9.310830529454492 0.10423606739764012\n",
      "Evaluating for {'lmda': 0.1052015217616159} ...\n",
      "The training loss is 6.293988735799412 with std:12.540661029113652. The val loss is 9.057102890942838 with std:13.957693076633296.\n",
      "9.057102890942838 0.1052015217616159\n",
      "The training loss is 6.323451299171683 with std:10.98566054563332. The val loss is 13.75599452122124 with std:58.50997093855743.\n",
      "13.75599452122124 0.1052015217616159\n",
      "The training loss is 6.631488139831427 with std:13.179414740421535. The val loss is 7.469418314347773 with std:9.565262281100894.\n",
      "7.469418314347773 0.1052015217616159\n",
      "The training loss is 5.982470492283204 with std:8.97181700551311. The val loss is 9.309534779635543 with std:24.019625828469298.\n",
      "9.309534779635543 0.1052015217616159\n",
      "Evaluating for {'lmda': 0.10617591834830001} ...\n",
      "The training loss is 6.296442719391838 with std:12.539067630112864. The val loss is 9.011748501083106 with std:13.797113507656233.\n",
      "9.011748501083106 0.10617591834830001\n",
      "The training loss is 6.3248118812398975 with std:10.98566940190994. The val loss is 13.770201842591058 with std:58.627406361033536.\n",
      "13.770201842591058 0.10617591834830001\n",
      "The training loss is 6.634081412145871 with std:13.179761144229106. The val loss is 7.467525956722982 with std:9.554800044588362.\n",
      "7.467525956722982 0.10617591834830001\n",
      "The training loss is 5.986350078078381 with std:8.977751863158579. The val loss is 9.308305961729593 with std:24.006671424515897.\n",
      "9.308305961729593 0.10617591834830001\n",
      "Evaluating for {'lmda': 0.10715933998226711} ...\n",
      "The training loss is 6.298905009708796 with std:12.537491722744514. The val loss is 8.968922332697062 with std:13.657567585666762.\n",
      "8.968922332697062 0.10715933998226711\n",
      "The training loss is 6.326177568291761 with std:10.985686726120852. The val loss is 13.78445575484042 with std:58.74527608145773.\n",
      "13.78445575484042 0.10715933998226711\n",
      "The training loss is 6.6366926142573455 with std:13.180128925738241. The val loss is 7.4656352042137275 with std:9.544372110970915.\n",
      "7.4656352042137275 0.10715933998226711\n",
      "The training loss is 5.990244878186232 with std:8.983719228670058. The val loss is 9.307144108205236 with std:23.993802773116634.\n",
      "9.307144108205236 0.10715933998226711\n",
      "Evaluating for {'lmda': 0.10815187025522881} ...\n",
      "The training loss is 6.301375670531252 with std:12.535933619235328. The val loss is 8.928544652241063 with std:13.537192082021173.\n",
      "8.928544652241063 0.10815187025522881\n",
      "The training loss is 6.3275483602704785 with std:10.98571262501391. The val loss is 13.79875606276844 with std:58.86357900496074.\n",
      "13.79875606276844 0.10815187025522881\n",
      "The training loss is 6.639321856345191 with std:13.180518424341285. The val loss is 7.463746253699274 with std:9.533979622620107.\n",
      "7.463746253699274 0.10815187025522881\n",
      "The training loss is 5.994154863803718 with std:8.989719149337358. The val loss is 9.306049243669825 with std:23.981019489797173.\n",
      "9.306049243669825 0.10815187025522881\n",
      "Evaluating for {'lmda': 0.10915359353313911} ...\n",
      "The training loss is 6.303854765877458 with std:12.534393633517007. The val loss is 8.890537556757769 with std:13.434188367498646.\n",
      "8.890537556757769 0.10915359353313911\n",
      "The training loss is 6.328924256582699 with std:10.985747205419134. The val loss is 13.813102563992892 with std:58.98231397307188.\n",
      "13.813102563992892 0.10915359353313911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 6.641969247904307 with std:13.18092998121716. The val loss is 7.461859303097604 with std:9.523623724069544.\n",
      "7.461859303097604 0.10915359353313911\n",
      "The training loss is 5.998080005247674 with std:8.995751671289595. The val loss is 9.305021384889677 with std:23.968321200519558.\n",
      "9.305021384889677 0.10915359353313911\n",
      "Evaluating for {'lmda': 0.1101645949633657} ...\n",
      "The training loss is 6.30634235999879 with std:12.532872081232847. The val loss is 8.854824946121841 with std:13.346834624920763.\n",
      "8.854824946121841 0.1101645949633657\n",
      "The training loss is 6.3303052560980335 with std:10.985790574241848. The val loss is 13.827495048927533 with std:59.101479763383495.\n",
      "13.827495048927533 0.1101645949633657\n",
      "The training loss is 6.64463489771946 with std:13.181363939318354. The val loss is 7.459974551329067 with std:9.513305561680008.\n",
      "7.459974551329067 0.1101645949633657\n",
      "The training loss is 6.002020271956791 with std:9.001816839488763. The val loss is 9.304060540807496 with std:23.955707541961.\n",
      "9.304060540807496 0.1101645949633657\n",
      "Evaluating for {'lmda': 0.1111849604819271} ...\n",
      "The training loss is 6.308838517376461 with std:12.53136927975312. The val loss is 8.821332495532436 with std:13.273495733373775.\n",
      "8.821332495532436 0.1111849604819271\n",
      "The training loss is 6.33169135714739 with std:10.985842838459433. The val loss is 13.84193330075482 with std:59.22107508921067.\n",
      "13.84193330075482 0.1111849604819271\n",
      "The training loss is 6.647318913839239 with std:13.181820643359762. The val loss is 7.458092198284886 with std:9.503026283322097.\n",
      "7.458092198284886 0.1111849604819271\n",
      "The training loss is 6.0059756324914755 with std:9.007914697716362. The val loss is 9.303166712564044 with std:23.943178161782903.\n",
      "9.303166712564044 0.1111849604819271\n",
      "Evaluating for {'lmda': 0.11221477682079803} ...\n",
      "The training loss is 6.311343302715194 with std:12.529885548183744. The val loss is 8.789987628124177 with std:13.212630761880108.\n",
      "8.789987628124177 0.11221477682079803\n",
      "The training loss is 6.333082557522492 with std:10.985904105112386. The val loss is 13.856417095411722 with std:59.341098599320475.\n",
      "13.856417095411722 0.11221477682079803\n",
      "The training loss is 6.650021403548523 with std:13.182300439806587. The val loss is 7.456212444789172 with std:9.492787038037868.\n",
      "7.456212444789172 0.11221477682079803\n",
      "The training loss is 6.009946054532555 with std:9.014045288563157. The val loss is 9.302339893521518 with std:23.93073271889362.\n",
      "9.302339893521518 0.11221477682079803\n",
      "Evaluating for {'lmda': 0.11325413151528115} ...\n",
      "The training loss is 6.313856780939233 with std:12.528421207374922. The val loss is 8.760719487723758 with std:13.162798131033103.\n",
      "8.760719487723758 0.11325413151528115\n",
      "The training loss is 6.334478854475101 with std:10.985974481300296. The val loss is 13.870946201566117 with std:59.46154887761438.\n",
      "13.870946201566117 0.11325413151528115\n",
      "The training loss is 6.652742473341641 with std:13.182803676861015. The val loss is 7.45433549256726 with std:9.482588975714728.\n",
      "7.45433549256726 0.11325413151528115\n",
      "The training loss is 6.013931504883465 with std:9.020208653419967. The val loss is 9.301580069284116 with std:23.91837088368581.\n",
      "9.301580069284116 0.11325413151528115\n",
      "Evaluating for {'lmda': 0.11430311291144786} ...\n",
      "The training loss is 6.316379017186274 with std:12.526976579932208. The val loss is 8.73345891181316 with std:13.122658595568176.\n",
      "8.73345891181316 0.11430311291144786\n",
      "The training loss is 6.335880244715838 with std:10.986054074175476. The val loss is 13.885520380591378 with std:59.582424442769316.\n",
      "13.885520380591378 0.11430311291144786\n",
      "The training loss is 6.655482228895162 with std:13.18333070444704. The val loss is 7.45246154420566 with std:9.472433246735822.\n",
      "7.45246154420566 0.11430311291144786\n",
      "The training loss is 6.01793194946879 with std:9.02640483246499. The val loss is 9.300887217721858 with std:23.906092338260166.\n",
      "9.300887217721858 0.11430311291144786\n",
      "Evaluating for {'lmda': 0.11536181017364786} ...\n",
      "The training loss is 6.31891007680249 with std:12.52555199022504. The val loss is 8.70813840461973 with std:13.090976267295725.\n",
      "8.70813840461973 0.11536181017364786\n",
      "The training loss is 6.337286724413924 with std:10.986142990935841. The val loss is 13.900139386565003 with std:59.70372374811457.\n",
      "13.900139386565003 0.11536181017364786\n",
      "The training loss is 6.658240775040986 with std:13.183881874197466. The val loss is 7.450590803116417 with std:9.462321001637331.\n",
      "7.450590803116417 0.11536181017364786\n",
      "The training loss is 6.021947353333951 with std:9.032633864651965. The val loss is 9.300261308998962 with std:23.893896776656327.\n",
      "9.300261308998962 0.11536181017364786\n",
      "Evaluating for {'lmda': 0.11643031329208768} ...\n",
      "The training loss is 6.321450025335946 with std:12.524147764393534. The val loss is 8.684692110402956 with std:13.066617938595957.\n",
      "8.684692110402956 0.11643031329208768\n",
      "The training loss is 6.338698289196166 with std:10.986241338820305. The val loss is 13.91480296623325 with std:59.825445181179795.\n",
      "13.91480296623325 0.11643031329208768\n",
      "The training loss is 6.661018215736756 with std:13.184457539437183. The val loss is 7.4487234734980445 with std:9.452253390759731.\n",
      "7.4487234734980445 0.11643031329208768\n",
      "The training loss is 6.0259776806465 with std:9.038895787701119. The val loss is 9.29970230559503 with std:23.88178390503823.\n",
      "9.29970230559503 0.11643031329208768\n",
      "Evaluating for {'lmda': 0.11750871309048075} ...\n",
      "The training loss is 6.323998928530396 with std:12.522764230358753. The val loss is 8.663055786908807 with std:13.0485509840301.\n",
      "8.663055786908807 0.11750871309048075\n",
      "The training loss is 6.340114934146639 with std:10.986349225101577. The val loss is 13.929510859009966 with std:59.94758706357186.\n",
      "13.929510859009966 0.11750871309048075\n",
      "The training loss is 6.663814654039588 with std:13.185058055168149. The val loss is 7.446859760295949 with std:9.442231563889374.\n",
      "7.446859760295949 0.11750871309048075\n",
      "The training loss is 6.030022894692519 with std:9.045190638082511. The val loss is 9.299210162333571 with std:23.86975344188707.\n",
      "9.299210162333571 0.11750871309048075\n",
      "Evaluating for {'lmda': 0.11859710123376695} ...\n",
      "The training loss is 6.3265568523176965 with std:12.521401717826278. The val loss is 8.643166778991747 with std:13.035840116798253.\n",
      "8.643166778991747 0.11859710123376695\n",
      "The training loss is 6.341536653805758 with std:10.986466757079667. The val loss is 13.944262796957172 with std:60.070147650675786.\n",
      "13.944262796957172 0.11859710123376695\n",
      "The training loss is 6.666630192076467 with std:13.185683778051672. The val loss is 7.444999869163725 with std:9.432256669901317.\n",
      "7.444999869163725 0.11859710123376695\n",
      "The training loss is 6.034082957878841 with std:9.051518451007107. The val loss is 9.298784826411783 with std:23.857805118185794.\n",
      "9.298784826411783 0.11859710123376695\n",
      "Evaluating for {'lmda': 0.11969557023590428} ...\n",
      "The training loss is 6.329123862812615 with std:12.520060558296064. The val loss is 8.624963992431987 with std:13.027643262283899.\n",
      "8.624963992431987 0.11969557023590428\n",
      "The training loss is 6.342963442170156 with std:10.986594042076288. The val loss is 13.959058504773832 with std:60.193125131437846.\n",
      "13.959058504773832 0.11969557023590428\n",
      "The training loss is 6.669464931015891 with std:13.186335066390733. The val loss is 7.44314400642075 with std:9.422329856396695.\n",
      "7.44314400642075 0.11969557023590428\n",
      "The training loss is 6.0381578317313656 with std:9.057879260413982. The val loss is 9.298426237425572 with std:23.84593867756349.\n",
      "9.298426237425572 0.11969557023590428\n",
      "Evaluating for {'lmda': 0.12080421346773289} ...\n",
      "The training loss is 6.331700026303378 with std:12.51874108506521. The val loss is 8.608387867938003 with std:13.023206787248794.\n",
      "8.608387867938003 0.12080421346773289\n",
      "The training loss is 6.344395292692575 with std:10.98673118742827. The val loss is 13.9738976997886 with std:60.316517628162785.\n",
      "13.9738976997886 0.12080421346773289\n",
      "The training loss is 6.672318971040141 with std:13.187012280112395. The val loss is 7.441292379010895 with std:9.412452269328615.\n",
      "7.441292379010895 0.12080421346773289\n",
      "The training loss is 6.042247476893974 with std:9.064273098955702. The val loss is 9.298134327401112 with std:23.83415387645271.\n",
      "9.298134327401112 0.12080421346773289\n",
      "Evaluating for {'lmda': 0.12192312516491108} ...\n",
      "The training loss is 6.334285409244922 with std:12.517443633235356. The val loss is 8.593380355321464 with std:13.021860294404108.\n",
      "8.593380355321464 0.12192312516491108\n",
      "The training loss is 6.345832198280578 with std:10.986878300480356. The val loss is 13.988780091944589 with std:60.44032319627252.\n",
      "13.988780091944589 0.12192312516491108\n",
      "The training loss is 6.675192411313916 with std:13.187715780748233. The val loss is 7.439445194461608 with std:9.402625052643037.\n",
      "7.439445194461608 0.12192312516491108\n",
      "The training loss is 6.046351853127218 with std:9.070699997985654. The val loss is 9.297909020825385 with std:23.822450484226547.\n",
      "9.297909020825385 0.12192312516491108\n",
      "Evaluating for {'lmda': 0.12305240043592616} ...\n",
      "The training loss is 6.336880078250569 with std:12.516168539715016. The val loss is 8.579884887877052 with std:13.02301116115806.\n",
      "8.579884887877052 0.12305240043592616\n",
      "The training loss is 6.3472741512974995 with std:10.987035488579446. The val loss is 14.003705383797524 with std:60.5645398241658.\n",
      "14.003705383797524 0.12305240043592616\n",
      "The training loss is 6.678085349957209 with std:13.188445931412263. The val loss is 7.437602660838027 with std:9.392849347889035.\n",
      "7.437602660838027 0.12305240043592616\n",
      "The training loss is 6.05047091930796 with std:9.07715998754485. The val loss is 9.2977502346752 with std:23.810828283305916.\n",
      "9.2977502346752 0.12305240043592616\n",
      "Evaluating for {'lmda': 0.12419213527017833} ...\n",
      "The training loss is 6.339484100083655 with std:12.514916143224518. The val loss is 8.567846356949108 with std:13.026138970551061.\n",
      "8.567846356949108 0.12419213527017833\n",
      "The training loss is 6.348721143561064 with std:10.987202859068665. The val loss is 14.018673270501703 with std:60.689165432952706.\n",
      "14.018673270501703 0.12419213527017833\n",
      "The training loss is 6.6809978840145305 with std:13.189203096784189. The val loss is 7.43576498669989 with std:9.383126293848186.\n",
      "7.43576498669989 0.12419213527017833\n",
      "The training loss is 6.054604633427708 with std:9.083653096347938. The val loss is 9.297657878453146 with std:23.79928706928461.\n",
      "9.297657878453146 0.12419213527017833\n",
      "Evaluating for {'lmda': 0.12534242654613995} ...\n",
      "The training loss is 6.34209754164861 with std:12.5136867842983. The val loss is 8.557211086698347 with std:13.030789953465296.\n",
      "8.557211086698347 0.12534242654613995\n",
      "The training loss is 6.350173166344749 with std:10.987380519278414. The val loss is 14.033683439820058 with std:60.81419787644233.\n",
      "14.033683439820058 0.12534242654613995\n",
      "The training loss is 6.683930109426405 with std:13.18998764308353. The val loss is 7.433932381058557 with std:9.373457026153106.\n",
      "7.433932381058557 0.12534242654613995\n",
      "The training loss is 6.058752952591875 with std:9.09017935176989. The val loss is 9.297631854214877 with std:23.78782665099961.\n",
      "9.297631854214877 0.12534242654613995\n",
      "Evaluating for {'lmda': 0.1265033720395904} ...\n",
      "The training loss is 6.344720469981329 with std:12.512480805286975. The val loss is 8.547926809065322 with std:13.036571534986285.\n",
      "8.547926809065322 0.1265033720395904\n",
      "The training loss is 6.3516302103764435 with std:10.987568576523275. The val loss is 14.048735572103995 with std:60.93963494083425.\n",
      "14.048735572103995 0.1265033720395904\n",
      "The training loss is 6.686882120997753 with std:13.190799938049908. The val loss is 7.432105053327626 with std:9.363842676895036.\n",
      "7.432105053327626 0.1265033720395904\n",
      "The training loss is 6.06291583301744 with std:9.096738779829934. The val loss is 9.297672056608427 with std:23.77644685063806.\n",
      "9.297672056608427 0.1265033720395904\n",
      "Evaluating for {'lmda': 0.12767507043192658} ...\n",
      "The training loss is 6.347352952240766 with std:12.511298550357493. The val loss is 8.539942638933036 with std:13.043147054876234.\n",
      "8.539942638933036 0.12767507043192658\n",
      "The training loss is 6.353092265839251 with std:10.987767138091051. The val loss is 14.063829340303874 with std:61.065474344687516.\n",
      "14.063829340303874 0.12767507043192658\n",
      "The training loss is 6.689854012369943 with std:13.191640350917762. The val loss is 7.430283213279883 with std:9.354284374241493.\n",
      "7.430283213279883 0.12767507043192658\n",
      "The training loss is 6.067093230031721 with std:9.103331405178103. The val loss is 9.297778372904174 with std:23.765147503780913.\n",
      "9.297778372904174 0.12767507043192658\n",
      "Evaluating for {'lmda': 0.1288576213185518} ...\n",
      "The training loss is 6.349995055698067 with std:12.510140365496538. The val loss is 8.53320904950893 with std:13.050230712559781.\n",
      "8.53320904950893 0.1288576213185518\n",
      "The training loss is 6.354559322371456 with std:10.98797631124058. The val loss is 14.07896440995868 with std:61.19171373870621.\n",
      "14.07896440995868 0.1288576213185518\n",
      "The training loss is 6.692845875988693 with std:13.19250925239314. The val loss is 7.428467070997827 with std:9.344783242040938.\n",
      "7.428467070997827 0.1288576213185518\n",
      "The training loss is 6.071285098071828 with std:9.109957251080958. The val loss is 9.297950683032958 with std:23.753928459470572.\n",
      "9.297950683032958 0.1288576213185518\n",
      "Evaluating for {'lmda': 0.13005112521734086} ...\n",
      "The training loss is 6.352646847726171 with std:12.50900659850691. The val loss is 8.527677847887574 with std:13.057582770824576.\n",
      "8.527677847887574 0.13005112521734086\n",
      "The training loss is 6.3560313690670736 with std:10.988196203191155. The val loss is 14.094140439211923 with std:61.31835070576711.\n",
      "14.094140439211923 0.13005112521734086\n",
      "The training loss is 6.695857803074718 with std:13.193407014627782. The val loss is 7.426656836826962 with std:9.335340399429843.\n",
      "7.426656836826962 0.13005112521734086\n",
      "The training loss is 6.075491390680635 with std:9.116616339404498. The val loss is 9.298188859620057 with std:23.74278958025268.\n",
      "9.298188859620057 0.13005112521734086\n",
      "Evaluating for {'lmda': 0.13125568357718428} ...\n",
      "The training loss is 6.355308395790887 with std:12.507897599007052. The val loss is 8.523302150852693 with std:13.06500503922839.\n",
      "8.523302150852693 0.13125568357718428\n",
      "The training loss is 6.357508394475803 with std:10.988426921118302. The val loss is 14.109357078800095 with std:61.44538276068894.\n",
      "14.109357078800095 0.13125568357718428\n",
      "The training loss is 6.69888988359346 with std:13.194334011192831. The val loss is 7.424852721326381 with std:9.32595696043504.\n",
      "7.424852721326381 0.13125568357718428\n",
      "The training loss is 6.079712060508203 with std:9.123308690601965. The val loss is 9.298492768023952 with std:23.731730742193314.\n",
      "9.298492768023952 0.13125568357718428\n",
      "Evaluating for {'lmda': 0.13247139878661174} ...\n",
      "The training loss is 6.357979767437809 with std:12.50681371843008. The val loss is 8.520036360872728 with std:13.072336647746425.\n",
      "8.520036360872728 0.13247139878661174\n",
      "The training loss is 6.358990386603859 with std:10.988668572145706. The val loss is 14.124613972067008 with std:61.572807350239785.\n",
      "14.124613972067008 0.13247139878661174\n",
      "The training loss is 6.701942206223621 with std:13.195290617052269. The val loss is 7.423054935219541 with std:9.316634033575808.\n",
      "7.423054935219541 0.13247139878661174\n",
      "The training loss is 6.083947059306586 with std:9.130034323694865. The val loss is 9.298862266371327 with std:23.720751834908064.\n",
      "9.298862266371327 0.13247139878661174\n",
      "Evaluating for {'lmda': 0.13369837418249467} ...\n",
      "The training loss is 6.360661030282868 with std:12.505755310019264. The val loss is 8.517836142315467 with std:13.079450113109369.\n",
      "8.517836142315467 0.13369837418249467\n",
      "The training loss is 6.360477332914167 with std:10.988921263338966. The val loss is 14.139910754966493 with std:61.700621853052816.\n",
      "14.139910754966493 0.13369837418249467\n",
      "The training loss is 6.7050148583272335 with std:13.196277208534465. The val loss is 7.421263689344958 with std:9.307372721461514.\n",
      "7.421263689344958 0.13369837418249467\n",
      "The training loss is 6.088196337930375 with std:9.136793256259288. The val loss is 9.299297205596648 with std:23.709852761553993.\n",
      "9.299297205596648 0.13369837418249467\n",
      "Evaluating for {'lmda': 0.13493671405883065} ...\n",
      "The training loss is 6.363352251999314 with std:12.504722728824953. The val loss is 8.516658397882718 with std:13.086247694121248.\n",
      "8.516658397882718 0.13493671405883065\n",
      "The training loss is 6.361969220327605 with std:10.989185101698162. The val loss is 14.15524705607218 with std:61.828823579569466.\n",
      "14.15524705607218 0.13493671405883065\n",
      "The training loss is 6.708107925918296 with std:13.197294163303328. The val loss is 7.419479194604385 with std:9.29817412039146.\n",
      "7.419479194604385 0.13493671405883065\n",
      "The training loss is 6.092459846333693 with std:9.14358550441003. The val loss is 9.299797429480204 with std:23.69903343882392.\n",
      "9.299797429480204 0.13493671405883065\n",
      "Evaluating for {'lmda': 0.13618652367560813} ...\n",
      "The training loss is 6.3660535003070695 with std:12.503716331698515. The val loss is 8.516461245261635 with std:13.092658027821564.\n",
      "8.516461245261635 0.13618652367560813\n",
      "The training loss is 6.363466035223287 with std:10.989460194151354. The val loss is 14.1706224965883 with std:61.95740977204235.\n",
      "14.1706224965883 0.13618652367560813\n",
      "The training loss is 6.711221493632219 with std:13.198341860328213. The val loss is 7.417701661911492 with std:9.289039319945394.\n",
      "7.417701661911492 0.13618652367560813\n",
      "The training loss is 6.096737533568306 with std:9.150411082783654. The val loss is 9.300362774689631 with std:23.68829379692975.\n",
      "9.300362774689631 0.13618652367560813\n",
      "Evaluating for {'lmda': 0.13744790926775366} ...\n",
      "The training loss is 6.368764842959233 with std:12.502736477287021. The val loss is 8.517203993998871 with std:13.098633035188962.\n",
      "8.517203993998871 0.13744790926775366\n",
      "The training loss is 6.364967763439303 with std:10.989746647548477. The val loss is 14.186036690352106 with std:62.08637760441741.\n",
      "14.186036690352106 0.13744790926775366\n",
      "The training loss is 6.714355644694204 with std:13.199420679852325. The val loss is 7.415931302138846 with std:9.27996940258032.\n",
      "7.415931302138846 0.13744790926775366\n",
      "The training loss is 6.101029347780702 with std:9.157270004521523. The val loss is 9.300993070816636 with std:23.67763377956217.\n",
      "9.300993070816636 0.13744790926775366\n",
      "Evaluating for {'lmda': 0.1387209780541621} ...\n",
      "The training loss is 6.371486347730678 with std:12.501783526025406. The val loss is 8.518847122595727 with std:13.10414508304024.\n",
      "8.518847122595727 0.1387209780541621\n",
      "The training loss is 6.366474390274369 with std:10.99004456865267. The val loss is 14.201489243865359 with std:62.21572418251502.\n",
      "14.201489243865359 0.1387209780541621\n",
      "The training loss is 6.717510460889321 with std:13.200531003362174. The val loss is 7.414168326069319 with std:9.270965443229766.\n",
      "7.414168326069319 0.1387209780541621\n",
      "The training loss is 6.105335236211661 with std:9.164162281255006. The val loss is 9.301688140422126 with std:23.667053343850814.\n",
      "9.301688140422126 0.1387209780541621\n",
      "Evaluating for {'lmda': 0.14000583824680976} ...\n",
      "The training loss is 6.3742180824037415 with std:12.500857840129562. The val loss is 8.521352255835616 with std:13.109184387541623.\n",
      "8.521352255835616 0.14000583824680976\n",
      "The training loss is 6.3679859004884145 with std:10.990354064135731. The val loss is 14.216979756288636 with std:62.345446543841575.\n",
      "14.216979756288636 0.14000583824680976\n",
      "The training loss is 6.720686022530317 with std:13.20167321355288. The val loss is 7.412412944336149 with std:9.262028508882924.\n",
      "7.412412944336149 0.14000583824680976\n",
      "The training loss is 6.10965514519296 with std:9.171087923087889. The val loss is 9.302447799073777 with std:23.656552460295572.\n",
      "9.302447799073777 0.14000583824680976\n",
      "Evaluating for {'lmda': 0.14130259905995338} ...\n",
      "The training loss is 6.37696011475638 with std:12.499959783587222. The val loss is 8.524682142329977 with std:13.113756644163034.\n",
      "8.524682142329977 0.14130259905995338\n",
      "The training loss is 6.36950227830389 with std:10.99067524056873. The val loss is 14.232507819474337 with std:62.475541657783374.\n",
      "14.232507819474337 0.14130259905995338\n",
      "The training loss is 6.723882408426942 with std:13.202847694295484. The val loss is 7.410665367373911 with std:9.253159658186897.\n",
      "7.410665367373911 0.14130259905995338\n",
      "The training loss is 6.113989020144387 with std:9.17804693857813. The val loss is 9.30327185539062 with std:23.646131112713608.\n",
      "9.30327185539062 0.14130259905995338\n",
      "Evaluating for {'lmda': 0.14261137071941282} ...\n",
      "The training loss is 6.3797125125472665 with std:12.499089722145552. The val loss is 8.528800632301516 with std:13.117880868799828.\n",
      "8.528800632301516 0.14261137071941282\n",
      "The training loss is 6.371023507407182 with std:10.991008204416866. The val loss is 14.24807301797495 with std:62.6060064255532.\n",
      "14.24807301797495 0.14261137071941282\n",
      "The training loss is 6.727099695854831 with std:13.20405483060056. The val loss is 7.4089258053625215 with std:9.244359941034578.\n",
      "7.4089258053625215 0.14261137071941282\n",
      "The training loss is 6.118336805573462 with std:9.185039334723918. The val loss is 9.304160111085125 with std:23.635789298141532.\n",
      "9.304160111085125 0.14261137071941282\n",
      "Evaluating for {'lmda': 0.14393226447194066} ...\n",
      "The training loss is 6.382475343502723 with std:12.498248023302251. The val loss is 8.533672655593936 with std:13.121587435048708.\n",
      "8.533672655593936 0.14393226447194066\n",
      "The training loss is 6.372549570950162 with std:10.991353062032056. The val loss is 14.263674929070683 with std:62.73683768032425.\n",
      "14.263674929070683 0.14393226447194066\n",
      "The training loss is 6.730337960524041 with std:13.20529500858419. The val loss is 7.407194468171652 with std:9.235630398157808.\n",
      "7.407194468171652 0.14393226447194066\n",
      "The training loss is 6.122698445071188 with std:9.192065116942963. The val loss is 9.30511236100719 with std:23.625527026759332.\n",
      "9.30511236100719 0.14393226447194066\n",
      "Evaluating for {'lmda': 0.14526539259467813} ...\n",
      "The training loss is 6.385248675302207 with std:12.497435056292735. The val loss is 8.539264199918485 with std:13.124916293103611.\n",
      "8.539264199918485 0.14526539259467813\n",
      "The training loss is 6.374080451551378 with std:10.991709919646036. The val loss is 14.2793131227837 with std:62.86803218724068.\n",
      "14.2793131227837 0.14526539259467813\n",
      "The training loss is 6.733597276548282 with std:13.206568615427683. The val loss is 7.405471565304576 with std:9.226972060711494.\n",
      "7.405471565304576 0.14526539259467813\n",
      "The training loss is 6.127073881311343 with std:9.199124289058126. The val loss is 9.30612839318771 with std:23.615344321775797.\n",
      "9.30612839318771 0.14526539259467813\n",
      "Evaluating for {'lmda': 0.14661086840469845} ...\n",
      "The training loss is 6.388032575563232 with std:12.496651192075399. The val loss is 8.545542289330061 with std:13.127915356419637.\n",
      "8.545542289330061 0.14661086840469845\n",
      "The training loss is 6.375616131298326 with std:10.992078883363934. The val loss is 14.29498716190859 with std:62.999586643577516.\n",
      "14.29498716190859 0.14661086840469845\n",
      "The training loss is 6.736877716413796 with std:13.207876039341935. The val loss is 7.403757305843642 with std:9.218385949875897.\n",
      "7.403757305843642 0.14661086840469845\n",
      "The training loss is 6.131463056046697 with std:9.206216853277628. The val loss is 9.307207988884306 with std:23.605241219324018.\n",
      "9.307207988884306 0.14661086840469845\n",
      "Evaluating for {'lmda': 0.14796880626863962} ...\n",
      "The training loss is 6.390827111827462 with std:12.49589680331748. The val loss is 8.552474962947224 with std:13.130639043078268.\n",
      "8.552474962947224 0.14796880626863962\n",
      "The training loss is 6.377156591748641 with std:10.992460059157201. The val loss is 14.310696602031353 with std:63.131497678788385.\n",
      "14.310696602031353 0.14796880626863962\n",
      "The training loss is 6.740179350947557 with std:13.209217669526833. The val loss is 7.402051898391937 with std:9.209873076442022.\n",
      "7.402051898391937 0.14796880626863962\n",
      "The training loss is 6.13586591010815 with std:9.21334281017981. The val loss is 9.308350922625241 with std:23.5952177683271.\n",
      "9.308350922625241 0.14796880626863962\n",
      "Evaluating for {'lmda': 0.1493393216124252} ...\n",
      "The training loss is 6.393632351544022 with std:12.495172264379748. The val loss is 8.560031253900219 with std:13.133146959585659.\n",
      "8.560031253900219 0.1493393216124252\n",
      "The training loss is 6.378701813932627 with std:10.992853552855351. The val loss is 14.326440991566285 with std:63.26376185473142.\n",
      "14.326440991566285 0.1493393216124252\n",
      "The training loss is 6.743502249287328 with std:13.210593896131154. The val loss is 7.400355551016495 with std:9.201434440407454.\n",
      "7.400355551016495 0.1493393216124252\n",
      "The training loss is 6.140282383401534 with std:9.220502158693662. The val loss is 9.30955696225673 with std:23.5852740303721.\n",
      "9.30955696225673 0.1493393216124252\n",
      "Evaluating for {'lmda': 0.15072253093107554} ...\n",
      "The training loss is 6.396448362055786 with std:12.494477951298263. The val loss is 8.56818116853016 with std:13.135502715719893.\n",
      "8.56818116853016 0.15072253093107554\n",
      "The training loss is 6.3802517783552375 with std:10.993259470142387. The val loss is 14.342219871774494 with std:63.39637566570994.\n",
      "14.342219871774494 0.15072253093107554\n",
      "The training loss is 6.746846478849735 with std:13.212005110210931. The val loss is 7.398668471191279 with std:9.193071030571346.\n",
      "7.398668471191279 0.15072253093107554\n",
      "The training loss is 6.144712414905396 with std:9.227694896081253. The val loss is 9.310825868989484 with std:23.575410079563746.\n",
      "9.310825868989484 0.15072253093107554\n",
      "Evaluating for {'lmda': 0.15211855179861047} ...\n",
      "The training loss is 6.399275210581685 with std:12.493814241767348. The val loss is 8.576895665816307 with std:13.137772859887601.\n",
      "8.576895665816307 0.15211855179861047\n",
      "The training loss is 6.381806464997701 with std:10.993677916547165. The val loss is 14.358032776801062 with std:63.52933553870676.\n",
      "14.358032776801062 0.15211855179861047\n",
      "The training loss is 6.750212105299757 with std:13.213451703686209. The val loss is 7.396990865737538 with std:9.184783824129198.\n",
      "7.396990865737538 0.15211855179861047\n",
      "The training loss is 6.149155942668895 with std:9.234921017920653. The val loss is 9.312157397444826 with std:23.565626002364137.\n",
      "9.312157397444826 0.15211855179861047\n",
      "Evaluating for {'lmda': 0.15352750287804226} ...\n",
      "The training loss is 6.402112964202282 with std:12.493181515118355. The val loss is 8.586146637055835 with std:13.140025925250391.\n",
      "8.586146637055835 0.15352750287804226\n",
      "The training loss is 6.383365853320977 with std:10.994108997437905. The val loss is 14.373879233708465 with std:63.66263783355107.\n",
      "14.373879233708465 0.15352750287804226\n",
      "The training loss is 6.753599192520757 with std:13.214934069298453. The val loss is 7.395322940766533 with std:9.176573786273327.\n",
      "7.395322940766533 0.15352750287804226\n",
      "The training loss is 6.153612903810181 with std:9.242180518088597. The val loss is 9.313551295703817 with std:23.555921897434338.\n",
      "9.313551295703817 0.15352750287804226\n",
      "Evaluating for {'lmda': 0.15494950393146317} ...\n",
      "The training loss is 6.404961689841737 with std:12.492580152300373. The val loss is 8.59590688577894 with std:13.142331577688068.\n",
      "8.59590688577894 0.15494950393146317\n",
      "The training loss is 6.384929922267042 with std:10.994552818015551. The val loss is 14.389758762502584 with std:63.796278843058296.\n",
      "14.389758762502584 0.15494950393146317\n",
      "The training loss is 6.757007802582099 with std:13.216452600564843. The val loss is 7.393664901619973 with std:9.16844186979138.\n",
      "7.393664901619973 0.15494950393146317\n",
      "The training loss is 6.158083234513364 with std:9.249473388741057. The val loss is 9.315007305356248 with std:23.546297875459558.\n",
      "9.315007305356248 0.15494950393146317\n",
      "Evaluating for {'lmda': 0.1563846758302246} ...\n",
      "The training loss is 6.407821454252235 with std:12.492010535856826. The val loss is 8.606150107915504 with std:13.144759857399636.\n",
      "8.606150107915504 0.1563846758302246\n",
      "The training loss is 6.386498650262592 with std:10.995009483306857. The val loss is 14.405670876183883 with std:63.93025479337332.\n",
      "14.405670876183883 0.1563846758302246\n",
      "The training loss is 6.760437995710146 with std:13.218007691731355. The val loss is 7.392016952810836 with std:9.160389014670367.\n",
      "7.392016952810836 0.1563846758302246\n",
      "The training loss is 6.162566870026326 with std:9.256799620295675. The val loss is 9.316525161547208 with std:23.53675405896006.\n",
      "9.316525161547208 0.1563846758302246\n",
      "Evaluating for {'lmda': 0.1578331405652118} ...\n",
      "The training loss is 6.410692323996304 with std:12.491473049903036. The val loss is 8.616850872197245 with std:13.147380506626027.\n",
      "8.616850872197245 0.1578331405652118\n",
      "The training loss is 6.388072015221169 with std:10.995479098157528. The val loss is 14.421615080772348 with std:64.0645618440767.\n",
      "14.421615080772348 0.1578331405652118\n",
      "The training loss is 6.763889830257273 with std:13.21959973772731. The val loss is 7.390379297964122 with std:9.152416147702436.\n",
      "7.390379297964122 0.1578331405652118\n",
      "The training loss is 6.1670637446596155 with std:9.264159201414794. The val loss is 9.318104593029316 with std:23.527290582108666.\n",
      "9.318104593029316 0.1578331405652118\n",
      "Evaluating for {'lmda': 0.1592950212572123} ...\n",
      "The training loss is 6.413574365430016 with std:12.490968080101172. The val loss is 8.627984600819037 with std:13.150262376606957.\n",
      "8.627984600819037 0.1592950212572123\n",
      "The training loss is 6.389649994546117 with std:10.995961767227078. The val loss is 14.437590875349128 with std:64.19919608845318.\n",
      "14.437590875349128 0.1592950212572123\n",
      "The training loss is 6.767363362671783 with std:13.221229134115827. The val loss is 7.388752139755363 with std:9.14452418209123.\n",
      "7.388752139755363 0.1592950212572123\n",
      "The training loss is 6.171573791783467 with std:9.271552118986117. The val loss is 9.319745322213029 with std:23.517907590526754.\n",
      "9.319745322213029 0.1592950212572123\n",
      "Evaluating for {'lmda': 0.16077044216738237} ...\n",
      "The training loss is 6.416467644685054 with std:12.490496013632646. The val loss is 8.639527550335469 with std:13.153472907479287.\n",
      "8.639527550335469 0.16077044216738237\n",
      "The training loss is 6.391232565134267 with std:10.996457594980663. The val loss is 14.45359775210822 with std:64.33415355384605.\n",
      "14.45359775210822 0.16077044216738237\n",
      "The training loss is 6.770858647468002 with std:13.22289627704445. The val loss is 7.387135679852221 with std:9.136714017066337.\n",
      "7.387135679852221 0.16077044216738237\n",
      "The training loss is 6.176096943826588 with std:9.278978358106192. The val loss is 9.321447065213945 with std:23.50860524106293.\n",
      "9.321447065213945 0.16077044216738237\n",
      "Evaluating for {'lmda': 0.16225952870780871} ...\n",
      "The training loss is 6.419372227650562 with std:12.490057239173998. The val loss is 8.651456792809773 with std:13.157077675321183.\n",
      "8.651456792809773 0.16225952870780871\n",
      "The training loss is 6.392819703378482 with std:10.996966685683395. The val loss is 14.469635196390113 with std:64.46943020185353.\n",
      "14.469635196390113 0.16225952870780871\n",
      "The training loss is 6.774375737195851 with std:13.224601563193769. The val loss is 7.38553011885182 with std:9.12898653749628.\n",
      "7.38553011885182 0.16225952870780871\n",
      "The training loss is 6.1806331322739725 with std:9.286437902060852. The val loss is 9.323209531910905 with std:23.499383701596106.\n",
      "9.323209531910905 0.16225952870780871\n",
      "Evaluating for {'lmda': 0.16376240745216872} ...\n",
      "The training loss is 6.422288179956217 with std:12.489652146864694. The val loss is 8.663750197218116 with std:13.161140001055456.\n",
      "8.663750197218116 0.16376240745216872\n",
      "The training loss is 6.394411385171307 with std:10.997489143394441. The val loss is 14.485702686730603 with std:64.60502192865667.\n",
      "14.485702686730603 0.16376240745216872\n",
      "The training loss is 6.777914682412841 with std:13.226345389728015. The val loss is 7.383935656221932 with std:9.12134261351061.\n",
      "7.383935656221932 0.16376240745216872\n",
      "The training loss is 6.185182287665212 with std:9.293930732309203. The val loss is 9.325032425990171 with std:23.49024315079099.\n",
      "9.325032425990171 0.16376240745216872\n",
      "Evaluating for {'lmda': 0.16527920614648955} ...\n",
      "The training loss is 6.425215566952229 with std:12.489281128278082. The val loss is 8.676386411086654 with std:13.165720616314069.\n",
      "8.676386411086654 0.16527920614648955\n",
      "The training loss is 6.396007585908469 with std:10.99802507196047. The val loss is 14.501799694906962 with std:64.74092456532432.\n",
      "14.501799694906962 0.16527920614648955\n",
      "The training loss is 6.781475531652354 with std:13.228128154238187. The val loss is 7.382352490238524 with std:9.113783100121017.\n",
      "7.382352490238524 0.16527920614648955\n",
      "The training loss is 6.18974433959251 with std:9.30145682846297. The val loss is 9.326915445005808 with std:23.48118377788102.\n",
      "9.326915445005808 0.16527920614648955\n",
      "Evaluating for {'lmda': 0.1668100537200059} ...\n",
      "The training loss is 6.428154453692478 with std:12.488944576391347. The val loss is 8.689344842404328 with std:13.170877381779027.\n",
      "8.689344842404328 0.1668100537200059\n",
      "The training loss is 6.397608280492755 with std:10.9985745750092. The val loss is 14.517925685990678 with std:64.87713387819153.\n",
      "14.517925685990678 0.1668100537200059\n",
      "The training loss is 6.785058331397207 with std:13.2299502546913. The val loss is 7.380780817924987 with std:9.106308836851435.\n",
      "7.380780817924987 0.1668100537200059\n",
      "The training loss is 6.19431921670031 with std:9.309016168272548. The val loss is 9.32885828042804 with std:23.472205782413383.\n",
      "9.32885828042804 0.1668100537200059\n",
      "Evaluating for {'lmda': 0.16835508029612023} ...\n",
      "The training loss is 6.4311049049138616 with std:12.48864288554991. The val loss is 8.702605641756282 with std:13.176665053827698.\n",
      "8.702605641756282 0.16835508029612023\n",
      "The training loss is 6.399213443337515 with std:10.999137755943746. The val loss is 14.53408011839351 with std:65.01364556915341.\n",
      "14.53408011839351 0.16835508029612023\n",
      "The training loss is 6.788663126048838 with std:13.23181208937242. The val loss is 7.379220834990392 with std:9.098920647368278.\n",
      "7.379220834990392 0.16835508029612023\n",
      "The training loss is 6.198906846682348 with std:9.316608727605294. The val loss is 9.330860617698137 with std:23.463309374008965.\n",
      "9.330860617698137 0.16835508029612023\n",
      "Evaluating for {'lmda': 0.1699144172034626} ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 6.434066985018737 with std:12.48837645143435. The val loss is 8.716149684729924 with std:13.183135095629556.\n",
      "8.716149684729924 0.1699144172034626\n",
      "The training loss is 6.4008230483709365 with std:10.99971471793602. The val loss is 14.550262443924796 with std:65.15045527609033.\n",
      "14.550262443924796 0.1699144172034626\n",
      "The training loss is 6.792289957899661 with std:13.233714056829603. The val loss is 7.377672735768209 with std:9.09161933912157.\n",
      "7.377672735768209 0.1699144172034626\n",
      "The training loss is 6.203507156281246 with std:9.324234480430245. The val loss is 9.332922136284934 with std:23.454494772107175.\n",
      "9.332922136284934 0.1699144172034626\n",
      "Evaluating for {'lmda': 0.17148819698705392} ...\n",
      "The training loss is 6.437040758054348 with std:12.488145671024817. The val loss is 8.72995855456543 with std:13.190335529105162.\n",
      "8.72995855456543 0.17148819698705392\n",
      "The training loss is 6.402437069040098 with std:11.000305563921758. The val loss is 14.566472107842912 with std:65.28755857321812.\n",
      "14.566472107842912 0.17148819698705392\n",
      "The training loss is 6.795938867104606 with std:13.235656555815712. The val loss is 7.376136713155105 with std:9.084405702987887.\n",
      "7.376136713155105 0.17148819698705392\n",
      "The training loss is 6.208120071287542 with std:9.331893398800704. The val loss is 9.335042509737628 with std:23.445762205698.\n",
      "9.335042509737628 0.17148819698705392\n",
      "Evaluating for {'lmda': 0.1730765534195726} ...\n",
      "The training loss is 6.440026287694772 with std:12.48795094256308. The val loss is 8.744014525056036 with std:13.198310824386647.\n",
      "8.744014525056036 0.1730765534195726\n",
      "The training loss is 6.404055478315349 with std:11.000910396594044. The val loss is 14.582708548912134 with std:65.42495097149809.\n",
      "14.582708548912134 0.1730765534195726\n",
      "The training loss is 6.799609891653355 with std:13.237639985228597. The val loss is 7.374612958547937 with std:9.077280512916197.\n",
      "7.374612958547937 0.1730765534195726\n",
      "The training loss is 6.212745516537793 with std:9.339585452834923. The val loss is 9.337221405744176 with std:23.437111913068318.\n",
      "9.337221405744176 0.1730765534195726\n",
      "Evaluating for {'lmda': 0.17467962151272456} ...\n",
      "The training loss is 6.443023637219519 with std:12.487792665513235. The val loss is 8.758300543704758 with std:13.207101823649971.\n",
      "8.758300543704758 0.17467962151272456\n",
      "The training loss is 6.40567824869471 with std:11.001529318397493. The val loss is 14.598971199462085 with std:65.56262791907731.\n",
      "14.598971199462085 0.17467962151272456\n",
      "The training loss is 6.80330306734349 with std:13.23966474405241. The val loss is 7.373101661782505 with std:9.070244525586537.\n",
      "7.373101661782505 0.17467962151272456\n",
      "The training loss is 6.2173834159149575 with std:9.347310610701225. The val loss is 9.339458486183872 with std:23.4285441415052.\n",
      "9.339458486183872 0.17467962151272456\n",
      "Evaluating for {'lmda': 0.17629753752872057} ...\n",
      "The training loss is 6.446032869494726 with std:12.487671240522433. The val loss is 8.772800215119888 with std:13.216745696356412.\n",
      "8.772800215119888 0.17629753752872057\n",
      "The training loss is 6.407305352208652 with std:11.002162431523503. The val loss is 14.615259485445877 with std:65.7005848016974.\n",
      "14.615259485445877 0.17629753752872057\n",
      "The training loss is 6.8070184277519425 with std:13.24173123129484. The val loss is 7.371603011072729 with std:9.063298480070646.\n",
      "7.371603011072729 0.17629753752872057\n",
      "The training loss is 6.2220336923468755 with std:9.355068838599665. The val loss is 9.341753407190076 with std:23.420059147040316.\n",
      "9.341753407190076 0.17629753752872057\n",
      "Evaluating for {'lmda': 0.17793043899185773} ...\n",
      "The training loss is 6.449054046952499 with std:12.487587069378769. The val loss is 8.787497784690872 with std:13.227275923137249.\n",
      "8.787497784690872 0.17793043899185773\n",
      "The training loss is 6.408936760424661 with std:11.002809837904016. The val loss is 14.6315728265043 with std:65.83881694318782.\n",
      "14.6315728265043 0.17793043899185773\n",
      "The training loss is 6.810756004208954 with std:13.243839845925311. The val loss is 7.370117192946977 with std:9.056443097499407.\n",
      "7.370117192946977 0.17793043899185773\n",
      "The training loss is 6.226696267805024 with std:9.36286010074345. The val loss is 9.344105819202285 with std:23.411657194149488.\n",
      "9.344105819202285 0.17793043899185773\n",
      "Evaluating for {'lmda': 0.1795784647002095} ...\n",
      "The training loss is 6.452087231570917 with std:12.487540554967605. The val loss is 8.802378122478457 with std:13.238722305673933.\n",
      "8.802378122478457 0.1795784647002095\n",
      "The training loss is 6.41057244445281 with std:11.00347163920624. The val loss is 14.6479106360277 with std:65.97731960591167.\n",
      "14.6479106360277 0.1795784647002095\n",
      "The training loss is 6.8145158257722445 with std:13.245990986811767. The val loss is 7.368644392187613 with std:9.049679080737794.\n",
      "7.368644392187613 0.1795784647002095\n",
      "The training loss is 6.231371063306622 with std:9.370684359346688. The val loss is 9.346515367028424 with std:23.40333855546136.\n",
      "9.346515367028424 0.1795784647002095\n",
      "Evaluating for {'lmda': 0.18124175473742377} ...\n",
      "The training loss is 6.455132484853045 with std:12.487532101227208. The val loss is 8.817426707386966 with std:13.251111000106372.\n",
      "8.817426707386966 0.18124175473742377\n",
      "The training loss is 6.41221237494995 with std:11.004147936827815. The val loss is 14.664272321219732 with std:66.11608799124866.\n",
      "14.664272321219732 0.18124175473742377\n",
      "The training loss is 6.818297919200094 with std:13.248185052653762. The val loss is 7.367184791768999 with std:9.043007114067553.\n",
      "7.367184791768999 0.18124175473742377\n",
      "The training loss is 6.236057998911594 with std:9.378541574602648. The val loss is 9.348981689902324 with std:23.395103511472648.\n",
      "9.348981689902324 0.18124175473742377\n",
      "Evaluating for {'lmda': 0.18292045048462938} ...\n",
      "The training loss is 6.45818986780683 with std:12.487562113102078. The val loss is 8.832629611570438 with std:13.26446457159381.\n",
      "8.832629611570438 0.18292045048462938\n",
      "The training loss is 6.413856522126147 with std:11.00483883189113. The val loss is 14.680657283170879 with std:66.25511724015858.\n",
      "14.680657283170879 0.18292045048462938\n",
      "The training loss is 6.822102308926682 with std:13.250422441919358. The val loss is 7.365738572795455 with std:9.036427862875613.\n",
      "7.365738572795455 0.18292045048462938\n",
      "The training loss is 6.240756993725058 with std:9.386431704671395. The val loss is 9.351504421544039 with std:23.386952350236427.\n",
      "9.351504421544039 0.18292045048462938\n",
      "Evaluating for {'lmda': 0.18461469463245475} ...\n",
      "The training loss is 6.461259440923693 with std:12.48763099649555. The val loss is 8.84797348510649 with std:13.278802067809728.\n",
      "8.84797348510649 0.18461469463245475\n",
      "The training loss is 6.415504855749128 with std:11.005544425239338. The val loss is 14.697064916915341 with std:66.39440243359488.\n",
      "14.697064916915341 0.18461469463245475\n",
      "The training loss is 6.825929017035963 with std:13.25270355277511. The val loss is 7.364305914440582 with std:9.029941973353864.\n",
      "7.364305914440582 0.18461469463245475\n",
      "The training loss is 6.245467965895441 with std:9.39435470566106. The val loss is 9.354083190218935 with std:23.37888536706364.\n",
      "9.354083190218935 0.18461469463245475\n",
      "Evaluating for {'lmda': 0.18632463119315598} ...\n",
      "The training loss is 6.464341264157975 with std:12.487739158219975. The val loss is 8.863445540893176 with std:13.29413910920694.\n",
      "8.863445540893176 0.18632463119315598\n",
      "The training loss is 6.417157345150713 with std:11.006264817430178. The val loss is 14.713494611515388 with std:66.5339385931732.\n",
      "14.713494611515388 0.18632463119315598\n",
      "The training loss is 6.829778063238561 with std:13.255028783019382. The val loss is 7.362886993884741 with std:9.023550072200093.\n",
      "7.362886993884741 0.18632463119315598\n",
      "The training loss is 6.250190832616812 with std:9.402310531613415. The val loss is 9.356717618800793 with std:23.37090286420982.\n",
      "9.356717618800793 0.18632463119315598\n",
      "Evaluating for {'lmda': 0.18805040551285815} ...\n",
      "The training loss is 6.46743539690548 with std:12.487887005946233. The val loss is 8.879033539832266 with std:13.310487994088875.\n",
      "8.879033539832266 0.18805040551285815\n",
      "The training loss is 6.418813959232336 with std:11.007000108732239. The val loss is 14.729945750124266 with std:66.67372068164433.\n",
      "14.729945750124266 0.18805040551285815\n",
      "The training loss is 6.833649464846242 with std:13.257398530012726. The val loss is 7.361481986255327 with std:9.017252766336115.\n",
      "7.361481986255327 0.18805040551285815\n",
      "The training loss is 6.254925510128164 with std:9.41029913448765. The val loss is 9.359407324831063 with std:23.363005150561445.\n",
      "9.359407324831063 0.18805040551285815\n",
      "Evaluating for {'lmda': 0.18979216428391013} ...\n",
      "The training loss is 6.470541897982757 with std:12.488074948150649. The val loss is 8.894725776233301 with std:13.327857816515603.\n",
      "8.894725776233301 0.18979216428391013\n",
      "The training loss is 6.420474666470886 with std:11.00775039912069. The val loss is 14.746417710060907 with std:66.81374360347212.\n",
      "14.746417710060907 0.18979216428391013\n",
      "The training loss is 6.837543236749183 with std:13.25981319060602. The val loss is 7.360091064564602 with std:9.011050642626357.\n",
      "7.360091064564602 0.18979216428391013\n",
      "The training loss is 6.259671913715505 with std:9.41832046414578. The val loss is 9.362151920584536 with std:23.355192541327362.\n",
      "9.362151920584536 0.18979216428391013\n",
      "Evaluating for {'lmda': 0.1915500555573528} ...\n",
      "The training loss is 6.473660825604679 with std:12.488303394062159. The val loss is 8.910511063476843 with std:13.34625459526272.\n",
      "8.910511063476843 0.1915500555573528\n",
      "The training loss is 6.422139434925167 with std:11.00851578827122. The val loss is 14.762909862890849 with std:66.95400220547386.\n",
      "14.762909862890849 0.1915500555573528\n",
      "The training loss is 6.841459391392583 with std:13.262273161070514. The val loss is 7.35871439965025 with std:9.004944267611494.\n",
      "7.35871439965025 0.1915500555573528\n",
      "The training loss is 6.264429957711737 with std:9.426374468336226. The val loss is 9.364951013128676 with std:23.347465357709183.\n",
      "9.364951013128676 0.1915500555573528\n",
      "Evaluating for {'lmda': 0.19332422875550453} ...\n",
      "The training loss is 6.476792237364353 with std:12.488572753605313. The val loss is 8.9263787199398 with std:13.365681412122937.\n",
      "8.9263787199398 0.19332422875550453\n",
      "The training loss is 6.423808232242193 with std:11.009296375558781. The val loss is 14.779421574496308 with std:67.0944912773564.\n",
      "14.779421574496308 0.19332422875550453\n",
      "The training loss is 6.845397938754331 with std:13.264778837023238. The val loss is 7.357352160113575 with std:8.998934187244755.\n",
      "7.357352160113575 0.19332422875550453\n",
      "The training loss is 6.269199555499365 with std:9.434461092680799. The val loss is 9.367804204390248 with std:23.339823926578084.\n",
      "9.367804204390248 0.19332422875550453\n",
      "Evaluating for {'lmda': 0.19511483468466165} ...\n",
      "The training loss is 6.479936190210201 with std:12.488883437344695. The val loss is 8.942318555132648 with std:13.386138557871666.\n",
      "8.942318555132648 0.19511483468466165\n",
      "The training loss is 6.425481025663446 with std:11.010092260049863. The val loss is 14.795952205160631 with std:67.23520555239156.\n",
      "14.795952205160631 0.19511483468466165\n",
      "The training loss is 6.849358886323168 with std:13.267330613353622. The val loss is 7.356004512261261 with std:8.993020926645109.\n",
      "7.356004512261261 0.19511483468466165\n",
      "The training loss is 6.273980619511441 with std:9.442580280659726. The val loss is 9.370711091219176 with std:23.332268580158573.\n",
      "9.370711091219176 0.19511483468466165\n",
      "Evaluating for {'lmda': 0.19692202554791716} ...\n",
      "The training loss is 6.4830927404250565 with std:12.489235856424969. The val loss is 8.9583208561364 with std:13.407623684484053.\n",
      "8.9583208561364 0.19692202554791716\n",
      "The training loss is 6.4271577820319035 with std:11.010903540501847. The val loss is 14.812501109646394 with std:67.37613970803555.\n",
      "14.812501109646394 0.19692202554791716\n",
      "The training loss is 6.853342239076636 with std:13.269928884149417. The val loss is 7.354671620044287 with std:8.987204989854003.\n",
      "7.354671620044287 0.19692202554791716\n",
      "The training loss is 6.278773061233068 with std:9.450731973596026. The val loss is 9.373671265452717 with std:23.32479965569034.\n",
      "9.373671265452717 0.19692202554791716\n",
      "Evaluating for {'lmda': 0.19874595495809838} ...\n",
      "The training loss is 6.486261943604463 with std:12.48963042251154. The val loss is 8.974376374229566 with std:13.430131962029698.\n",
      "8.974376374229566 0.19874595495809838\n",
      "The training loss is 6.428838467798525 with std:11.011730315356722. The val loss is 14.829067637277232 with std:67.51728836657327.\n",
      "14.829067637277232 0.19874595495809838\n",
      "The training loss is 6.857347999461547 with std:13.272574042618938. The val loss is 7.353353644998944 with std:8.981486859601834.\n",
      "7.353353644998944 0.19874595495809838\n",
      "The training loss is 6.283576791204765 with std:9.458916110644836. The val loss is 9.376684313982798 with std:23.317417495099473.\n",
      "9.376684313982798 0.19874595495809838\n",
      "Evaluating for {'lmda': 0.2005867779508234} ...\n",
      "The training loss is 6.489443854634178 with std:12.490067547728762. The val loss is 8.990476311804798 with std:13.453656239052851.\n",
      "8.990476311804798 0.2005867779508234\n",
      "The training loss is 6.430523049029846 with std:11.012572682739727. The val loss is 14.845651132023065 with std:67.65864609579862.\n",
      "14.845651132023065 0.2005867779508234\n",
      "The training loss is 6.861376167372396 with std:13.275266481014636. The val loss is 7.352050746188692 with std:8.975866997090742.\n",
      "7.352050746188692 0.2005867779508234\n",
      "The training loss is 6.288391719023961 with std:9.467132628777556. The val loss is 9.379749818821226 with std:23.31012244466725.\n",
      "9.379749818821226 0.2005867779508234\n",
      "Evaluating for {'lmda': 0.2024446509976804} ...\n",
      "The training loss is 6.492638527668957 with std:12.490547644596203. The val loss is 9.006612309497553 with std:13.478187205081127.\n",
      "9.006612309497553 0.2024446509976804\n",
      "The training loss is 6.432211491414408 with std:11.013430740454616. The val loss is 14.862250932580098 with std:67.80020740966228.\n",
      "14.862250932580098 0.2024446509976804\n",
      "The training loss is 6.865426740133258 with std:13.27800659055509. The val loss is 7.350763080145185 with std:8.970345841779945.\n",
      "7.350763080145185 0.2024446509976804\n",
      "The training loss is 6.293217753348079 with std:9.47538146277031. The val loss is 9.382867357168953 with std:23.302914854697125.\n",
      "9.382867357168953 0.2024446509976804\n",
      "Evaluating for {'lmda': 0.20431973201952705} ...\n",
      "The training loss is 6.49584601611053 with std:12.491071125964496. The val loss is 9.022776433587968 with std:13.503713554223044.\n",
      "9.022776433587968 0.20431973201952705\n",
      "The training loss is 6.433903760270701 with std:11.014304585980522. The val loss is 14.878866372466463 with std:67.94196676904174.\n",
      "14.878866372466463 0.20431973201952705\n",
      "The training loss is 6.869499712477565 with std:13.280794761345371. The val loss is 7.349490800810304 with std:8.964923811185603.\n",
      "7.349490800810304 0.20431973201952705\n",
      "The training loss is 6.298054801897534 with std:9.483662545190843. The val loss is 9.386036501482296 with std:23.295795079172528.\n",
      "9.386036501482296 0.20431973201952705\n",
      "Evaluating for {'lmda': 0.20621218039991424} ...\n",
      "The training loss is 6.499066372585228 with std:12.49163840494821. The val loss is 9.038961163617032 with std:13.530222148695177.\n",
      "9.038961163617032 0.20621218039991424\n",
      "The training loss is 6.435599820554606 with std:11.015194316469689. The val loss is 14.89549678010329 with std:68.08391858239062.\n",
      "14.89549678010329 0.20621218039991424\n",
      "The training loss is 6.873595076531114 with std:13.283631382296434. The val loss is 7.348234059478701 with std:8.959601300689721.\n",
      "7.348234059478701 0.20621218039991424\n",
      "The training loss is 6.302902771459252 with std:9.491975806386632. The val loss is 9.389256819542709 with std:23.288763475425107.\n",
      "9.389256819542709 0.20621218039991424\n",
      "Evaluating for {'lmda': 0.20812215699863393} ...\n",
      "The training loss is 6.502299648922397 with std:12.49224989485871. The val loss is 9.055159380261447 with std:13.5576981814049.\n",
      "9.055159380261447 0.20812215699863393\n",
      "The training loss is 6.437299636866862 with std:11.016100028744027. The val loss is 14.912141478903452 with std:68.22605720646176.\n",
      "14.912141478903452 0.20812215699863393\n",
      "The training loss is 6.877712821793487 with std:13.286516841044733. The val loss is 7.346993004742119 with std:8.954378683364467.\n",
      "7.346993004742119 0.20812215699863393\n",
      "The training loss is 6.307761567889739 with std:9.50032117447341. The val loss is 9.392527874526063 with std:23.281820403793873.\n",
      "9.392527874526063 0.20812215699863393\n",
      "Evaluating for {'lmda': 0.21004982416539153} ...\n",
      "The training loss is 6.505545896132515 with std:12.49290600913489. The val loss is 9.071364353431127 with std:13.586125336641462.\n",
      "9.071364353431127 0.21004982416539153\n",
      "The training loss is 6.439003173461315 with std:11.01702181929264. The val loss is 14.92879978737266 with std:68.36837694713553.\n",
      "14.92879978737266 0.21004982416539153\n",
      "The training loss is 6.881852935122353 with std:13.289451523869499. The val loss is 7.34576778243169 with std:8.94925630979755.\n",
      "7.34576778243169 0.21004982416539153\n",
      "The training loss is 6.312631096120122 with std:9.508698575324065. The val loss is 9.39584922507517 with std:23.274966227292673.\n",
      "9.39584922507517 0.21004982416539153\n",
      "Evaluating for {'lmda': 0.2119953457536071} ...\n",
      "The training loss is 6.508805164384728 with std:12.493607161270715. The val loss is 9.08756973062655 with std:13.615485948174452.\n",
      "9.08756973062655 0.2119953457536071\n",
      "The training loss is 6.440710394252505 with std:11.017959784269722. The val loss is 14.945471019188725 with std:68.51087206006187.\n",
      "14.945471019188725 0.2119953457536071\n",
      "The training loss is 6.886015400716566 with std:13.292435815609638. The val loss is 7.344558535562534 with std:8.944234507939084.\n",
      "7.344558535562534 0.2119953457536071\n",
      "The training loss is 6.317511260159151 with std:9.517107932557488. The val loss is 9.399220425366492 with std:23.268201311257837.\n",
      "9.399220425366492 0.2119953457536071\n",
      "Evaluating for {'lmda': 0.21395888713434216} ...\n",
      "The training loss is 6.512077502985289 with std:12.49435376474535. The val loss is 9.10376952551593 with std:13.64576115398207.\n",
      "9.10376952551593 0.21395888713434216\n",
      "The training loss is 6.4424212628242055 with std:11.018914019492248. The val loss is 14.96215448330171 with std:68.65353675148066.\n",
      "14.96215448330171 0.21395888713434216\n",
      "The training loss is 6.890200200100861 with std:13.295470099579767. The val loss is 7.343365404280619 with std:8.939313582956487.\n",
      "7.343365404280619 0.21395888713434216\n",
      "The training loss is 6.322401963098619 with std:9.525549167528752. The val loss is 9.402641025186945 with std:23.26152602302316.\n",
      "9.402641025186945 0.21395888713434216\n",
      "Evaluating for {'lmda': 0.21594061521035654} ...\n",
      "The training loss is 6.515362960355388 with std:12.495146232946503. The val loss is 9.119958106747132 with std:13.67693104701932.\n",
      "9.119958106747132 0.21594061521035654\n",
      "The training loss is 6.444135742437723 with std:11.01988462043748. The val loss is 14.97884948403375 with std:68.7963651790476.\n",
      "14.97884948403375 0.21594061521035654\n",
      "The training loss is 6.894407312111389 with std:13.298554757485029. The val loss is 7.342188525805397 with std:8.934493817094342.\n",
      "7.342188525805397 0.21594061521035654\n",
      "The training loss is 6.3273031071178405 with std:9.534022199318864. The val loss is 9.406110570002657 with std:23.254940731570088.\n",
      "9.406110570002657 0.21594061521035654\n",
      "Evaluating for {'lmda': 0.2179406984302956} ...\n",
      "The training loss is 6.518661584009515 with std:12.495984979097475. The val loss is 9.136130187003547 with std:13.708974821522839.\n",
      "9.136130187003547 0.2179406984302956\n",
      "The training loss is 6.445853796040369 with std:11.02087168224188. The val loss is 14.995555321166373 with std:68.93935145255311.\n",
      "14.995555321166373 0.2179406984302956\n",
      "The training loss is 6.8986367128822765 with std:13.301690169336466. The val loss is 7.341028034379267 with std:8.929775469557516.\n",
      "7.341028034379267 0.2179406984302956\n",
      "The training loss is 6.332214593489279 with std:9.542526944726704. The val loss is 9.40962860103463 with std:23.248445807193168.\n",
      "9.40962860103463 0.2179406984302956\n",
      "Evaluating for {'lmda': 0.2199593068030075} ...\n",
      "The training loss is 6.5219734205333175 with std:12.496870416178412. The val loss is 9.152280812279125 with std:13.74187091432988.\n",
      "9.152280812279125 0.2199593068030075\n",
      "The training loss is 6.447575386274166 with std:11.021875299699317. The val loss is 15.012271290044756 with std:69.08248963478245.\n",
      "15.012271290044756 0.2199593068030075\n",
      "The training loss is 6.902888375831308 with std:13.30487671336202. The val loss is 7.339884061213136 with std:8.925158776397808.\n",
      "7.339884061213136 0.2199593068030075\n",
      "The training loss is 6.337136322583908 with std:9.551063318259123. The val loss is 9.413194655333996 with std:23.242041621169147.\n",
      "9.413194655333996 0.2199593068030075\n",
      "Evaluating for {'lmda': 0.2219966119119955} ...\n",
      "The training loss is 6.525298515561973 with std:12.497802956849185. The val loss is 9.168405351396313 with std:13.7755971408941.\n",
      "9.168405351396313 0.2219966119119955\n",
      "The training loss is 6.449300475484816 with std:11.022895567259914. The val loss is 15.028996681674775 with std:69.2257737423219.\n",
      "15.028996681674775 0.2219966119119955\n",
      "The training loss is 6.90716227164958 with std:13.308114765921665. The val loss is 7.33875673443441 with std:8.92064395041254.\n",
      "7.33875673443441 0.2219966119119955\n",
      "The training loss is 6.342068193877776 with std:9.559631232124588. The val loss is 9.41680826585401 with std:23.23572854539962.\n",
      "9.41680826585401 0.2219966119119955\n",
      "Evaluating for {'lmda': 0.2240527869300018} ...\n",
      "The training loss is 6.528636913758637 with std:12.498783013368792. The val loss is 9.184499485737788 with std:13.810130825606393.\n",
      "9.184499485737788 0.2240527869300018\n",
      "The training loss is 6.451029025730555 with std:11.02393257902811. The val loss is 15.045730782824801 with std:69.3691977464122.\n",
      "15.045730782824801 0.2240527869300018\n",
      "The training loss is 6.91145836828777 with std:13.311404701418128. The val loss is 7.337646179037128 with std:8.916231181062587.\n",
      "7.337646179037128 0.2240527869300018\n",
      "The training loss is 6.347010105956596 with std:9.568230596222312. The val loss is 9.42046896152842 with std:23.22950695209901.\n",
      "9.42046896152842 0.2240527869300018\n",
      "Evaluating for {'lmda': 0.22612800663372773} ...\n",
      "The training loss is 6.53198865879237 with std:12.499810997515324. The val loss is 9.200559199217455 with std:13.84544892623955.\n",
      "9.200559199217455 0.22612800663372773\n",
      "The training loss is 6.452760998791367 with std:11.024986428762936. The val loss is 15.06247287612377 with std:69.51275557375736.\n",
      "15.06247287612377 0.22612800663372773\n",
      "The training loss is 6.915776630947433 with std:13.314746892207491. The val loss is 7.336552516829509 with std:8.911920634389915.\n",
      "7.336552516829509 0.22612800663372773\n",
      "The training loss is 6.351961956524407 with std:9.576861318139382. The val loss is 9.424176267346493 with std:23.223377213440152.\n",
      "9.424176267346493 0.22612800663372773\n",
      "Evaluating for {'lmda': 0.22822244741868986} ...\n",
      "The training loss is 6.535353793317189 with std:12.500887320501667. The val loss is 9.216580768462075 with std:13.881528152253212.\n",
      "9.216580768462075 0.22822244741868986\n",
      "The training loss is 6.454496356178453 with std:11.026057209876972. The val loss is 15.079222240167052 with std:69.65644110741565.\n",
      "15.079222240167052 0.22822244741868986\n",
      "The training loss is 6.920117022069879 with std:13.318141708510943. The val loss is 7.335475866387173 with std:8.907712452962908.\n",
      "7.335475866387173 0.22822244741868986\n",
      "The training loss is 6.356923642409656 with std:9.585523303141832. The val loss is 9.427929704433364 with std:23.21733970123412.\n",
      "9.427929704433364 0.22822244741868986\n",
      "Evaluating for {'lmda': 0.23033628731421313} ...\n",
      "The training loss is 6.538732358949935 with std:12.502012392892983. The val loss is 9.232560753230644 with std:13.918345076901414.\n",
      "9.232560753230644 0.23033628731421313\n",
      "The training loss is 6.456235059143376 with std:11.0271450154351. The val loss is 15.095978149620066 with std:69.80024818766863.\n",
      "15.095978149620066 0.23033628731421313\n",
      "The training loss is 6.924479501327755 with std:13.321589518323105. The val loss is 7.334416343002344 with std:8.903606755819908.\n",
      "7.334416343002344 0.23033628731421313\n",
      "The training loss is 6.361895059572029 with std:9.594216454168937. The val loss is 9.4317287901247 with std:23.211394786595058.\n",
      "9.4317287901247 0.23033628731421313\n",
      "Evaluating for {'lmda': 0.23246970599856479} ...\n",
      "The training loss is 6.542124396250031 with std:12.503186624520811. The val loss is 9.248495987038797 with std:13.955876242974371.\n",
      "9.248495987038797 0.23246970599856479\n",
      "The training loss is 6.457977068688133 with std:11.028249938155872. The val loss is 15.112739875322157 with std:69.94417061289352.\n",
      "15.112739875322157 0.23246970599856479\n",
      "The training loss is 6.928864025616814 with std:13.325090687321834. The val loss is 7.3333740586371245 with std:8.899603638432032.\n",
      "7.3333740586371245 0.23246970599856479\n",
      "The training loss is 6.366876103111075 with std:9.602940671828765. The val loss is 9.435573038047817 with std:23.20554283960452.\n",
      "9.435573038047817 0.23246970599856479\n",
      "Evaluating for {'lmda': 0.23462288481422625} ...\n",
      "The training loss is 6.545529944697439 with std:12.504410424396204. The val loss is 9.26438356801681 with std:13.994098262228967.\n",
      "9.26438356801681 0.23462288481422625\n",
      "The training loss is 6.459722345574594 with std:11.029372070410014. The val loss is 15.129506684392004 with std:70.08820214045305.\n",
      "15.129506684392004 0.23462288481422625\n",
      "The training loss is 6.933270549048835 with std:13.328645578777092. The val loss is 7.332349121877204 with std:8.895703172679372.\n",
      "7.332349121877204 0.23462288481422625\n",
      "The training loss is 6.371866667274294 with std:9.611695854394497. The val loss is 9.439461958201834 with std:23.199784228994147.\n",
      "9.439461958201834 0.23462288481422625\n",
      "Evaluating for {'lmda': 0.23679600678330762} ...\n",
      "The training loss is 6.548949042672125 with std:12.505684200622051. The val loss is 9.280220849962804 with std:14.032987908401301.\n",
      "9.280220849962804 0.23679600678330762\n",
      "The training loss is 6.461470850334631 with std:11.030511504222265. The val loss is 15.146277840336987 with std:70.23233648761489.\n",
      "15.146277840336987 0.23679600678330762\n",
      "The training loss is 6.937699022945004 with std:13.332254553458078. The val loss is 7.331341637885816 with std:8.891905406835903.\n",
      "7.331341637885816 0.23679600678330762\n",
      "The training loss is 6.376866645465122 with std:9.620481897798944. The val loss is 9.44339505703805 with std:23.194119321816284.\n",
      "9.44339505703805 0.23679600678330762\n",
      "Evaluating for {'lmda': 0.23898925662310502} ...\n",
      "The training loss is 6.552381727433002 with std:12.50700836030339. The val loss is 9.296005433628205 with std:14.07252220397091.\n",
      "9.296005433628205 0.23898925662310502\n",
      "The training loss is 6.4632225432797865 with std:11.031668331270263. The val loss is 15.163052603157963 with std:70.37656733244675.\n",
      "15.163052603157963 0.23898925662310502\n",
      "The training loss is 6.94214939583019 with std:13.335917969541137. The val loss is 7.33035170835951 with std:8.888210365569375.\n",
      "7.33035170835951 0.23898925662310502\n",
      "The training loss is 6.381875930251709 with std:9.629298695631567. The val loss is 9.44737183754217 with std:23.188548483125945.\n",
      "9.44737183754217 0.23898925662310502\n",
      "Evaluating for {'lmda': 0.2412028207618007} ...\n",
      "The training loss is 6.555828035097808 with std:12.508383309457024. The val loss is 9.311735158218516 with std:14.112678500711505.\n",
      "9.311735158218516 0.2412028207618007\n",
      "The training loss is 6.464977384512236 with std:11.032842642886722. The val loss is 15.179830229462803 with std:70.52088831477211.\n",
      "15.179830229462803 0.2412028207618007\n",
      "The training loss is 6.946621613429241 with std:13.339636182517088. The val loss is 7.329379431484226 with std:8.884618049952111.\n",
      "7.329379431484226 0.2412028207618007\n",
      "The training loss is 6.386894413376982 with std:9.638146139137499. The val loss is 9.451391799314584 with std:23.18307207565577.\n",
      "9.451391799314584 0.2412028207618007\n",
      "Evaluating for {'lmda': 0.24343688735431104} ...\n",
      "The training loss is 6.559288000621698 with std:12.509809452919756. The val loss is 9.32740809307196 with std:14.153434554034583.\n",
      "9.32740809307196 0.24343688735431104\n",
      "The training loss is 6.466735333934276 with std:11.034034530060138. The val loss is 15.196609972568893 with std:70.66529303704671.\n",
      "15.196609972568893 0.24343688735431104\n",
      "The training loss is 6.951115618662171 with std:13.343409545096373. The val loss is 7.3284249018939684 with std:8.881128437487476.\n",
      "7.3284249018939684 0.24343688735431104\n",
      "The training loss is 6.391921985766708 with std:9.647024117213059. The val loss is 9.45545443865596 with std:23.177690459509233.\n",
      "9.45545443865596 0.24343688735431104\n",
      "Evaluating for {'lmda': 0.245691646298279} ...\n",
      "The training loss is 6.5627616577785295 with std:12.511287194254985. The val loss is 9.343022529583301 with std:14.194768591470737.\n",
      "9.343022529583301 0.245691646298279\n",
      "The training loss is 6.468496351259302 with std:11.035244083435684. The val loss is 15.213391082621524 with std:70.80977506536443.\n",
      "15.213391082621524 0.245691646298279\n",
      "The training loss is 6.955631351642822 with std:13.347238407118024. The val loss is 7.327488210627721 with std:8.87774148214326.\n",
      "7.327488210627721 0.245691646298279\n",
      "The training loss is 6.396958537541722 with std:9.655932516407967. The val loss is 9.459559248651548 with std:23.1724039918416.\n",
      "9.459559248651548 0.245691646298279\n",
      "Evaluating for {'lmda': 0.24796728925021577} ...\n",
      "The training loss is 6.56624903913933 with std:12.512816935659034. The val loss is 9.358576973305691 with std:14.236659375265592.\n",
      "9.358576973305691 0.24796728925021577\n",
      "The training loss is 6.470260396022456 with std:11.03647139331707. The val loss is 15.23017280670035 with std:70.95432793036905.\n",
      "15.23017280670035 0.24796728925021577\n",
      "The training loss is 6.960168749675715 with std:13.351123115451921. The val loss is 7.326569445091147 with std:8.874457114403558.\n",
      "7.326569445091147 0.24796728925021577\n",
      "The training loss is 6.402003958026286 with std:9.664871220922. The val loss is 9.463705719254222 with std:23.16721302655756.\n",
      "9.463705719254222 0.24796728925021577\n",
      "Evaluating for {'lmda': 0.2502640096417919} ...\n",
      "The training loss is 6.569750176053411 with std:12.514399077865084. The val loss is 9.37407013627447 with std:14.279086259403213.\n",
      "9.37407013627447 0.2502640096417919\n",
      "The training loss is 6.472027427591007 with std:11.037716549668648. The val loss is 15.246954388933405 with std:71.09894512822571.\n",
      "15.246954388933405 0.2502640096417919\n",
      "The training loss is 6.964727747256296 with std:13.355064013907043. The val loss is 7.3256686890166 with std:8.87127524132975.\n",
      "7.3256686890166 0.2502640096417919\n",
      "The training loss is 6.407058135759696 with std:9.673840112606403. The val loss is 9.467893337371539 with std:23.16211791400444.\n",
      "9.467893337371539 0.2502640096417919\n",
      "Evaluating for {'lmda': 0.25258200269627845} ...\n",
      "The training loss is 6.573265098628599 with std:12.516034020047043. The val loss is 9.389500929504086 with std:14.322029241108666.\n",
      "9.389500929504086 0.25258200269627845\n",
      "The training loss is 6.473797405175558 with std:11.038979642116727. The val loss is 15.263735070610918 with std:71.2436201216049.\n",
      "15.263735070610918 0.25258200269627845\n",
      "The training loss is 6.9693082760700795 with std:13.359061443135783. The val loss is 7.324786022426382 with std:8.868195746635843.\n",
      "7.324786022426382 0.25258200269627845\n",
      "The training loss is 6.412120958507601 with std:9.682839070965382. The val loss is 9.472121586951936 with std:23.157119000669827.\n",
      "9.472121586951936 0.25258200269627845\n",
      "Evaluating for {'lmda': 0.2549214654451423} ...\n",
      "The training loss is 6.576793835712072 with std:12.517722159721853. The val loss is 9.404868455711922 with std:14.365469007234198.\n",
      "9.404868455711922 0.2549214654451423\n",
      "The training loss is 6.475570287840807 with std:11.040260759952677. The val loss is 15.280514090296272 with std:71.38834634062705.\n",
      "15.280514090296272 0.2549214654451423\n",
      "The training loss is 6.9739102649947435 with std:13.363115740538753. The val loss is 7.323921521594402 with std:8.865218490771111.\n",
      "7.323921521594402 0.2549214654451423\n",
      "The training loss is 6.417192313273329 with std:9.691867973157576. The val loss is 9.476389949072718 with std:23.152216628891786.\n",
      "9.476389949072718 0.2549214654451423\n",
      "Evaluating for {'lmda': 0.2572825967447932} ...\n",
      "The training loss is 6.580336414871142 with std:12.519463892650698. The val loss is 9.4201720022109 with std:14.409386975555599.\n",
      "9.4201720022109 0.2572825967447932\n",
      "The training loss is 6.477346034516817 with std:11.04155999213547. The val loss is 15.297290683943425 with std:71.53311718387178.\n",
      "15.297290683943425 0.2572825967447932\n",
      "The training loss is 6.97853364010137 with std:13.367227240170518. The val loss is 7.323075259013869 with std:8.8623433110264.\n",
      "7.323075259013869 0.2572825967447932\n",
      "The training loss is 6.422272086309665 with std:9.700926693998015. The val loss is 9.48069790202777 with std:23.147411136563328.\n",
      "9.48069790202777 0.2572825967447932\n",
      "Evaluating for {'lmda': 0.259665597293487} ...\n",
      "The training loss is 6.583892862374953 with std:12.521259612739529. The val loss is 9.435411034019968 with std:14.453765331390366.\n",
      "9.435411034019968 0.259665597293487\n",
      "The training loss is 6.479124604009913 with std:11.042877427293895. The val loss is 15.314064085007297 with std:71.67792601934504.\n",
      "15.314064085007297 0.259665597293487\n",
      "The training loss is 6.983178324657544 with std:13.371396272643105. The val loss is 7.322247303360307 with std:8.859570021637175.\n",
      "7.322247303360307 0.259665597293487\n",
      "The training loss is 6.427360163131498 with std:9.710015105962164. The val loss is 9.48504492141573 with std:23.142702856842767.\n",
      "9.48504492141573 0.259665597293487\n",
      "Evaluating for {'lmda': 0.26207066964838527} ...\n",
      "The training loss is 6.587463203175804 with std:12.52310971193835. The val loss is 9.450585187140433 with std:14.498587059601899.\n",
      "9.450585187140433 0.26207066964838527\n",
      "The training loss is 6.4809059550145225 with std:11.044213153730585. The val loss is 15.330833524565017 with std:71.82276618550101.\n",
      "15.330833524565017 0.26207066964838527\n",
      "The training loss is 6.987844239132286 with std:13.375623165032609. The val loss is 7.321437719461234 with std:8.856898413914033.\n",
      "7.321437719461234 0.26207066964838527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 6.432456428528479 with std:9.719133079190113. The val loss is 9.489430480232944 with std:23.138092117881584.\n",
      "9.489430480232944 0.26207066964838527\n",
      "Evaluating for {'lmda': 0.26449801824277197} ...\n",
      "The training loss is 6.591047460890839 with std:12.525014580138983. The val loss is 9.46569426201421 with std:14.543835972283057.\n",
      "9.46569426201421 0.26449801824277197\n",
      "The training loss is 6.482690046124115 with std:11.045567259424653. The val loss is 15.347598231429203 with std:71.96763099225271.\n",
      "15.347598231429203 0.26449801824277197\n",
      "The training loss is 6.9925313011997305 with std:13.379908240782182. The val loss is 7.320646568263755 with std:8.8543282563737.\n",
      "7.320646568263755 0.26449801824277197\n",
      "The training loss is 6.437560766577675 with std:9.728280481490277. The val loss is 9.493854048960761 with std:23.133579242539202.\n",
      "9.493854048960761 0.26449801824277197\n",
      "Evaluating for {'lmda': 0.2669478494034321} ...\n",
      "The training loss is 6.594645657784877 with std:12.526974605073313. The val loss is 9.480738217212412 with std:14.589496732563454.\n",
      "9.480738217212412 0.2669478494034321\n",
      "The training loss is 6.484476835843063 with std:11.04693983203618. The val loss is 15.364357432261597 with std:72.11251372192751.\n",
      "15.364357432261597 0.2669478494034321\n",
      "The training loss is 6.997239425746 with std:13.384251819608432. The val loss is 7.319873906804922 with std:8.851859294888834.\n",
      "7.319873906804922 0.2669478494034321\n",
      "The training loss is 6.442673060657813 with std:9.737457178346732. The val loss is 9.498315095658937 with std:23.12916454811469.\n",
      "9.498315095658937 0.2669478494034321\n",
      "Evaluating for {'lmda': 0.26942037136818825} ...\n",
      "The training loss is 6.598257814752583 with std:12.528990172209918. The val loss is 9.495717163247825 with std:14.635554874382178.\n",
      "9.495717163247825 0.26942037136818825\n",
      "The training loss is 6.486266282598092 with std:11.048330958909103. The val loss is 15.381110351694245 with std:72.25740763034011.\n",
      "15.381110351694245 0.26942037136818825\n",
      "The training loss is 7.001968524876035 with std:13.388654217405048. The val loss is 7.319119788183791 with std:8.849491252848399.\n",
      "7.319119788183791 0.26942037136818825\n",
      "The training loss is 6.447793193462558 with std:9.746663032924605. The val loss is 9.502813086057808 with std:23.124848346082484.\n",
      "9.502813086057808 0.26942037136818825\n",
      "Evaluating for {'lmda': 0.2719157943036019} ...\n",
      "The training loss is 6.601883951301317 with std:12.531061664649092. The val loss is 9.510631356590832 with std:14.681996818777582.\n",
      "9.510631356590832 0.2719157943036019\n",
      "The training loss is 6.488058344750037 with std:11.049740727076424. The val loss is 15.397856212444546 with std:72.40230594778643.\n",
      "15.397856212444546 0.2719157943036019\n",
      "The training loss is 7.006718507921393 with std:13.393115746148455. The val loss is 7.318384261532939 with std:8.847223831325527.\n",
      "7.318384261532939 0.2719157943036019\n",
      "The training loss is 6.452921047015068 with std:9.755897906077363. The val loss is 9.507347483650305 with std:23.120630941826843.\n",
      "9.507347483650305 0.2719157943036019\n",
      "Evaluating for {'lmda': 0.27443433032283654} ...\n",
      "The training loss is 6.605524085534779 with std:12.533189463018786. The val loss is 9.525481193877148 with std:14.728809886893249.\n",
      "9.525481193877148 0.27443433032283654\n",
      "The training loss is 6.489852980605682 with std:11.05116922326423. The val loss is 15.414594235431734 with std:72.54720188005362.\n",
      "15.414594235431734 0.27443433032283654\n",
      "The training loss is 7.011489281449501 with std:13.397636713803742. The val loss is 7.317667371995015 with std:8.845056709266062.\n",
      "7.317667371995015 0.27443433032283654\n",
      "The training loss is 6.458056502682739 with std:9.765161656356085. The val loss is 9.511917749787676 with std:23.116512634394454.\n",
      "9.511917749787676 0.27443433032283654\n",
      "Evaluating for {'lmda': 0.27697619350368907} ...\n",
      "The training loss is 6.609178234136318 with std:12.5353739453679. The val loss is 9.540267206250865 with std:14.775982309724954.\n",
      "9.540267206250865 0.27697619350368907\n",
      "The training loss is 6.491650148429802 with std:11.052616533896789. The val loss is 15.43132363989843 with std:72.69208860949128.\n",
      "15.43132363989843 0.27697619350368907\n",
      "The training loss is 7.016280749273693 with std:13.40221742422932. The val loss is 7.3169691606955904 with std:8.842989543675857.\n",
      "7.3169691606955904 0.27697619350368907\n",
      "The training loss is 6.463199441191958 with std:9.774454140016978. The val loss is 9.516523343773137 with std:23.112493716239314.\n",
      "9.516523343773137 0.27697619350368907\n",
      "Evaluating for {'lmda': 0.2795415999067856} ...\n",
      "The training loss is 6.612846412353445 with std:12.537615487061103. The val loss is 9.55499005392038 with std:14.823503235144642.\n",
      "9.55499005392038 0.2795415999067856\n",
      "The training loss is 6.493449806456893 with std:11.054082745100635. The val loss is 15.448043643524228 with std:72.83695929599173.\n",
      "15.448043643524228 0.2795415999067856\n",
      "The training loss is 7.021092812463775 with std:13.406858177082817. The val loss is 7.316289664723294 with std:8.84102196983331.\n",
      "7.316289664723294 0.2795415999067856\n",
      "The training loss is 6.4683497426436585 with std:9.783775211032088. The val loss is 9.521163722957281 with std:23.10857447298586.\n",
      "9.521163722957281 0.2795415999067856\n",
      "Evaluating for {'lmda': 0.28213076759394706} ...\n",
      "The training loss is 6.616528633981587 with std:12.539914460670587. The val loss is 9.56965052084207 with std:14.871362732099378.\n",
      "9.56965052084207 0.28213076759394706\n",
      "The training loss is 6.495251912903419 with std:11.055567942711294. The val loss is 15.464753462547002 with std:72.98180707806898.\n",
      "15.464753462547002 0.28213076759394706\n",
      "The training loss is 7.025925369358583 with std:13.411559267728435. The val loss is 7.315628917106953 with std:8.839153601502352.\n",
      "7.315628917106953 0.28213076759394706\n",
      "The training loss is 6.473507286529151 with std:9.79312472109962. The val loss is 9.525838342834227 with std:23.10475518318664.\n",
      "9.525838342834227 0.28213076759394706\n",
      "Evaluating for {'lmda': 0.28474391664672477} ...\n",
      "The training loss is 6.620224911349569 with std:12.542271235869299. The val loss is 9.584249509585726 with std:14.919551792408033.\n",
      "9.584249509585726 0.28474391664672477\n",
      "The training loss is 6.497056425979942 with std:11.057072212277706. The val loss is 15.481452311880467 with std:73.12662507386943.\n",
      "15.481452311880467 0.28474391664672477\n",
      "The training loss is 7.030778315579211 with std:13.416320987142958. The val loss is 7.3149869467961 with std:8.837384031161182.\n",
      "7.3149869467961 0.28474391664672477\n",
      "The training loss is 6.478671951746225 with std:9.802502519655631. The val loss is 9.530546657139944 with std:23.10103611809839.\n",
      "9.530546657139944 0.28474391664672477\n",
      "Evaluating for {'lmda': 0.28738126918510665} ...\n",
      "The training loss is 6.6239352553046205 with std:12.54468617932202. The val loss is 9.598788036388447 with std:14.968062330426282.\n",
      "9.598788036388447 0.28738126918510665\n",
      "The training loss is 6.498863303903229 with std:11.058595639068585. The val loss is 15.4981394052303 with std:73.27140638221003.\n",
      "15.4981394052303 0.28738126918510665\n",
      "The training loss is 7.03565154404278 with std:13.42114362182344. The val loss is 7.3143637786431634 with std:8.835712830242112.\n",
      "7.3143637786431634 0.28738126918510665\n",
      "The training loss is 6.483843616616167 with std:9.811908453887153. The val loss is 9.535288117948525 with std:23.097417541453414.\n",
      "9.535288117948525 0.28738126918510665\n",
      "Evaluating for {'lmda': 0.2900430493863992} ...\n",
      "The training loss is 6.627659675198182 with std:12.547159654578056. The val loss is 9.613267226303865 with std:15.016887180376468.\n",
      "9.613267226303865 0.2900430493863992\n",
      "The training loss is 6.500672504908613 with std:11.06013830807846. The val loss is 15.514813955217223 with std:73.41614408364975.\n",
      "15.514813955217223 0.2900430493863992\n",
      "The training loss is 7.040544944977114 with std:13.426027453694195. The val loss is 7.313759433386267 with std:8.834139549378548.\n",
      "7.313759433386267 0.2900430493863992\n",
      "The training loss is 6.489022158899508 with std:9.821342368743043. The val loss is 9.540062175770846 with std:23.09389970924896.\n",
      "9.540062175770846 0.2900430493863992\n",
      "Evaluating for {'lmda': 0.2927294835042816} ...\n",
      "The training loss is 6.6313981788724625 with std:12.549692021960832. The val loss is 9.627688308572104 with std:15.066020092076787.\n",
      "9.627688308572104 0.2927294835042816\n",
      "The training loss is 6.5024839872623215 with std:11.061700304034135. The val loss is 15.531475173491524 with std:73.56083124150406.\n",
      "15.531475173491524 0.2927294835042816\n",
      "The training loss is 7.045458405938042 with std:13.43097276001648. The val loss is 7.313173927634868 with std:8.832663718667545.\n",
      "7.313173927634868 0.2927294835042816\n",
      "The training loss is 6.494207455814682 with std:9.830804106951245. The val loss is 9.544868279654391 with std:23.090482869528273.\n",
      "9.544868279654391 0.2927294835042816\n",
      "Evaluating for {'lmda': 0.295440799888038} ...\n",
      "The training loss is 6.635150772646084 with std:12.552283638458904. The val loss is 9.642052612099778 with std:15.115455724801146.\n",
      "9.642052612099778 0.295440799888038\n",
      "The training loss is 6.504297709273836 with std:11.063281711401043. The val loss is 15.548122270853801 with std:73.7054609029095.\n",
      "15.548122270853801 0.295440799888038\n",
      "The training loss is 7.0503918118254 with std:13.43597981329714. The val loss is 7.31260727385511 with std:8.831284847936281.\n",
      "7.31260727385511 0.295440799888038\n",
      "The training loss is 6.499399384054831 with std:9.840293509031842. The val loss is 9.54970587728383 with std:23.087167262190032.\n",
      "9.54970587728383 0.295440799888038\n",
      "Evaluating for {'lmda': 0.29817722900196736} ...\n",
      "The training loss is 6.638917461302075 with std:12.554934857616102. The val loss is 9.656361561109296 with std:15.165189639685126.\n",
      "9.656361561109296 0.29817722900196736\n",
      "The training loss is 6.506113629308307 with std:11.064882614390255. The val loss is 15.564754457371773 with std:73.85002609985406.\n",
      "15.564754457371773 0.29817722900196736\n",
      "The training loss is 7.05534504490189 with std:13.441048881197927. The val loss is 7.312059480359612 with std:8.830002427025454.\n",
      "7.312059480359612 0.29817722900196736\n",
      "The training loss is 6.5045978198061 with std:9.849810413313481. The val loss is 9.554574415079161 with std:23.083953118779874.\n",
      "9.554574415079161 0.29817722900196736\n",
      "Evaluating for {'lmda': 0.3009390034449721} ...\n",
      "The training loss is 6.642698248075213 with std:12.557646029421926. The val loss is 9.670616670930919 with std:15.215218290756788.\n",
      "9.670616670930919 0.3009390034449721\n",
      "The training loss is 6.507931705799143 with std:11.066503096966198. The val loss is 15.581370942499992 with std:73.99451985024454.\n",
      "15.581370942499992 0.3009390034449721\n",
      "The training loss is 7.060317984811993 with std:13.446180226447652. The val loss is 7.311530551295567 with std:8.82881592607622.\n",
      "7.311530551295567 0.3009390034449721\n",
      "The training loss is 6.509802638765656 with std:9.859354655949831. The val loss is 9.559473338299755 with std:23.080840662311644.\n",
      "9.559473338299755 0.3009390034449721\n",
      "Evaluating for {'lmda': 0.30372635797033115} ...\n",
      "The training loss is 6.64649313464012 with std:12.560417500200847. The val loss is 9.684819543935976 with std:15.265539014739067.\n",
      "9.684819543935976 0.30372635797033115\n",
      "The training loss is 6.509751897260471 with std:11.068143242853456. The val loss is 15.59797093519681 with std:74.1389351589299.\n",
      "15.59797093519681 0.30372635797033115\n",
      "The training loss is 7.065310508601912 with std:13.451374106751928. The val loss is 7.311020486636741 with std:8.827724795829667.\n",
      "7.311020486636741 0.30372635797033115\n",
      "The training loss is 6.51501371616103 with std:9.86892607093829. The val loss is 9.564402091145142 with std:23.077830107080818.\n",
      "9.564402091145142 0.30372635797033115\n",
      "Evaluating for {'lmda': 0.30653952950565266} ...\n",
      "The training loss is 6.650302121099399 with std:12.563249612501851. The val loss is 9.698971865615528 with std:15.316150019806077.\n",
      "9.698971865615528 0.30653952950565266\n",
      "The training loss is 6.511574162299511 with std:11.069803135544934. The val loss is 15.614553644040495 with std:74.28326501874876.\n",
      "15.614553644040495 0.30653952950565266\n",
      "The training loss is 7.070322490741715 with std:13.456630774707811. The val loss is 7.31052928217642 with std:8.826728467935242.\n",
      "7.31052928217642 0.30653952950565266\n",
      "The training loss is 6.520230926768405 with std:9.878524490136964. The val loss is 9.569360116856704 with std:23.0749216584901.\n",
      "9.569360116856704 0.30653952950565266\n",
      "Evaluating for {'lmda': 0.30937875717301366} ...\n",
      "The training loss is 6.65412520597361 with std:12.56614270498842. The val loss is 9.71307540080407 with std:15.367050373434108.\n",
      "9.71307540080407 0.30937875717301366\n",
      "The training loss is 6.5133984596296495 with std:11.071482858309276. The val loss is 15.631118277353004 with std:74.42750241161777.\n",
      "15.631118277353004 0.30937875717301366\n",
      "The training loss is 7.075353803146431 with std:13.461950477716067. The val loss is 7.31005692952193 with std:8.825826355268315.\n",
      "7.31005692952193 0.30937875717301366\n",
      "The training loss is 6.5254541449327705 with std:9.888149743286412. The val loss is 9.574346857823386 with std:23.072115512887734.\n",
      "9.574346857823386 0.30937875717301366\n",
      "Evaluating for {'lmda': 0.3122442823092858} ...\n",
      "The training loss is 6.657962386190352 with std:12.569097112327913. The val loss is 9.727131990010529 with std:15.418239989318476.\n",
      "9.727131990010529 0.3122442823092858\n",
      "The training loss is 6.515224748082458 with std:11.0731824942001. The val loss is 15.647664043306923 with std:74.57164030949103.\n",
      "15.647664043306923 0.3122442823092858\n",
      "The training loss is 7.080404315199573 with std:13.467333457895164. The val loss is 7.309603416090936 with std:8.82501785225504.\n",
      "7.309603416090936 0.3122442823092858\n",
      "The training loss is 6.530683244586539 with std:9.897801658027047. The val loss is 9.579361755682699 with std:23.069411857404706.\n",
      "9.579361755682699 0.3122442823092858\n",
      "Evaluating for {'lmda': 0.31513634848664795} ...\n",
      "The training loss is 6.66181365707397 with std:12.572113165079966. The val loss is 9.741143545916847 with std:15.469719613767925.\n",
      "9.741143545916847 0.31513634848664795\n",
      "The training loss is 6.517052986620914 with std:11.07490212606321. The val loss is 15.664190150054386 with std:74.71567167550667.\n",
      "15.664190150054386 0.31513634848664795\n",
      "The training loss is 7.085473893777971 with std:13.47277995199928. The val loss is 7.309168725109279 with std:8.824302335207582.\n",
      "7.309168725109279 0.31513634848664795\n",
      "The training loss is 6.535918099271116 with std:9.907480059923618. The val loss is 9.584404251427616 with std:23.066810869804666.\n",
      "9.584404251427616 0.31513634848664795\n",
      "Evaluating for {'lmda': 0.318055201533292} ...\n",
      "The training loss is 6.665679012337353 with std:12.575191189587684. The val loss is 9.75511204999366 with std:15.521490811460023.\n",
      "9.75511204999366 0.318055201533292\n",
      "The training loss is 6.518883134351581 with std:11.076641836546706. The val loss is 15.680695805831498 with std:74.85958946492431.\n",
      "15.680695805831498 0.318055201533292\n",
      "The training loss is 7.0905624032758725 with std:13.478290191332446. The val loss is 7.308752835610752 with std:8.823679162666357.\n",
      "7.308752835610752 0.318055201533292\n",
      "The training loss is 6.541158582155813 with std:9.917184772485204. The val loss is 9.589473785509767 with std:23.064312718340293.\n",
      "9.589473785509767 0.318055201533292\n",
      "Evaluating for {'lmda': 0.32100108955431716} ...\n",
      "The training loss is 6.66955844407222 with std:12.578331507866706. The val loss is 9.769039549220816 with std:15.573555950619953.\n",
      "9.769039549220816 0.32100108955431716\n",
      "The training loss is 6.52071515053752 with std:11.078401708108874. The val loss is 15.69718021908409 with std:75.00338662624605.\n",
      "15.69718021908409 0.32100108955431716\n",
      "The training loss is 7.095669705632058 with std:13.483864401669232. The val loss is 7.3083557224378 with std:8.823147675749329.\n",
      "7.3083557224378 0.32100108955431716\n",
      "The training loss is 6.546404566059149 with std:9.926915617188959. The val loss is 9.594569797945939 with std:23.061917561617772.\n",
      "9.594569797945939 0.32100108955431716\n",
      "Evaluating for {'lmda': 0.32397426295281956} ...\n",
      "The training loss is 6.67345194274143 with std:12.58153443749449. The val loss is 9.782928152970582 with std:15.62591818800597.\n",
      "9.782928152970582 0.32397426295281956\n",
      "The training loss is 6.522548994611003 with std:11.080181823028758. The val loss is 15.713642598575918 with std:75.14705610218972.\n",
      "15.713642598575918 0.32397426295281956\n",
      "The training loss is 7.100795660356399 with std:13.489502803172183. The val loss is 7.307977356245314 with std:8.822707198512115.\n",
      "7.307977356245314 0.32397426295281956\n",
      "The training loss is 6.551655923470083 with std:9.93667241350495. The val loss is 9.59969172842493 with std:23.059625548472958.\n",
      "9.59969172842493 0.32397426295281956\n",
      "Evaluating for {'lmda': 0.3269749744511768} ...\n",
      "The training loss is 6.677359497171069 with std:12.58480029150147. The val loss is 9.796780029979837 with std:15.678581453449995.\n",
      "9.796780029979837 0.3269749744511768\n",
      "The training loss is 6.524384626186016 with std:11.081982263413863. The val loss is 15.730082153508958 with std:75.29059083076767.\n",
      "15.730082153508958 0.3269749744511768\n",
      "The training loss is 7.105940124558269 with std:13.495205610314173. The val loss is 7.3076177035044845 with std:8.822357038310729.\n",
      "7.3076177035044845 0.3269749744511768\n",
      "The training loss is 6.556912526568375 with std:9.946454978919204. The val loss is 9.6048390164124 with std:23.057436817846654.\n",
      "9.6048390164124 0.3269749744511768\n",
      "Evaluating for {'lmda': 0.33000347911252853} ...\n",
      "The training loss is 6.6812810945438885 with std:12.58812937826125. The val loss is 9.810597405462286 with std:15.731550434292254.\n",
      "9.810597405462286 0.33000347911252853\n",
      "The training loss is 6.526222005071156 with std:11.083803111212042. The val loss is 15.746498093633813 with std:75.43398374627908.\n",
      "15.746498093633813 0.33000347911252853\n",
      "The training loss is 7.111102952975308 with std:13.50097303180013. The val loss is 7.307276726509586 with std:8.822096486176218.\n",
      "7.307276726509586 0.33000347911252853\n",
      "The training loss is 6.562174247247348 with std:9.956263128961984. The val loss is 9.610011101259545 with std:23.05535149867588.\n",
      "9.610011101259545 0.33000347911252853\n",
      "Evaluating for {'lmda': 0.33306003436245885} ...\n",
      "The training loss is 6.6852167203922095 with std:12.591522001381392. The val loss is 9.82438255830683 with std:15.784830559535655.\n",
      "9.82438255830683 0.33306003436245885\n",
      "The training loss is 6.5280610912820505 with std:11.085644448220146. The val loss is 15.762889629364414 with std:75.57722778034343.\n",
      "15.762889629364414 0.33306003436245885\n",
      "The training loss is 7.116283998003513 with std:13.50680527049037. The val loss is 7.306954383385919 with std:8.821924817191475.\n",
      "7.306954383385919 0.33306003436245885\n",
      "The training loss is 6.567440957134625 with std:9.966096677232514. The val loss is 9.615207422310268 with std:23.053369709791053.\n",
      "9.615207422310268 0.33306003436245885\n",
      "Evaluating for {'lmda': 0.3361449000108765} ...\n",
      "The training loss is 6.689166358592349 with std:12.59497845959474. The val loss is 9.838137818424205 with std:15.838427984113475.\n",
      "9.838137818424205 0.3361449000108765\n",
      "The training loss is 6.529901845054093 with std:11.087506356094401. The val loss is 15.779255971890288 with std:75.72031586291432.\n",
      "15.779255971890288 0.3361449000108765\n",
      "The training loss is 7.121483109728358 with std:13.51270252332655. The val loss is 7.306650628099635 with std:8.821841290879401.\n",
      "7.306650628099635 0.3361449000108765\n",
      "The training loss is 6.572712527615052 with std:9.975955435428109. The val loss is 9.620427419009346 with std:23.051491559820406.\n",
      "9.620427419009346 0.3361449000108765\n",
      "Evaluating for {'lmda': 0.3392583382740992} ...\n",
      "The training loss is 6.6931299913596956 with std:12.598499046651925. The val loss is 9.851865564185676 with std:15.892349573055842.\n",
      "9.851865564185676 0.3392583382740992\n",
      "The training loss is 6.531744226855189 with std:11.089388916361964. The val loss is 15.79559633329044 with std:75.86324092330852.\n",
      "15.79559633329044 0.3392583382740992\n",
      "The training loss is 7.126700135956491 with std:13.518664981258855. The val loss is 7.30636541046866 with std:8.821845151591399.\n",
      "7.30636541046866 0.3392583382740992\n",
      "The training loss is 6.577988829852207 with std:9.985839213370918. The val loss is 9.625670531009433 with std:23.049717147102502.\n",
      "9.625670531009433 0.3392583382740992\n",
      "Evaluating for {'lmda': 0.34240061379714254} ...\n",
      "The training loss is 6.697107599243476 with std:12.602084051213676. The val loss is 9.865568219956497 with std:15.946602885650723.\n",
      "9.865568219956497 0.34240061379714254\n",
      "The training loss is 6.533588197398383 with std:11.091292210430392. The val loss is 15.811909926644402 with std:76.0059958912094.\n",
      "15.811909926644402 0.34240061379714254\n",
      "The training loss is 7.1319349222483055 with std:13.524692829173736. The val loss is 7.3060986761764655 with std:8.821935628906793.\n",
      "7.3060986761764655 0.34240061379714254\n",
      "The training loss is 6.583269734812339 with std:9.995747819040245. The val loss is 9.630936198283713 with std:23.048046559611198.\n",
      "9.630936198283713 0.34240061379714254\n",
      "Evaluating for {'lmda': 0.34557199367621394} ...\n",
      "The training loss is 6.701099161123096 with std:12.605733756743412. The val loss is 9.879248253753746 with std:16.001196159800454.\n",
      "9.879248253753746 0.34557199367621394\n",
      "The training loss is 6.535433717654091 with std:11.093216319599463. The val loss is 15.828195966139885 with std:76.14857369764451.\n",
      "15.828195966139885 0.34557199367621394\n",
      "The training loss is 7.137187311952131 with std:13.530786245825247. The val loss is 7.305850366786393 with std:8.822111938035166.\n",
      "7.305850366786393 0.34557199367621394\n",
      "The training loss is 6.588555113285558 with std:10.005681058599514. The val loss is 9.636223861229919 with std:23.046479874877864.\n",
      "9.636223861229919 0.34557199367621394\n",
      "Evaluating for {'lmda': 0.34877274748141773} ...\n",
      "The training loss is 6.705104654204568 with std:12.609448441401852. The val loss is 9.892908174991389 with std:16.05613829645784.\n",
      "9.892908174991389 0.34877274748141773\n",
      "The training loss is 6.537280748863263 with std:11.095161325072487. The val loss is 15.84445366718511 with std:76.29096727600506.\n",
      "15.84445366718511 0.34877274748141773\n",
      "The training loss is 7.14245714623813 with std:13.536945403766374. The val loss is 7.305620419758797 with std:8.822373280224687.\n",
      "7.305620419758797 0.34877274748141773\n",
      "The training loss is 6.593844835910645 with std:10.01563873643066. The val loss is 9.641532960785488 with std:23.04501715994146.\n",
      "9.641532960785488 0.34877274748141773\n",
      "Evaluating for {'lmda': 0.3520031472796679} ...\n",
      "The training loss is 6.709124054018025 with std:12.61322837794215. The val loss is 9.90655053234138 with std:16.11143884432862.\n",
      "9.90655053234138 0.3520031472796679\n",
      "The training loss is 6.5391292525492135 with std:11.097127307967352. The val loss is 15.86068224651852 with std:76.43316956304515.\n",
      "15.86068224651852 0.3520031472796679\n",
      "The training loss is 7.147744264134439 with std:13.543170469284075. The val loss is 7.305408768468659 with std:8.822718843174673.\n",
      "7.305408768468659 0.3520031472796679\n",
      "The training loss is 6.599138773197371 with std:10.025620655164454. The val loss is 9.646862938532987 with std:23.043658471282914.\n",
      "9.646862938532987 0.3520031472796679\n",
      "Evaluating for {'lmda': 0.3552634676578139} ...\n",
      "The training loss is 6.713157334415072 with std:12.617073833604168. The val loss is 9.920177911657442 with std:16.167107984631055.\n",
      "9.920177911657442 0.3552634676578139\n",
      "The training loss is 6.540979190530582 with std:11.099114349328447. The val loss is 15.876880922311722 with std:76.57517349981772.\n",
      "15.876880922311722 0.3552634676578139\n",
      "The training loss is 7.153048502562833 with std:13.549461602334. The val loss is 7.305215342225659 with std:8.82314780145298.\n",
      "7.305215342225659 0.3552634676578139\n",
      "The training loss is 6.604436795549968 with std:10.035626615713344. The val loss is 9.652213236812976 with std:23.04240385479057.\n",
      "9.652213236812976 0.3552634676578139\n",
      "Evaluating for {'lmda': 0.3585539857459817} ...\n",
      "The training loss is 6.717204467567728 with std:12.620985070012287. The val loss is 9.933792934035631 with std:16.223156516328363.\n",
      "9.933792934035631 0.3585539857459817\n",
      "The training loss is 6.542830524933573 with std:11.101122530138632. The val loss is 15.893048914282843 with std:76.71697203271151.\n",
      "15.893048914282843 0.3585539857459817\n",
      "The training loss is 7.158369696376367 with std:13.555818956479646. The val loss is 7.305040066295883 with std:8.823659316918032.\n",
      "7.305040066295883 0.3585539857459817\n",
      "The training loss is 6.6097387732917285 with std:10.045656417306779. The val loss is 9.657583298832995 with std:23.041253345712718.\n",
      "9.657583298832995 0.3585539857459817\n",
      "Evaluating for {'lmda': 0.36187498124112805} ...\n",
      "The training loss is 6.721265423967353 with std:12.624962343072394. The val loss is 9.947398253922358 with std:16.27959584141743.\n",
      "9.947398253922358 0.36187498124112805\n",
      "The training loss is 6.544683218204341 with std:11.103151931331633. The val loss is 15.909185443795312 with std:76.85855811435283.\n",
      "15.909185443795312 0.36187498124112805\n",
      "The training loss is 7.163707678397058 with std:13.562242678831398. The val loss is 7.304882861925342 with std:8.82425253914524.\n",
      "7.304882861925342 0.36187498124112805\n",
      "The training loss is 6.615044576688618 with std:10.055709857525384. The val loss is 9.662972568780104 with std:23.04020696864255.\n",
      "9.662972568780104 0.36187498124112805\n",
      "Evaluating for {'lmda': 0.36522673643081754} ...\n",
      "The training loss is 6.725340172424633 with std:12.62900590287086. The val loss is 9.96099655734347 with std:16.33643795069327.\n",
      "9.96099655734347 0.36522673643081754\n",
      "The training loss is 6.546537233121388 with std:11.10520263380425. The val loss is 15.925289733968752 with std:76.99992470462196.\n",
      "15.925289733968752 0.36522673643081754\n",
      "The training loss is 7.169062279455118 with std:13.568732909989446. The val loss is 7.304743646364351 with std:8.824926605855193.\n",
      "7.304743646364351 0.36522673643081754\n",
      "The training loss is 6.620354075973225 with std:10.065786732336301. The val loss is 9.668380491930618 with std:23.039264737488995.\n",
      "9.668380491930618 0.36522673643081754\n",
      "Evaluating for {'lmda': 0.36860953621721576} ...\n",
      "The training loss is 6.72942868006974 with std:12.633115993573561. The val loss is 9.974590560193393 with std:16.393695409699873.\n",
      "9.974590560193393 0.36860953621721576\n",
      "The training loss is 6.548392532807677 with std:11.10727471842876. The val loss is 15.941361009778518 with std:77.14106477156919.\n",
      "15.941361009778518 0.36860953621721576\n",
      "The training loss is 7.174433328427903 with std:13.575289783986724. The val loss is 7.304622332894928 with std:8.825680643347166.\n",
      "7.304622332894928 0.36860953621721576\n",
      "The training loss is 6.625667141369232 with std:10.075886836129747. The val loss is 9.673806514760354 with std:23.03842665546567.\n",
      "9.673806514760354 0.36860953621721576\n",
      "Evaluating for {'lmda': 0.3720236681413066} ...\n",
      "The training loss is 6.733530912353533 with std:12.637292853327727. The val loss is 9.988183006630043 with std:16.451381345155255.\n",
      "9.988183006630043 0.3720236681413066\n",
      "The training loss is 6.55024908074286 with std:11.109368266066058. The val loss is 15.957398498159861 with std:77.28197129236881.\n",
      "15.957398498159861 0.3720236681413066\n",
      "The training loss is 7.179820652280989 with std:13.581913428236456. The val loss is 7.304518830858211 with std:8.826513766933724.\n",
      "7.304518830858211 0.3720236681413066\n",
      "The training loss is 6.630983643116385 with std:10.086009961757364. The val loss is 9.679250085057834 with std:23.037692715093574.\n",
      "9.679250085057834 0.3720236681413066\n",
      "Evaluating for {'lmda': 0.37546942240733366} ...\n",
      "The training loss is 6.737646833049506 with std:12.64153671416351. The val loss is 10.001776667524647 with std:16.50950943157278.\n",
      "10.001776667524647 0.37546942240733366\n",
      "The training loss is 6.55210684077555 with std:11.111483357578608. The val loss is 15.973401428108385 with std:77.42263725424661.\n",
      "15.973401428108385 0.37546942240733366\n",
      "The training loss is 7.1852240761088275 with std:13.588603963480116. The val loss is 7.304433045685291 with std:8.827425081380628.\n",
      "7.304433045685291 0.37546942240733366\n",
      "The training loss is 6.6363034514945785 with std:10.09615590056877. The val loss is 9.684710652034427 with std:23.03706289820201.\n",
      "9.684710652034427 0.37546942240733366\n",
      "Evaluating for {'lmda': 0.3789470919074668} ...\n",
      "The training loss is 6.741776404256375 with std:12.645847801898721. The val loss is 10.015374339014523 with std:16.568093878400987.\n",
      "10.015374339014523 0.3789470919074668\n",
      "The training loss is 6.5539657771352084 with std:11.113620073844235. The val loss is 15.989369030779425 with std:77.5630556553966.\n",
      "15.989369030779425 0.3789470919074668\n",
      "The training loss is 7.190643423177392 with std:13.595361503738543. The val loss is 7.304364878928161 with std:8.828413681346166.\n",
      "7.304364878928161 0.3789470919074668\n",
      "The training loss is 6.641626436849332 with std:10.10632444245286. The val loss is 9.690187666437183 with std:23.036537175946524.\n",
      "9.690187666437183 0.3789470919074668\n",
      "Evaluating for {'lmda': 0.3824569722466999} ...\n",
      "The training loss is 6.745919586400175 with std:12.65022633604163. The val loss is 10.028978841108943 with std:16.627149417405754.\n",
      "10.028978841108943 0.3824569722466999\n",
      "The training loss is 6.555825854444003 with std:11.11577849576829. The val loss is 16.005300539588383 with std:77.70321950591308.\n",
      "16.005300539588383 0.3824569722466999\n",
      "The training loss is 7.196078514965856 with std:13.602186156265024. The val loss is 7.304314228293018 with std:8.829478651824465.\n",
      "7.304314228293018 0.3824569722466999\n",
      "The training loss is 6.6469524696162265 with std:10.116515375875368. The val loss is 9.695680580658195 with std:23.03611550882661.\n",
      "9.695680580658195 0.3824569722466999\n",
      "Evaluating for {'lmda': 0.3859993617679767} ...\n",
      "The training loss is 6.750076338239651 with std:12.654672529700104. The val loss is 10.042593016378687 with std:16.686691290488312.\n",
      "10.042593016378687 0.3859993617679767\n",
      "The training loss is 6.557687037729116 with std:11.117958704298637. The val loss is 16.02119519030824 with std:77.84312182868788.\n",
      "16.02119519030824 0.3859993617679767\n",
      "The training loss is 7.2015291712108205 with std:13.609078021500965. The val loss is 7.3042809876756625 with std:8.830619068589879.\n",
      "7.3042809876756625 0.3859993617679767\n",
      "The training loss is 6.652281420346601 with std:10.126728487921698. The val loss is 9.701188848847696 with std:23.03579784671757.\n",
      "9.701188848847696 0.3859993617679767\n",
      "Evaluating for {'lmda': 0.3895745615775501} ...\n",
      "The training loss is 6.754246616869432 with std:12.659186589487875. The val loss is 10.056219728714959 with std:16.746735237881413.\n",
      "10.056219728714959 0.3895745615775501\n",
      "The training loss is 6.559549292434089 with std:11.120160780439093. The val loss is 16.037052221162913 with std:77.9827556602836.\n",
      "16.037052221162913 0.3895745615775501\n",
      "The training loss is 7.206995209950448 with std:13.616037193034327. The val loss is 7.304265047197133 with std:8.831833998643482.\n",
      "7.304265047197133 0.3895745615775501\n",
      "The training loss is 6.657613159732721 with std:10.136963564337615. The val loss is 9.706711927025971 with std:23.035584128912532.\n",
      "9.706711927025971 0.3895745615775501\n",
      "Evaluating for {'lmda': 0.393182875570577} ...\n",
      "The training loss is 6.758430377725519 with std:12.663768715434724. The val loss is 10.069861862155882 with std:16.80729748672396.\n",
      "10.069861862155882 0.393182875570577\n",
      "The training loss is 6.561412584430744 with std:11.122384805263163. The val loss is 16.052870872925066 with std:78.12211405183884.\n",
      "16.052870872925066 0.393182875570577\n",
      "The training loss is 7.212476447568488 with std:13.623063757559214. The val loss is 7.3042662932422004 with std:8.833122500658911.\n",
      "7.3042662932422004 0.393182875570577\n",
      "The training loss is 6.662947558632636 with std:10.147220389571055. The val loss is 9.71224927319415 with std:23.035474284167034.\n",
      "9.71224927319415 0.393182875570577\n",
      "Evaluating for {'lmda': 0.39682461045694817} ...\n",
      "The training loss is 6.762627574591447 with std:12.668419100897903. The val loss is 10.083522319771996 with std:16.868394739976655.\n",
      "10.083522319771996 0.39682461045694817\n",
      "The training loss is 6.563276880030837 with std:11.124630859928835. The val loss is 16.068650389008717 with std:78.26119006993822.\n",
      "16.068650389008717 0.39682461045694817\n",
      "The training loss is 7.217972698840746 with std:13.630157794840306. The val loss is 7.304284608498864 with std:8.83448362543186.\n",
      "7.304284608498864 0.39682461045694817\n",
      "The training loss is 6.668284488096489 with std:10.157498746816467. The val loss is 9.717800347445634 with std:23.035468230753793.\n",
      "9.717800347445634 0.39682461045694817\n",
      "Evaluating for {'lmda': 0.40050007578736113} ...\n",
      "The training loss is 6.766838159604151 with std:12.673137932474793. The val loss is 10.097204022625947 with std:16.930044165777257.\n",
      "10.097204022625947 0.40050007578736113\n",
      "The training loss is 6.565142145997356 with std:11.12689902569288. The val loss is 16.0843900155567 with std:78.3999767974253.\n",
      "16.0843900155567 0.40050007578736113\n",
      "The training loss is 7.223483776980072 with std:13.63731937767667. The val loss is 7.304319871999658 with std:8.8359164163275.\n",
      "7.304319871999658 0.40050007578736113\n",
      "The training loss is 6.673623819391043 with std:10.167798418057053. The val loss is 9.723364612076574 with std:23.035565876526213.\n",
      "9.723364612076574 0.40050007578736113\n",
      "Evaluating for {'lmda': 0.4042095839796306} ...\n",
      "The training loss is 6.771062083261633 with std:12.677925389918347. The val loss is 10.11090990877845 with std:16.992263387087807.\n",
      "10.11090990877845 0.4042095839796306\n",
      "The training loss is 6.567008349556152 with std:11.12918938392496. The val loss is 16.100089001540812 with std:78.53846733434341.\n",
      "16.100089001540812 0.4042095839796306\n",
      "The training loss is 7.229009493684332 with std:13.644548571871841. The val loss is 7.304371959164176 with std:8.8374199097296.\n",
      "7.304371959164176 0.4042095839796306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 6.678965424026928 with std:10.178119184112422. The val loss is 9.728941531700693 with std:23.035767118994414.\n",
      "9.728941531700693 0.4042095839796306\n",
      "Evaluating for {'lmda': 0.4079534503452449} ...\n",
      "The training loss is 6.775299294430616 with std:12.682781646053654. The val loss is 10.124642932377206 with std:17.055070471848047.\n",
      "10.124642932377206 0.4079534503452449\n",
      "The training loss is 6.568875458407264 with std:11.131502016123354. The val loss is 16.115746598841206 with std:78.67665479868312.\n",
      "16.115746598841206 0.4079534503452449\n",
      "The training loss is 7.234549659182584 with std:13.651845436203185. The val loss is 7.304440741843089 with std:8.838993135489355.\n",
      "7.304440741843089 0.4079534503452449\n",
      "The training loss is 6.684309173782825 with std:10.188460824681012. The val loss is 9.734530573355944 with std:23.036071845397696.\n",
      "9.734530573355944 0.4079534503452449\n",
      "Evaluating for {'lmda': 0.4117319931161679} ...\n",
      "The training loss is 6.779549740355244 with std:12.687706866696525. The val loss is 10.138406062762048 with std:17.11848392324997.\n",
      "10.138406062762048 0.4117319931161679\n",
      "The training loss is 6.570743440735864 with std:11.13383700392932. The val loss is 16.13136206233845 with std:78.81453232724556.\n",
      "16.13136206233845 0.4117319931161679\n",
      "The training loss is 7.240104082283927 with std:13.659210022396698. The val loss is 7.304526088364407 with std:8.840635117373575.\n",
      "7.304526088364407 0.4117319931161679\n",
      "The training loss is 6.689654940731906 with std:10.198823118387669. The val loss is 9.740131206616907 with std:23.036479932794997.\n",
      "9.740131206616907 0.4117319931161679\n",
      "Evaluating for {'lmda': 0.41554553347188755} ...\n",
      "The training loss is 6.783813366665833 with std:12.692701210574143. The val loss is 10.152202283670368 with std:17.182522670645376.\n",
      "10.152202283670368 0.41554553347188755\n",
      "The training loss is 6.572612265223582 with std:11.136194429142614. The val loss is 16.14693464999895 with std:78.9520930764525.\n",
      "16.14693464999895 0.41554553347188755\n",
      "The training loss is 7.245672570425411 with std:13.666642375102148. The val loss is 7.304627863580384 with std:8.84234487351286.\n",
      "7.304627863580384 0.41554553347188755\n",
      "The training loss is 6.695002597267886 with std:10.209205842831011. The val loss is 9.74574290370688 with std:23.03699124816201.\n",
      "9.74574290370688 0.41554553347188755\n",
      "Evaluating for {'lmda': 0.41939439556671865} ...\n",
      "The training loss is 6.788090117389366 with std:12.697764829248399. The val loss is 10.166034592456352 with std:17.24720606055533.\n",
      "10.166034592456352 0.41939439556671865\n",
      "The training loss is 6.574481901059341 with std:11.138574373736578. The val loss is 16.16246362295866 with std:79.08933022313936.\n",
      "16.16246362295866 0.41939439556671865\n",
      "The training loss is 7.2512549297212185 with std:13.674142531872747. The val loss is 7.304745928915922 with std:8.84412141684763.\n",
      "7.304745928915922 0.41939439556671865\n",
      "The training loss is 6.7003520161303785 with std:10.219608774630057. The val loss is 9.751365139605957 with std:23.03760564849251.\n",
      "9.751365139605957 0.41939439556671865\n",
      "Evaluating for {'lmda': 0.4232789065573549} ...\n",
      "The training loss is 6.79237993495949 with std:12.702897867039667. The val loss is 10.179905999394872 with std:17.31255384823435.\n",
      "10.179905999394872 0.4232789065573549\n",
      "The training loss is 6.576352317950313 with std:11.140976919874063. The val loss is 16.177948245605673 with std:79.22623696533545.\n",
      "16.177948245605673 0.4232789065573549\n",
      "The training loss is 7.256850965012506 with std:13.681710523146712. The val loss is 7.304880142419251 with std:8.845963755575335.\n",
      "7.304880142419251 0.4232789065573549\n",
      "The training loss is 6.705703070431709 with std:10.230031689474256. The val loss is 9.756997392163546 with std:23.038322980909417.\n",
      "9.756997392163546 0.4232789065573549\n",
      "Evaluating for {'lmda': 0.4271993966306777} ...\n",
      "The training loss is 6.796682760228016 with std:12.708100460955077. The val loss is 10.193819526987314 with std:17.37858618926459.\n",
      "10.193819526987314 0.4271993966306777\n",
      "The training loss is 6.578223486132565 with std:11.143402149922787. The val loss is 16.19338778566235 with std:79.36280652305012.\n",
      "16.19338778566235 0.4271993966306777\n",
      "The training loss is 7.262460479916993 with std:13.689346372231823. The val loss is 7.305030358813307 with std:8.847870893593672.\n",
      "7.305030358813307 0.4271993966306777\n",
      "The training loss is 6.711055633681471 with std:10.240474362170227. The val loss is 9.762639142204687 with std:23.039143082781262.\n",
      "9.762639142204687 0.4271993966306777\n",
      "Evaluating for {'lmda': 0.43115619903182284} ...\n",
      "The training loss is 6.800998532477006 with std:12.71337274061732. The val loss is 10.207778209367035 with std:17.445323631793695.\n",
      "10.207778209367035 0.43115619903182284\n",
      "The training loss is 6.580095376381573 with std:11.145850146471012. The val loss is 16.208781514263574 with std:79.4990321390213.\n",
      "16.208781514263574 0.43115619903182284\n",
      "The training loss is 7.268083276880019 with std:13.697050095293358. The val loss is 7.305196429548237 with std:8.84984183094409.\n",
      "7.305196429548237 0.43115619903182284\n",
      "The training loss is 6.716409579813878 with std:10.250936566693829. The val loss is 9.76828987364261 with std:23.040065781852487.\n",
      "9.76828987364261 0.43115619903182284\n",
      "Evaluating for {'lmda': 0.435149650092505} ...\n",
      "The training loss is 6.805327189430779 with std:12.718714828195985. The val loss is 10.22178509171445 with std:17.512787108856326.\n",
      "10.22178509171445 0.435149650092505\n",
      "The training loss is 6.581967960022971 with std:11.14832099234407. The val loss is 16.22412870603664 with std:79.6349070794747.\n",
      "16.22412870603664 0.435149650092505\n",
      "The training loss is 7.273719157224753 with std:13.704821701342984. The val loss is 7.305378202856142 with std:8.851875564251628.\n",
      "7.305378202856142 0.435149650092505\n",
      "The training loss is 6.721764783212757 with std:10.26141807623929. The val loss is 9.773949073585603 with std:23.04109089637116.\n",
      "9.773949073585603 0.435149650092505\n",
      "Evaluating for {'lmda': 0.4391800892596086} ...\n",
      "The training loss is 6.809668667269957 with std:12.724126838342455. The val loss is 10.235843229710493 with std:17.580997930960304.\n",
      "10.235843229710493 0.4391800892596086\n",
      "The training loss is 6.583841208942537 with std:11.150814770620688. The val loss is 16.239428639173106 with std:79.7704246348199.\n",
      "16.239428639173106 0.4391800892596086\n",
      "The training loss is 7.2793679212047 with std:13.712661192233927. The val loss is 7.305575523807045 with std:8.853971087165041.\n",
      "7.305575523807045 0.4391800892596086\n",
      "The training loss is 6.727121118737848 with std:10.271918663270606. The val loss is 9.779616232446584 with std:23.042218235234575.\n",
      "9.779616232446584 0.4391800892596086\n",
      "Evaluating for {'lmda': 0.4432478591240396} ...\n",
      "The training loss is 6.814022900644534 with std:12.72960887812519. The val loss is 10.249955689065041 with std:17.64997777915831.\n",
      "10.249955689065041 0.4432478591240396\n",
      "The training loss is 6.585715095596679 with std:11.153331564648383. The val loss is 16.254680595507846 with std:79.90557812041257.\n",
      "16.254680595507846 0.4432478591240396\n",
      "The training loss is 7.28502936805452 with std:13.720568562655204. The val loss is 7.305788234364888 with std:8.856127390790947.\n",
      "7.305788234364888 0.4432478591240396\n",
      "The training loss is 6.732478461750551 with std:10.2824380995732. The val loss is 9.785290844049443 with std:23.04344759813207.\n",
      "9.785290844049443 0.4432478591240396\n",
      "Evaluating for {'lmda': 0.4473533054498463} ...\n",
      "The training loss is 6.818389822688994 with std:12.735161046969507. The val loss is 10.26412554503556 with std:17.719748698076966.\n",
      "10.26412554503556 0.4473533054498463\n",
      "The training loss is 6.587589593022395 with std:11.15587145806084. The val loss is 16.269883860590106 with std:80.04036087723856.\n",
      "16.269883860590106 0.4473533054498463\n",
      "The training loss is 7.290703296042889 with std:13.728543800131705. The val loss is 7.3060161734470785 with std:8.858343464129948.\n",
      "7.3060161734470785 0.4473533054498463\n",
      "The training loss is 6.737836688140065 with std:10.292976156306729. The val loss is 9.790972405738541 with std:23.04477877570408.\n",
      "9.790972405738541 0.4473533054498463\n",
      "Evaluating for {'lmda': 0.4514967772036102} ...\n",
      "The training loss is 6.822769365037283 with std:12.74078343659848. The val loss is 10.278355882031462 with std:17.790333089491572.\n",
      "10.278355882031462 0.4514967772036102\n",
      "The training loss is 6.589464674847164 with std:11.15843453479412. The val loss is 16.285037723752975 with std:80.17476627259182.\n",
      "16.285037723752975 0.4514967772036102\n",
      "The training loss is 7.296389502524755 with std:13.736586885025819. The val loss is 7.306259176982869 with std:8.860618294504684.\n",
      "7.306259176982869 0.4514967772036102\n",
      "The training loss is 6.743195674348658 with std:10.303532604057098. The val loss is 9.796660418484336 with std:23.046211549702893.\n",
      "9.796660418484336 0.4514967772036102\n",
      "Evaluating for {'lmda': 0.4556786265841064} ...\n",
      "The training loss is 6.827161457838974 with std:12.746476130977797. The val loss is 10.292649793215631 with std:17.86175370590578.\n",
      "10.292649793215631 0.4556786265841064\n",
      "The training loss is 6.591340315298864 with std:11.161020879103793. The val loss is 16.30014147818408 with std:80.30878770076231.\n",
      "16.30014147818408 0.4556786265841064\n",
      "The training loss is 7.302087783994536 with std:13.744697790542556. The val loss is 7.306517077975192 with std:8.862950867989454.\n",
      "7.306517077975192 0.4556786265841064\n",
      "The training loss is 6.74855529739804 with std:10.314107212890896. The val loss is 9.802354386989602 with std:23.04774569315767.\n",
      "9.802354386989602 0.4556786265841064\n",
      "Evaluating for {'lmda': 0.45989920905224385} ...\n",
      "The training loss is 6.8315660297749705 with std:12.752239206260251. The val loss is 10.307010380154237 with std:17.934033644435534.\n",
      "10.307010380154237 0.45989920905224385\n",
      "The training loss is 6.593216489215199 with std:11.163630575581237. The val loss is 16.31519442099222 with std:80.44241858368899.\n",
      "16.31519442099222 0.45989920905224385\n",
      "The training loss is 7.307797936138949 with std:13.752876482736342. The val loss is 7.306789706561955 with std:8.865340169833065.\n",
      "7.306789706561955 0.45989920905224385\n",
      "The training loss is 6.753915434914499 with std:10.324699752408753. The val loss is 9.80805381979499 with std:23.04938097055354.\n",
      "9.80805381979499 0.45989920905224385\n",
      "Evaluating for {'lmda': 0.46415888336127775} ...\n",
      "The training loss is 6.835983008076046 with std:12.758072730738421. The val loss is 10.321440752506897 with std:18.007196340940038.\n",
      "10.321440752506897 0.46415888336127775\n",
      "The training loss is 6.595093172053553 with std:11.166263709171538. The val loss is 16.3301958532749 with std:80.57565237162152.\n",
      "16.3301958532749 0.46415888336127775\n",
      "The training loss is 7.313519753891216 with std:13.761122920523965. The val loss is 7.307076890079873 with std:8.867785184880683.\n",
      "7.307076890079873 0.46415888336127775\n",
      "The training loss is 6.759275965154722 with std:10.33530999180019. The val loss is 9.813758229384907 with std:23.051117138012003.\n",
      "9.813758229384907 0.46415888336127775\n",
      "Evaluating for {'lmda': 0.46845801158730455} ...\n",
      "The training loss is 6.840412318539335 with std:12.763976764793712. The val loss is 10.33594402772155 with std:18.081265564201818.\n",
      "10.33594402772155 0.46845801158730455\n",
      "The training loss is 6.596970339899964 with std:11.168920365190205. The val loss is 16.345145080177904 with std:80.7084825437127.\n",
      "16.345145080177904 0.46845801158730455\n",
      "The training loss is 7.319253031484161 with std:13.769437055696441. The val loss is 7.307378453128545 with std:8.870284897990217.\n",
      "7.307378453128545 0.46845801158730455\n",
      "The training loss is 6.7646367670314955 with std:10.345937699899103. The val loss is 9.819467132290793 with std:23.052953943478702.\n",
      "9.819467132290793 0.46845801158730455\n",
      "Evaluating for {'lmda': 0.47279695916003905} ...\n",
      "The training loss is 6.844853885547321 with std:12.769951360852485. The val loss is 10.350523330792731 with std:18.156265410484085.\n",
      "10.350523330792731 0.47279695916003905\n",
      "The training loss is 6.598847969478573 with std:11.171600629340325. The val loss is 16.360041410961415 with std:80.84090260866903.\n",
      "16.360041410961415 0.47279695916003905\n",
      "The training loss is 7.324997562504506 with std:13.777818832936415. The val loss is 7.30769421763577 with std:8.872838294444437.\n",
      "7.30769421763577 0.47279695916003905\n",
      "The training loss is 6.769997720138963 with std:10.3565826452395. The val loss is 9.825180049195254 with std:23.054891126918328.\n",
      "9.825180049195254 0.47279695916003905\n",
      "Evaluating for {'lmda': 0.4771760948938746} ...\n",
      "The training loss is 6.84930763208696 with std:12.77599656334368. The val loss is 10.365181794011683 with std:18.232220298029148.\n",
      "10.365181794011683 0.4771760948938746\n",
      "The training loss is 6.6007260381606825 with std:11.17430458773039. The val loss is 16.374884159059164 with std:80.97290610534384.\n",
      "16.374884159059164 0.4771760948938746\n",
      "The training loss is 7.330753139947762 with std:13.786268189839165. The val loss is 7.308024002924777 with std:8.875444360361836.\n",
      "7.308024002924777 0.4771760948938746\n",
      "The training loss is 6.775358704777663 with std:10.36724459611047. The val loss is 9.83089650503455 with std:23.056928420515238.\n",
      "9.83089650503455 0.4771760948938746\n",
      "Evaluating for {'lmda': 0.4815957910192351} ...\n",
      "The training loss is 6.853773479768904 with std:12.782112408658842. The val loss is 10.379922556764864 with std:18.309154961852656.\n",
      "10.379922556764864 0.4815957910192351\n",
      "The training loss is 6.602604523973541 with std:11.177032326892027. The val loss is 16.389672642133434 with std:81.10448660328713.\n",
      "16.389672642133434 0.4815957910192351\n",
      "The training loss is 7.336519556271327 with std:13.794785056932998. The val loss is 7.308367625781536 with std:8.878102083099094.\n",
      "7.308367625781536 0.4815957910192351\n",
      "The training loss is 6.780719601980134 with std:10.37792332061425. The val loss is 9.83661602910049 with std:23.059065548878692.\n",
      "9.83661602910049 0.4815957910192351\n",
      "Evaluating for {'lmda': 0.4860564232142134} ...\n",
      "The training loss is 6.85825134884845 with std:12.788298925116342. The val loss is 10.3947487653501 with std:18.387094448621493.\n",
      "10.3947487653501 0.4860564232142134\n",
      "The training loss is 6.604483405609157 with std:11.179783933797394. The val loss is 16.404406182136963 with std:81.23563770336725.\n",
      "16.404406182136963 0.4860564232142134\n",
      "The training loss is 7.342296603450782 with std:13.803369357707892. The val loss is 7.308724900523701 with std:8.880810451653025.\n",
      "7.308724900523701 0.4860564232142134\n",
      "The training loss is 6.786080293535708 with std:10.388618586722389. The val loss is 9.842338155141393 with std:23.06130222925577.\n",
      "9.842338155141393 0.4860564232142134\n",
      "Evaluating for {'lmda': 0.49055837063650454} ...\n",
      "The training loss is 6.8627411582457185 with std:12.794556132926699. The val loss is 10.409663572805137 with std:18.466064111588963.\n",
      "10.409663572805137 0.49055837063650454\n",
      "The training loss is 6.606362662433033 with std:11.182559495877145. The val loss is 16.419084105361605 with std:81.36635303826681.\n",
      "16.419084105361605 0.49055837063650454\n",
      "The training loss is 7.348084073033263 with std:13.812021008642242. The val loss is 7.3090956390703035 with std:8.883568457054418.\n",
      "7.3090956390703035 0.49055837063650454\n",
      "The training loss is 6.791440662015566 with std:10.399330162333856. The val loss is 9.848062421463643 with std:23.06363817175203.\n",
      "9.848062421463643 0.49055837063650454\n",
      "Evaluating for {'lmda': 0.49510201595563513} ...\n",
      "The training loss is 6.867242825567651 with std:12.800884044162904. The val loss is 10.424670138782071 with std:18.546089605790375.\n",
      "10.424670138782071 0.49510201595563513\n",
      "The training loss is 6.6082422744923734 with std:11.185359101038443. The val loss is 16.433705742496105 with std:81.49662627306982.\n",
      "16.433705742496105 0.49510201595563513\n",
      "The training loss is 7.3538817561935055 with std:13.82073991923582. The val loss is 7.309479651012573 with std:8.886375092759513.\n",
      "7.309479651012573 0.49510201595563513\n",
      "The training loss is 6.796800590797458 with std:10.410057815332532. The val loss is 9.853788371029369 with std:23.066073079550264.\n",
      "9.853788371029369 0.49510201595563513\n",
      "Evaluating for {'lmda': 0.4996877453854884} ...\n",
      "The training loss is 6.871756267129603 with std:12.807282662731776. The val loss is 10.43977162941536 with std:18.62719688316133.\n",
      "10.43977162941536 0.4996877453854884\n",
      "The training loss is 6.610122222524649 with std:11.188182837683224. The val loss is 16.44827042867426 with std:81.62645110575326.\n",
      "16.44827042867426 0.4996877453854884\n",
      "The training loss is 7.359689443788 with std:13.829525992043878. The val loss is 7.309876743685535 with std:8.88922935503456.\n",
      "7.309876743685535 0.4996877453854884\n",
      "The training loss is 6.802159964090249 with std:10.420801313645537. The val loss is 9.859515551555303 with std:23.068606649141756.\n",
      "9.859515551555303 0.4996877453854884\n",
      "Evaluating for {'lmda': 0.5043159487171359} ...\n",
      "The training loss is 6.876281397978582 with std:12.813751984350759. The val loss is 10.454971217219706 with std:18.70941218780446.\n",
      "10.454971217219706 0.5043159487171359\n",
      "The training loss is 6.612002487965547 with std:11.191030794726306. The val loss is 16.462777503522762 with std:81.7558212676872.\n",
      "16.462777503522762 0.5043159487171359\n",
      "The training loss is 7.36550692641079 with std:13.838379122715818. The val loss is 7.3102867222410435 with std:8.892130243337894.\n",
      "7.3102867222410435 0.5043159487171359\n",
      "The training loss is 6.807518666958411 with std:10.431560425302225. The val loss is 9.865243515611194 with std:23.07123857056231.\n",
      "9.865243515611194 0.5043159487171359\n",
      "Evaluating for {'lmda': 0.508987019351968} ...\n",
      "The training loss is 6.880818131915637 with std:12.820291996524446. The val loss is 10.47027208100732 with std:18.792762051338418.\n",
      "10.47027208100732 0.508987019351968\n",
      "The training loss is 6.613883052957091 with std:11.193903061613637. The val loss is 16.477226311211666 with std:81.88473052415013.\n",
      "16.477226311211666 0.508987019351968\n",
      "The training loss is 7.371333994448338 with std:13.847299200035483. The val loss is 7.310709389720679 with std:8.895076760693012.\n",
      "7.310709389720679 0.508987019351968\n",
      "The training loss is 6.812876585346525 with std:10.442334918493174. The val loss is 9.870971820715187 with std:23.073968527626484.\n",
      "9.870971820715187 0.508987019351968\n",
      "Evaluating for {'lmda': 0.5137013543351344} ...\n",
      "The training loss is 6.885366381520349 with std:12.82690267852812. The val loss is 10.485677405823697 with std:18.87727328831467.\n",
      "10.485677405823697 0.5137013543351344\n",
      "The training loss is 6.61576390035534 with std:11.196799728340753. The val loss is 16.49161620049591 with std:82.013172674776.\n",
      "16.49161620049591 0.5137013543351344\n",
      "The training loss is 7.3771704381343675 with std:13.856286105964067. The val loss is 7.3111445471306045 with std:8.898067914059308.\n",
      "7.3111445471306045 0.5137013543351344\n",
      "The training loss is 6.81823360610265 with std:10.453124561628623. The val loss is 9.876700029428873 with std:23.076796198174666.\n",
      "9.876700029428873 0.5137013543351344\n",
      "Evaluating for {'lmda': 0.5184593543892912} ...\n",
      "The training loss is 6.889926058174849 with std:12.833584001391527. The val loss is 10.501190382883077 with std:18.96297299157406.\n",
      "10.501190382883077 0.5184593543892912\n",
      "The training loss is 6.61764501373827 with std:11.199720885472075. The val loss is 16.505946524759885 with std:82.1411415540135.\n",
      "16.505946524759885 0.5184593543892912\n",
      "The training loss is 7.38301604760577 with std:13.86533971568844. The val loss is 7.311591993516806 with std:8.901102714696526.\n",
      "7.311591993516806 0.5184593543892912\n",
      "The training loss is 6.823589617003248 with std:10.463929123400089. The val loss is 9.882427709454223 with std:23.079721254323882.\n",
      "9.882427709454223 0.5184593543892912\n",
      "Evaluating for {'lmda': 0.5232614239486661} ...\n",
      "The training loss is 6.89449707208872 with std:12.840335927886903. The val loss is 10.516814209537339 with std:19.049888527767468.\n",
      "10.516814209537339 0.5232614239486661\n",
      "The training loss is 6.619526377413096 with std:11.20266662415847. The val loss is 16.52021664205996 with std:82.26863103158793.\n",
      "16.52021664205996 0.5232614239486661\n",
      "The training loss is 7.388870612957562 with std:13.874459897669826. The val loss is 7.312051526040632 with std:8.904180178521937.\n",
      "7.312051526040632 0.5232614239486661\n",
      "The training loss is 6.82894450677615 with std:10.474748372838501. The val loss is 9.888154433722653 with std:23.082743362712517.\n",
      "9.888154433722653 0.5232614239486661\n",
      "Evaluating for {'lmda': 0.5281079711934331} ...\n",
      "The training loss is 6.899079332324255 with std:12.847158412520137. The val loss is 10.532552089237956 with std:19.13804753276243.\n",
      "10.532552089237956 0.5281079711934331\n",
      "The training loss is 6.621407976423768 with std:11.205637036156993. The val loss is 16.53442591516221 with std:82.39563501290502.\n",
      "16.53442591516221 0.5281079711934331\n",
      "The training loss is 7.394733924297709 with std:13.883646513696103. The val loss is 7.312522940056125 with std:8.907299326464384.\n",
      "7.312522940056125 0.5281079711934331\n",
      "The training loss is 6.834298165124164 with std:10.485582079375309. The val loss is 9.893879780491252 with std:23.08586218476777.\n",
      "9.893879780491252 0.5281079711934331\n",
      "Evaluating for {'lmda': 0.5329994080844088} ...\n",
      "The training loss is 6.903672746822548 with std:12.85405140152502. The val loss is 10.548407231541784 with std:19.22747790727779.\n",
      "10.548407231541784 0.5329994080844088\n",
      "The training loss is 6.623289796558211 with std:11.208632213849489. The val loss is 16.548573711579774 with std:82.52214743945467.\n",
      "16.548573711579774 0.5329994080844088\n",
      "The training loss is 7.4006057718028195 with std:13.892899418937798. The val loss is 7.313006029186842 with std:8.910459184809584.\n",
      "7.313006029186842 0.5329994080844088\n",
      "The training loss is 6.839650482748595 with std:10.496430012902973. The val loss is 9.89960333343101 with std:23.08907737695949.\n",
      "9.89960333343101 0.5329994080844088\n",
      "Evaluating for {'lmda': 0.5379361503980703} ...\n",
      "The training loss is 6.9082772224298985 with std:12.861014832861311. The val loss is 10.56438285207429 with std:19.318207812158438.\n",
      "10.56438285207429 0.5379361503980703\n",
      "The training loss is 6.625171824355312 with std:11.211652250261608. The val loss is 16.56265940361076 with std:82.64816228923087.\n",
      "16.56265940361076 0.5379361503980703\n",
      "The training loss is 7.406485945772357 with std:13.902218462005637. The val loss is 7.31350058540467 with std:8.913658785541534.\n",
      "7.31350058540467 0.5379361503980703\n",
      "The training loss is 6.845001351371833 with std:10.507291943835023. The val loss is 9.905324681717323 with std:23.092388591068612.\n",
      "9.905324681717323 0.5379361503980703\n",
      "Evaluating for {'lmda': 0.5429186177618943} ...\n",
      "The training loss is 6.9128926649246685 with std:12.868048636214967. The val loss is 10.580482172565896 with std:19.41026566407327.\n",
      "10.580482172565896 0.5429186177618943\n",
      "The training loss is 6.62705404711189 with std:11.21469723908215. The val loss is 16.5766823683689 with std:82.77367357707446.\n",
      "16.5766823683689 0.5429186177618943\n",
      "The training loss is 7.412374236684364 with std:13.911603485012026. The val loss is 7.314006399107958 with std:8.916897166676907.\n",
      "7.314006399107958 0.5429186177618943\n",
      "The training loss is 6.85035066376077 with std:10.518167643167923. The val loss is 9.91104342011973 with std:23.095795474457795.\n",
      "9.91104342011973 0.5429186177618943\n",
      "Evaluating for {'lmda': 0.5479472336900287} ...\n",
      "The training loss is 6.917518979044711 with std:12.875152733002384. The val loss is 10.5967084208429 with std:19.50368013082317.\n",
      "10.5967084208429 0.5479472336900287\n",
      "The training loss is 6.628936452889379 with std:11.217767274682288. The val loss is 16.590641987816888 with std:82.8986753550504.\n",
      "16.590641987816888 0.5479472336900287\n",
      "The training loss is 7.418270435250019 with std:13.921054323635227. The val loss is 7.314523259201668 with std:8.92017337259373.\n",
      "7.314523259201668 0.5479472336900287\n",
      "The training loss is 6.855698313748931 with std:10.52905688254162. The val loss is 9.916759149086932 with std:23.099297670339666.\n",
      "9.916759149086932 0.5479472336900287\n",
      "Evaluating for {'lmda': 0.55302242561929} ...\n",
      "The training loss is 6.922156068515534 with std:12.882327036378399. The val loss is 10.613064830858981 with std:19.59848012685291.\n",
      "10.613064830858981 0.55302242561929\n",
      "The training loss is 6.63081903052062 with std:11.220862452135075. The val loss is 16.604537648796267 with std:83.02316171278459.\n",
      "16.604537648796267 0.55302242561929\n",
      "The training loss is 7.424174332467708 with std:13.930570807184992. The val loss is 7.315050953176504 with std:8.923486454352028.\n",
      "7.315050953176504 0.55302242561929\n",
      "The training loss is 6.861044196259096 with std:10.539959434301096. The val loss is 9.922471474835154 with std:23.10289481805733.\n",
      "9.922471474835154 0.55302242561929\n",
      "Evaluating for {'lmda': 0.5581446249454961} ...\n",
      "The training loss is 6.926803836078037 with std:12.889571451245738. The val loss is 10.629554642724617 with std:19.69469480866522.\n",
      "10.629554642724617 0.5581446249454961\n",
      "The training loss is 6.632701769616118 with std:11.223982867234625. The val loss is 16.618368743056603 with std:83.1471267778083.\n",
      "16.618368743056603 0.5581446249454961\n",
      "The training loss is 7.430085719678365 with std:13.940152758673094. The val loss is 7.315589267190144 with std:8.926835470011286.\n",
      "7.315589267190144 0.5581446249454961\n",
      "The training loss is 6.8663882073247855 with std:10.55087507155683. The val loss is 9.928180009430758 with std:23.10658655336111.\n",
      "9.928180009430758 0.5581446249454961\n",
      "Evaluating for {'lmda': 0.5633142670601358} ...\n",
      "The training loss is 6.931462183518186 with std:12.896885874270716. The val loss is 10.646181102734756 with std:19.792353570142456.\n",
      "10.646181102734756 0.5633142670601358\n",
      "The training loss is 6.634584660570573 with std:11.227128616516122. The val loss is 16.632134667279335 with std:83.27056471584332.\n",
      "16.632134667279335 0.5633142670601358\n",
      "The training loss is 7.436004388619222 with std:13.949799994884597. The val loss is 7.316137986148302 with std:8.930219484938613.\n",
      "7.316137986148302 0.5633142670601358\n",
      "The training loss is 6.871730244113016 with std:10.561803568247822. The val loss is 9.933884370875134 with std:23.11037250869103.\n",
      "9.933884370875134 0.5633142670601358\n",
      "Evaluating for {'lmda': 0.5685317913873753} ...\n",
      "The training loss is 6.936131011695757 with std:12.904270193899023. The val loss is 10.662947463416039 with std:19.89148603791779.\n",
      "10.662947463416039 0.5685317913873753\n",
      "The training loss is 6.636467694568813 with std:11.230299797275451. The val loss is 16.645834823103055 with std:83.39346973110757.\n",
      "16.645834823103055 0.5685317913873753\n",
      "The training loss is 7.441930131477596 with std:13.959512326452383. The val loss is 7.31669689378574 with std:8.933637572111103.\n",
      "7.31669689378574 0.5685317913873753\n",
      "The training loss is 6.87707020494525 with std:10.572744699202413. The val loss is 9.939584183186893 with std:23.114252313465002.\n",
      "9.939584183186893 0.5685317913873753\n",
      "Evaluating for {'lmda': 0.5737976414214134} ...\n",
      "The training loss is 6.940810220573998 with std:12.911724290377114. The val loss is 10.67985698356365 with std:19.99212206660005.\n",
      "10.67985698356365 0.5737976414214134\n",
      "The training loss is 6.638350863592101 with std:11.23349650758919. The val loss is 16.659468617147425 with std:83.51583606660694.\n",
      "16.659468617147425 0.5737976414214134\n",
      "The training loss is 7.447862740945566 with std:13.969289557935717. The val loss is 7.317265772748498 with std:8.93708881241251.\n",
      "7.317265772748498 0.5737976414214134\n",
      "The training loss is 6.882407989318603 with std:10.583698240199526. The val loss is 9.945279076480114 with std:23.11822559435943.\n",
      "9.945279076480114 0.5737976414214134\n",
      "Evaluating for {'lmda': 0.5791122647641759} ...\n",
      "The training loss is 6.945499709250335 with std:12.919248035776574. The val loss is 10.6969129282971 with std:20.09429173403696.\n",
      "10.6969129282971 0.5791122647641759\n",
      "The training loss is 6.640234160423716 with std:11.236718846334224. The val loss is 16.6730354610313 with std:83.63765800437628.\n",
      "16.6730354610313 0.5791122647641759\n",
      "The training loss is 7.4538020102726685 with std:13.979131487898869. The val loss is 7.3178444046765785 with std:8.940572294922694.\n",
      "7.3178444046765785 0.5791122647641759\n",
      "The training loss is 6.8877434979272145 with std:10.594663968031156. The val loss is 9.950968687046885 with std:23.122291975608785.\n",
      "9.950968687046885 0.5791122647641759\n",
      "Evaluating for {'lmda': 0.5844761131633631} ...\n",
      "The training loss is 6.950199375986111 with std:12.926841294020189. The val loss is 10.714118569103508 with std:20.198025336416812.\n",
      "10.714118569103508 0.5844761131633631\n",
      "The training loss is 6.642117578654959 with std:11.23996691320823. The val loss is 16.686534771393273 with std:83.75892986574125.\n",
      "16.686534771393273 0.5844761131633631\n",
      "The training loss is 7.459747733319822 with std:13.989037908994778. The val loss is 7.318432570286493 with std:8.944087117200377.\n",
      "7.318432570286493 0.5844761131633631\n",
      "The training loss is 6.893076632682952 with std:10.605641660564281. The val loss is 9.956652657432992 with std:23.12645107929065.\n",
      "9.956652657432992 0.5844761131633631\n",
      "Evaluating for {'lmda': 0.58988964255085} ...\n",
      "The training loss is 6.954909118238355 with std:12.934503920913784. The val loss is 10.731477183903628 with std:20.30335338344531.\n",
      "10.731477183903628 0.58988964255085\n",
      "The training loss is 6.644001112690727 with std:11.243240808749897. The val loss is 16.699965969907566 with std:83.87964601154049.\n",
      "16.699965969907566 0.58988964255085\n",
      "The training loss is 7.465699704611819 with std:13.999008608048896. The val loss is 7.319030049454032 with std:8.947632385558405.\n",
      "7.319030049454032 0.58988964255085\n",
      "The training loss is 6.898407296735442 with std:10.616631096801678. The val loss is 9.96233063651435 with std:23.13070252562316.\n",
      "9.96233063651435 0.58988964255085\n",
      "Evaluating for {'lmda': 0.595353313081437} ...\n",
      "The training loss is 6.959628832690631 with std:12.94223576417984. The val loss is 10.748992057091971 with std:20.410306593258127.\n",
      "10.748992057091971 0.595353313081437\n",
      "The training loss is 6.64588475775481 with std:11.246540634358864. The val loss is 16.71332848330113 with std:83.99980084236064.\n",
      "16.71332848330113 0.595353313081437\n",
      "The training loss is 7.471657719390094 with std:14.009043366147125. The val loss is 7.319636621298229 with std:8.9512072153341.\n",
      "7.319636621298229 0.595353313081437\n",
      "The training loss is 6.90373539449263 with std:10.627632056944202. The val loss is 9.968002279572618 with std:23.135045933261082.\n",
      "9.968002279572618 0.595353313081437\n",
      "Evaluating for {'lmda': 0.6008675891719687} ...\n",
      "The training loss is 6.964358415284865 with std:12.95003666349436. The val loss is 10.7666664796036 with std:20.51891588744685.\n",
      "10.7666664796036 0.6008675891719687\n",
      "The training loss is 6.647768509895439 with std:11.249866492316684. The val loss is 16.72662174336394 with std:84.11938879870497.\n",
      "16.72662174336394 0.6008675891719687\n",
      "The training loss is 7.47762157366556 with std:14.019141958726522. The val loss is 7.320252064264622 with std:8.954810731150614.\n",
      "7.320252064264622 0.6008675891719687\n",
      "The training loss is 6.909060831640571 with std:10.638644322453155. The val loss is 9.973667248368342 with std:23.13948091959413.\n",
      "9.973667248368342 0.6008675891719687\n",
      "Evaluating for {'lmda': 0.6064329395408061} ...\n",
      "The training loss is 6.969097761253667 with std:12.957906450528272. The val loss is 10.784503748969412 with std:20.629212385922887.\n",
      "10.784503748969412 0.6064329395408061\n",
      "The training loss is 6.64965236599044 with std:11.253218485807125. The val loss is 16.739845186963702 with std:84.23840436121051.\n",
      "16.739845186963702 0.6064329395408061\n",
      "The training loss is 7.483591064269768 with std:14.029304155665795. The val loss is 7.3208761562095415 with std:8.958442067173968.\n",
      "7.3208761562095415 0.6064329395408061\n",
      "The training loss is 6.9143835151629895 with std:10.649667676111118. The val loss is 9.979325211212643 with std:23.1440071010458.\n",
      "9.979325211212643 0.6064329395408061\n",
      "Evaluating for {'lmda': 0.6120498372476697} ...\n",
      "The training loss is 6.97384676515232 with std:12.965844948990112. The val loss is 10.80250716936953 with std:20.74122740169862.\n",
      "10.80250716936953 0.6120498372476697\n",
      "The training loss is 6.651536323752249 with std:11.25659671893697. The val loss is 16.752998256053104 with std:84.35684205078871.\n",
      "16.752998256053104 0.6120498372476697\n",
      "The training loss is 7.489565988907167 with std:14.039529721381816. The val loss is 7.321508674483907 with std:8.962100367360403.\n",
      "7.321508674483907 0.6120498372476697\n",
      "The training loss is 6.919703353360449 with std:10.660701902083996. The val loss is 9.984975843037134 with std:23.148624093373947.\n",
      "9.984975843037134 0.6120498372476697\n",
      "Evaluating for {'lmda': 0.6177187597338495} ...\n",
      "The training loss is 6.978605320891922 with std:12.973851974674304. The val loss is 10.820680051698087 with std:20.854992435673978.\n",
      "10.820680051698087 0.6177187597338495\n",
      "The training loss is 6.653420381732983 with std:11.26000129675687. The val loss is 16.766080397679268 with std:84.47469642879611.\n",
      "16.766080397679268 0.6177187597338495\n",
      "The training loss is 7.4955461462059665 with std:14.04981841492449. The val loss is 7.322149396017512 with std:8.965784785698812.\n",
      "7.322149396017512 0.6177187597338495\n",
      "The training loss is 6.92502025586933 with std:10.671746785982178. The val loss is 9.990618825463628 with std:23.153331511974503.\n",
      "9.990618825463628 0.6177187597338495\n",
      "Evaluating for {'lmda': 0.6234401888627864} ...\n",
      "The training loss is 6.983373321772683 with std:12.981927335511669. The val loss is 10.839025713607933 with std:20.97053917121118.\n",
      "10.839025713607933 0.6234401888627864\n",
      "The training loss is 6.655304539329222 with std:11.263432325282151. The val loss is 16.77909106399087 with std:84.59196209717827.\n",
      "16.77909106399087 0.6234401888627864\n",
      "The training loss is 7.501531335769093 with std:14.06016999007751. The val loss is 7.322798097403624 with std:8.969494486446102.\n",
      "7.322798097403624 0.6234401888627864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 6.930334133680502 with std:10.682802114922467. The val loss is 9.996253846870426 with std:23.158128972181835.\n",
      "9.996253846870426 0.6234401888627864\n",
      "Evaluating for {'lmda': 0.6292146109610344} ...\n",
      "The training loss is 6.98815066051661 with std:12.99007083162171. The val loss is 10.857547479572945 with std:21.087899468779842.\n",
      "10.857547479572945 0.6292146109610344\n",
      "The training loss is 6.657188796786746 with std:11.266889911514298. The val loss is 16.79202971223971 with std:84.70863369856455.\n",
      "16.79202971223971 0.6292146109610344\n",
      "The training loss is 7.507521358224451 with std:14.070584195458142. The val loss is 7.323454554983725 with std:8.973228644355297.\n",
      "7.323454554983725 0.6292146109610344\n",
      "The training loss is 6.93564489915759 with std:10.693867677588512. The val loss is 10.001880602457868 with std:23.163016089573365.\n",
      "10.001880602457868 0.6292146109610344\n",
      "Evaluating for {'lmda': 0.6350425168595962} ...\n",
      "The training loss is 6.992937229301968 with std:12.998282255371358. The val loss is 10.876248680937765 with std:21.207105360439336.\n",
      "10.876248680937765 0.6350425168595962\n",
      "The training loss is 6.659073155205179 with std:11.270374163461629. The val loss is 16.804895804787503 with std:84.82470591641378.\n",
      "16.804895804787503 0.6350425168595962\n",
      "The training loss is 7.513516015274869 with std:14.081060774621347. The val loss is 7.324118544931369 with std:8.976986444895854.\n",
      "7.324118544931369 0.6350425168595962\n",
      "The training loss is 6.940952466055239 with std:10.704943264293142. The val loss is 10.00749879431303 with std:23.167992480273973.\n",
      "10.00749879431303 0.6350425168595962\n",
      "Evaluating for {'lmda': 0.640924401935645} ...\n",
      "The training loss is 6.997732919797194 with std:13.006561391434346. The val loss is 10.895132655961994 with std:21.328189044230538.\n",
      "10.895132655961994 0.640924401935645\n",
      "The training loss is 6.660957616542522 with std:11.273885190161396. The val loss is 16.81768880910386 with std:84.94017347507538.\n",
      "16.81768880910386 0.640924401935645\n",
      "The training loss is 7.5195151097478 with std:14.091599466165054. The val loss is 7.324789843337465 with std:8.980767084469253.\n",
      "7.324789843337465 0.640924401935645\n",
      "The training loss is 6.9462567495364995 with std:10.716028667038117. The val loss is 10.013108131471549 with std:23.173057761260193.\n",
      "10.013108131471549 0.640924401935645\n",
      "Evaluating for {'lmda': 0.6468607661546327} ...\n",
      "The training loss is 7.002537623194558 with std:13.014908016853754. The val loss is 10.91420274987533 with std:21.45118287860062.\n",
      "10.91420274987533 0.6468607661546327\n",
      "The training loss is 6.662842183619415 with std:11.277423101700556. The val loss is 16.83040819776826 with std:85.05503113989221.\n",
      "16.83040819776826 0.6468607661546327\n",
      "The training loss is 7.5255184456438435 with std:14.102200003836577. The val loss is 7.325468226294768 with std:8.984569770615687.\n",
      "7.325468226294768 0.6468607661546327\n",
      "The training loss is 6.951557666190305 with std:10.727123679575346. The val loss is 10.018708329977594 with std:23.178211550663896.\n",
      "10.018708329977594 0.6468607661546327\n",
      "Evaluating for {'lmda': 0.6528521141127848} ...\n",
      "The training loss is 7.007351230245609 with std:13.02332190111039. The val loss is 10.9334623149086 with std:21.576119376594264.\n",
      "10.9334623149086 0.6528521141127848\n",
      "The training loss is 6.664726860123564 with std:11.280988009237715. The val loss is 16.84305344846634 with std:85.16927371724888.\n",
      "16.84305344846634 0.6528521141127848\n",
      "The training loss is 7.531525828185477 with std:14.112862116642702. The val loss is 7.326153469982078 with std:8.988393722214425.\n",
      "7.326153469982078 0.6528521141127848\n",
      "The training loss is 6.956855134048631 with std:10.738228097467559. The val loss is 10.024299112944364 with std:23.183453468078543.\n",
      "10.024299112944364 0.6528521141127848\n",
      "Evaluating for {'lmda': 0.658898955079995} ...\n",
      "The training loss is 7.01217363129535 with std:13.031802806190854. The val loss is 10.95291471035243 with std:21.703031200220202.\n",
      "10.95291471035243 0.658898955079995\n",
      "The training loss is 6.666611650613979 with std:11.284580025025484. The val loss is 16.855624043987223 with std:85.28289605462956.\n",
      "16.855624043987223 0.658898955079995\n",
      "The training loss is 7.537537063865553 with std:14.1235855289612. The val loss is 7.32684535074944 with std:8.99223816967868.\n",
      "7.32684535074944 0.658898955079995\n",
      "The training loss is 6.962149072603214 with std:10.749341718148475. The val loss is 10.029880210609992 with std:23.188783134861715.\n",
      "10.029880210609992 0.658898955079995\n",
      "Evaluating for {'lmda': 0.6650018030431118} ...\n",
      "The training loss is 7.0170047163176745 with std:13.04035048666133. The val loss is 10.972563302568103 with std:21.831951154414835.\n",
      "10.972563302568103 0.6650018030431118\n",
      "The training loss is 6.668496560524885 with std:11.28819926243139. The val loss is 16.868119472217437 with std:85.39589304064856.\n",
      "16.868119472217437 0.6650018030431118\n",
      "The training loss is 7.54355196049372 with std:14.134369960651954. The val loss is 7.327543645202086 with std:8.996102355141156.\n",
      "7.327543645202086 0.6650018030431118\n",
      "The training loss is 6.967439402821725 with std:10.760464340982244. The val loss is 10.03545136039382 with std:23.194200174439615.\n",
      "10.03545136039382 0.6650018030431118\n",
      "Evaluating for {'lmda': 0.6711611767496279} ...\n",
      "The training loss is 7.021844374950311 with std:13.048964689742883. The val loss is 10.992411465035694 with std:21.96291218126645.\n",
      "10.992411465035694 0.6711611767496279\n",
      "The training loss is 6.67038159616996 with std:11.29184583596069. The val loss is 16.880539226135333 with std:85.5082596050814.\n",
      "16.880539226135333 0.6711611767496279\n",
      "The training loss is 7.549570327244153 with std:14.145215127173433. The val loss is 7.328248130284568 with std:8.999985532634152.\n",
      "7.328248130284568 0.6711611767496279\n",
      "The training loss is 6.972726047164348 with std:10.771595767324522. The val loss is 10.04101230695072 with std:23.19970421261205.\n",
      "10.04101230695072 0.6711611767496279\n",
      "Evaluating for {'lmda': 0.6773775997517751} ...\n",
      "The training loss is 7.0266924965304955 with std:13.057645155390741. The val loss is 11.01246257837457 with std:22.095947354002643.\n",
      "11.01246257837457 0.6773775997517751\n",
      "The training loss is 6.672266764746054 with std:11.295519861278605. The val loss is 16.89288280380121 with std:85.61999071886046.\n",
      "16.89288280380121 0.6773775997517751\n",
      "The training loss is 7.555591974701722 with std:14.156120739697615. The val loss is 7.3289585833656865 with std:9.003886968264155.\n",
      "7.3289585833656865 0.6773775997517751\n",
      "The training loss is 6.978008929598907 with std:10.782735800580113. The val loss is 10.046562802221983 with std:23.205294877852268.\n",
      "10.046562802221983 0.6773775997517751\n",
      "Evaluating for {'lmda': 0.6836516004510238} ...\n",
      "The training loss is 7.03154897013072 with std:13.066391616376274. The val loss is 11.032720030364993 with std:22.23108987096934.\n",
      "11.032720030364993 0.6836516004510238\n",
      "The training loss is 6.674152074337346 with std:11.299221455232505. The val loss is 16.905149708349263 with std:85.73108139408205.\n",
      "16.905149708349263 0.6836516004510238\n",
      "The training loss is 7.561616714907761 with std:14.167086505228914. The val loss is 7.329674782321713 with std:9.007805940376754.\n",
      "7.329674782321713 0.6836516004510238\n",
      "The training loss is 6.9832879756169985 with std:10.793884246263993. The val loss is 10.052102605487493 with std:23.21097180161106.\n",
      "10.052102605487493 0.6836516004510238\n",
      "Evaluating for {'lmda': 0.6899837121430018} ...\n",
      "The training loss is 7.03641368459448 with std:13.075203798371756. The val loss is 11.053187215963515 with std:22.3683730495547.\n",
      "11.053187215963515 0.6899837121430018\n",
      "The training loss is 6.676037533918696 with std:11.302950735874447. The val loss is 16.917339447974115 with std:85.84152668397326.\n",
      "16.917339447974115 0.6899837121430018\n",
      "The training loss is 7.567644361405497 with std:14.178112126723343. The val loss is 7.330396505620525 with std:9.011741739717138.\n",
      "7.330396505620525 0.6899837121430018\n",
      "The training loss is 6.988563112248544 with std:10.805040912058717. The val loss is 10.057631483412797 with std:23.216734618615547.\n",
      "10.057631483412797 0.6899837121430018\n",
      "Evaluating for {'lmda': 0.6963744730628222} ...\n",
      "The training loss is 7.041286528572395 with std:13.084081420038508. The val loss is 11.073867537308097 with std:22.50783032003244.\n",
      "11.073867537308097 0.6963744730628222\n",
      "The training loss is 6.677923153359789 with std:11.306707822484471. The val loss is 16.929451535919316 with std:85.95132168286737.\n",
      "16.929451535919316 0.6963744730628222\n",
      "The training loss is 7.5736747292843924 with std:14.189197303208989. The val loss is 7.331123532404797 with std:9.015693669582296.\n",
      "7.331123532404797 0.6963744730628222\n",
      "The training loss is 6.993834268076946 with std:10.816205607873567. The val loss is 10.063149210097993 with std:23.222582967171626.\n",
      "10.063149210097993 0.6963744730628222\n",
      "Evaluating for {'lmda': 0.7028244264308345} ...\n",
      "The training loss is 7.046167390558496 with std:13.09302419311778. The val loss is 11.094764403737246 with std:22.649495219512605.\n",
      "11.094764403737246 0.7028244264308345\n",
      "The training loss is 6.679808943428425 with std:11.310492835592793. The val loss is 16.94148549046302 with std:86.06046152615968.\n",
      "16.94148549046302 0.7028244264308345\n",
      "The training loss is 7.579707635224328 with std:14.200341729908697. The val loss is 7.331855642575578 with std:9.019661045968245.\n",
      "7.331855642575578 0.7028244264308345\n",
      "The training loss is 6.999101373253351 with std:10.827378145902399. The val loss is 10.06865556712117 with std:23.228516489457515.\n",
      "10.06865556712117 0.7028244264308345\n",
      "Evaluating for {'lmda': 0.7093341204987996} ...\n",
      "The training loss is 7.051056158926428 with std:13.102031822524149. The val loss is 11.115881231764545 with std:22.793401385562202.\n",
      "11.115881231764545 0.7093341204987996\n",
      "The training loss is 6.6816949157943295 with std:11.31430589700347. The val loss is 16.95344083490173 with std:86.16894139024173.\n",
      "16.95344083490173 0.7093341204987996\n",
      "The training loss is 7.585742897539329 with std:14.211545098364091. The val loss is 7.332592616874918 with std:9.023643197709161.\n",
      "7.332592616874918 0.7093341204987996\n",
      "The training loss is 7.004364359510881 with std:10.838558340681447. The val loss is 10.074150343583055 with std:23.23453483182449.\n",
      "10.074150343583055 0.7093341204987996\n",
      "Evaluating for {'lmda': 0.7159041085964888} ...\n",
      "The training loss is 7.055952721966045 with std:13.111104006441938. The val loss is 11.137221445092152 with std:22.93958255014253.\n",
      "11.137221445092152 0.7159041085964888\n",
      "The training loss is 6.6835810830325295 with std:11.318147129817417. The val loss is 16.965317097533443 with std:86.27675649243506.\n",
      "16.965317097533443 0.7159041085964888\n",
      "The training loss is 7.591780336220157 with std:14.222807096560395. The val loss is 7.333334236968632 with std:9.027639466610614.\n",
      "7.333334236968632 0.7159041085964888\n",
      "The training loss is 7.009623160178571 with std:10.849746009146754. The val loss is 10.079633336148605 with std:23.240637645089148.\n",
      "10.079633336148605 0.7159041085964888\n",
      "Evaluating for {'lmda': 0.7225349491787214} ...\n",
      "The training loss is 7.060856967920099 with std:13.12024043642505. The val loss is 11.158788474587215 with std:23.08807253328925.\n",
      "11.158788474587215 0.7225349491787214\n",
      "The training loss is 6.6854674586269365 with std:11.322016658456167. The val loss is 16.977113811639317 with std:86.3839020909062.\n",
      "16.977113811639317 0.7225349491787214\n",
      "The training loss is 7.597819772976396 with std:14.234127409052217. The val loss is 7.334080285528362 with std:9.031649207575523.\n",
      "7.334080285528362 0.7225349491787214\n",
      "The training loss is 7.014877710194653 with std:10.860940970690724. The val loss is 10.085104349086635 with std:23.246824584825767.\n",
      "10.085104349086635 0.7225349491787214\n",
      "Evaluating for {'lmda': 0.7292272058728313} ...\n",
      "The training loss is 7.065768785021021 with std:13.129440797498598. The val loss is 11.180585758255972 with std:23.238905236798047.\n",
      "11.180585758255972 0.7292272058728313\n",
      "The training loss is 6.68735405697372 with std:11.325914608685025. The val loss is 16.988830515463746 with std:86.49037348456791.\n",
      "16.988830515463746 0.7292272058728313\n",
      "The training loss is 7.603861031278194 with std:14.245505717091275. The val loss is 7.3348305463135945 with std:9.035671788725038.\n",
      "7.3348305463135945 0.7292272058728313\n",
      "The training loss is 7.020127946119913 with std:10.872143047218975. The val loss is 10.090563194308636 with std:23.253095311660214.\n",
      "10.090563194308636 0.7292272058728313\n",
      "Evaluating for {'lmda': 0.7359814475265763} ...\n",
      "The training loss is 7.07068806152759 with std:13.138704768263684. The val loss is 11.202616741225558 with std:23.392114638015894.\n",
      "11.202616741225558 0.7359814475265763\n",
      "The training loss is 6.689240893384696 with std:11.329841107637657. The val loss is 17.00046675219321 with std:86.5961660129726.\n",
      "17.00046675219321 0.7359814475265763\n",
      "The training loss is 7.609903936397382 with std:14.256941698755412. The val loss is 7.335584804252715 with std:9.039706591511239.\n",
      "7.335584804252715 0.7359814475265763\n",
      "The training loss is 7.0253738061507836 with std:10.883352063206608. The val loss is 10.09600969140507 with std:23.259449491555433.\n",
      "10.09600969140507 0.7359814475265763\n",
      "Evaluating for {'lmda': 0.7427982482564919} ...\n",
      "The training loss is 7.075614685762244 with std:13.148032021004964. The val loss is 11.224884875695595 with std:23.547734783441616.\n",
      "11.224884875695595 0.7427982482564919\n",
      "The training loss is 6.691127984090711 with std:11.333796283839565. The val loss is 17.012022069934932 with std:86.70127505620033.\n",
      "17.012022069934932 0.7427982482564919\n",
      "The training loss is 7.615948315447397 with std:14.268435029077228. The val loss is 7.336342845524506 with std:9.043753010825117.\n",
      "7.336342845524506 0.7427982482564919\n",
      "The training loss is 7.03061523013135 with std:10.894567845752686. The val loss is 10.101443667679542 with std:23.265886796098155.\n",
      "10.101443667679542 0.7427982482564919\n",
      "Evaluating for {'lmda': 0.7496781874966877} ...\n",
      "The training loss is 7.080548546147592 with std:13.157422221800031. The val loss is 11.24739362090837 with std:23.70579978252394.\n",
      "11.24739362090837 0.7496781874966877\n",
      "The training loss is 6.693015346244912 with std:11.337780267232608. The val loss is 17.02349602169081 with std:86.80569603470353.\n",
      "17.02349602169081 0.7496781874966877\n",
      "The training loss is 7.621993997422859 with std:14.279985380174404. The val loss is 7.337104457637896 with std:9.047810455096117.\n",
      "7.337104457637896 0.7496781874966877\n",
      "The training loss is 7.035852159566191 with std:10.905790224636636. The val loss is 10.106864958182932 with std:23.272406902784937.\n",
      "10.106864958182932 0.7496781874966877\n",
      "Evaluating for {'lmda': 0.7566218500481054} ...\n",
      "The training loss is 7.085489531244109 with std:13.166875030632603. The val loss is 11.270146443091656 with std:23.86634380130393.\n",
      "11.270146443091656 0.7566218500481054\n",
      "The training loss is 6.694902997926155 with std:11.3417931891997. The val loss is 17.03488816533537 with std:86.90942440918975.\n",
      "17.03488816533537 0.7566218500481054\n",
      "The training loss is 7.628040813238917 with std:14.291592421381958. The val loss is 7.337869429512845 with std:9.051878346388182.\n",
      "7.337869429512845 0.7566218500481054\n",
      "The training loss is 7.041084537632049 with std:10.917019032372268. The val loss is 10.112273405743442 with std:23.279009495300635.\n",
      "10.112273405743442 0.7566218500481054\n",
      "Evaluating for {'lmda': 0.7636298261282242} ...\n",
      "The training loss is 7.0904375297868345 with std:13.176390101506858. The val loss is 11.293146815398687 with std:24.02940105610609.\n",
      "11.293146815398687 0.7636298261282242\n",
      "The training loss is 6.696790958142226 with std:11.345835182589106. The val loss is 17.04619806358748 with std:87.01245568044591.\n",
      "17.04619806358748 0.7636298261282242\n",
      "The training loss is 7.634088595769271 with std:14.30325581938389. The val loss is 7.338637551559053 with std:9.055956120486847.\n",
      "7.338637551559053 0.7636298261282242\n",
      "The training loss is 7.046312309189703 with std:10.928254104261867. The val loss is 10.117668860996673 with std:23.285694263797836.\n",
      "10.117668860996673 0.7636298261282242\n",
      "Evaluating for {'lmda': 0.7707027114212296} ...\n",
      "The training loss is 7.095392430722837 with std:13.185967082565153. The val loss is 11.316398217853765 with std:24.195005807346114.\n",
      "11.316398217853765 0.7707027114212296\n",
      "The training loss is 6.698679246833094 with std:11.34990638173989. The val loss is 17.05742528398548 with std:87.11478538919468.\n",
      "17.05742528398548 0.7707027114212296\n",
      "The training loss is 7.640137179883586 with std:14.314975238345424. The val loss is 7.33940861575533 with std:9.060043226981794.\n",
      "7.33940861575533 0.7707027114212296\n",
      "The training loss is 7.05153542079507 with std:10.939495278450014. The val loss is 10.123051182413324 with std:23.2924609051736.\n",
      "10.123051182413324 0.7707027114212296\n",
      "Evaluating for {'lmda': 0.7778411071286491} ...\n",
      "The training loss is 7.10035412324847 with std:13.195605616207937. The val loss is 11.339904137262845 with std:24.36319235313997.\n",
      "11.339904137262845 0.7778411071286491\n",
      "The training loss is 6.7005678848743075 with std:11.354006922506688. The val loss is 17.06856939885684 with std:87.21640911589583.\n",
      "17.06856939885684 0.7778411071286491\n",
      "The training loss is 7.6461864024847035 with std:14.326750340047726. The val loss is 7.340182415727705 with std:9.0641391293426.\n",
      "7.340182415727705 0.7778411071286491\n",
      "The training loss is 7.056753820710468 with std:10.950742395976308. The val loss is 10.128420236324457 with std:23.299309123339178.\n",
      "10.128420236324457 0.7778411071286491\n",
      "Evaluating for {'lmda': 0.785045620020451} ...\n",
      "The training loss is 7.105322496846454 with std:13.205305339215215. The val loss is 11.363668067146723 with std:24.533995023185543.\n",
      "11.363668067146723 0.785045620020451\n",
      "The training loss is 6.70245689408014 with std:11.358136942285364. The val loss is 17.079629985289316 with std:87.31732248056842.\n",
      "17.079629985289316 0.785045620020451\n",
      "The training loss is 7.6522361025446894 with std:14.338580784021083. The val loss is 7.340958746827378 with std:9.068243304988984.\n",
      "7.340958746827378 0.785045620020451\n",
      "The training loss is 7.061967458915388 with std:10.961995300828294. The val loss is 10.13377589694676 with std:23.306238629492885.\n",
      "10.13377589694676 0.785045620020451\n",
      "Evaluating for {'lmda': 0.7923168624866254} ...\n",
      "The training loss is 7.110297441323444 with std:13.21506588287143. The val loss is 11.387693507648562 with std:24.707448172527197.\n",
      "11.387693507648562 0.7923168624866254\n",
      "The training loss is 6.70434629720696 with std:11.362296580038574. The val loss is 17.09060662510173 with std:87.41752114260262.\n",
      "17.09060662510173 0.7923168624866254\n",
      "The training loss is 7.658286121140363 with std:14.350466227680194. The val loss is 7.341737406207895 with std:9.072355245354714.\n",
      "7.341737406207895 0.7923168624866254\n",
      "The training loss is 7.067176287117176 with std:10.973253839993827. The val loss is 10.139118046404349 with std:23.313249142383768.\n",
      "10.139118046404349 0.7923168624866254\n",
      "Evaluating for {'lmda': 0.7996554525892346} ...\n",
      "The training loss is 7.1152788468470884 with std:13.224886873091664. The val loss is 11.411983965431324 with std:24.883586175346217.\n",
      "11.411983965431324 0.7996554525892346\n",
      "The training loss is 6.706236117956519 with std:11.366485976321723. The val loss is 17.101498904810814 with std:87.51700080053647.\n",
      "17.101498904810814 0.7996554525892346\n",
      "The training loss is 7.664336301487939 with std:14.362406326458803. The val loss is 7.342518192901257 with std:9.076474455944853.\n",
      "7.342518192901257 0.7996554525892346\n",
      "The training loss is 7.072380258760797 with std:10.984517863511266. The val loss is 10.144446574749331 with std:23.320340388575204.\n",
      "10.144446574749331 0.7996554525892346\n",
      "Evaluating for {'lmda': 0.8070620141149499} ...\n",
      "The training loss is 7.120266603983495 with std:13.234767930550358. The val loss is 11.436542953579426 with std:25.062443418885948.\n",
      "11.436542953579426 0.8070620141149499\n",
      "The training loss is 6.708126380979301 with std:11.370705273309271. The val loss is 17.112306415600123 with std:87.6157571918522.\n",
      "17.112306415600123 0.8070620141149499\n",
      "The training loss is 7.670386488977691 with std:14.374400733946059. The val loss is 7.343300907894439 with std:9.080600456389304.\n",
      "7.343300907894439 0.8070620141149499\n",
      "The training loss is 7.077579329039307 with std:10.995787224521784. The val loss is 10.149761379981312 with std:23.327512102704716.\n",
      "10.149761379981312 0.8070620141149499\n",
      "Evaluating for {'lmda': 0.8145371766280746} ...\n",
      "The training loss is 7.1252606037345405 with std:13.24470867081228. The val loss is 11.461373991483615 with std:25.244054297357806.\n",
      "11.461373991483615 0.8145371766280746\n",
      "The training loss is 6.710017111877855 with std:11.374954614821085. The val loss is 17.123028753285556 with std:87.7137860927396.\n",
      "17.123028753285556 0.8145371766280746\n",
      "The training loss is 7.676436531206654 with std:14.386449102021631. The val loss is 7.344085354203996 with std:9.084732780488045.\n",
      "7.344085354203996 0.8145371766280746\n",
      "The training loss is 7.082773454903303 with std:11.00706177931866. The val loss is 10.155062368064156 with std:23.334764027738448.\n",
      "10.155062368064156 0.8145371766280746\n",
      "Evaluating for {'lmda': 0.822081575524054} ...\n",
      "The training loss is 7.130260737574948 with std:13.254708704464901. The val loss is 11.486480604716743 with std:25.428453205886164.\n",
      "11.486480604716743 0.822081575524054\n",
      "The training loss is 6.711908337210144 with std:11.379234146348931. The val loss is 17.1336655182819 with std:87.81108331786903.\n",
      "17.1336655182819 0.822081575524054\n",
      "The training loss is 7.682486278011882 with std:14.39855108099275. The val loss is 7.344871336950825 with std:9.088870976252219.\n",
      "7.344871336950825 0.822081575524054\n",
      "The training loss is 7.087962595070355 with std:11.018341387397598. The val loss is 10.160349452941816 with std:23.34209591522434.\n",
      "10.160349452941816 0.822081575524054\n",
      "Evaluating for {'lmda': 0.8296958520834906} ...\n",
      "The training loss is 7.135266897489622 with std:13.264767637252923. The val loss is 11.511866324906114 with std:25.61567453455594.\n",
      "11.511866324906114 0.8296958520834906\n",
      "The training loss is 6.713800084493075 with std:11.383544015083842. The val loss is 17.14421631556699 with std:87.90764472013896.\n",
      "17.14421631556699 0.8296958520834906\n",
      "The training loss is 7.688535581501959 with std:14.410706319729302. The val loss is 7.345658663433797 with std:9.093014605939262.\n",
      "7.345658663433797 0.8296958520834906\n",
      "The training loss is 7.09314671003425 with std:11.029625911505372. The val loss is 10.165622556552409 with std:23.349507525540595.\n",
      "10.165622556552409 0.8296958520834906\n",
      "Evaluating for {'lmda': 0.8373806535266489} ...\n",
      "The training loss is 7.140278976011043 with std:13.274885070215499. The val loss is 11.537534689597425 with std:25.80575266251034.\n",
      "11.537534689597425 0.8373806535266489\n",
      "The training loss is 6.715692382205848 with std:11.387884369943253. The val loss is 17.154680754645387 with std:88.0034661904234.\n",
      "17.154680754645387 0.8373806535266489\n",
      "The training loss is 7.694584296088802 with std:14.422914465802027. The val loss is 7.34644714320302 with std:9.09716324608247.\n",
      "7.34644714320302 0.8373806535266489\n",
      "The training loss is 7.098325762074108 with std:11.040915217689149. The val loss is 10.170881608840098 with std:23.356998628139106.\n",
      "10.170881608840098 0.8373806535266489\n",
      "Evaluating for {'lmda': 0.8451366330684713} ...\n",
      "The training loss is 7.145296866256158 with std:13.285060599824384. The val loss is 11.56348924210432 with std:25.99872195207117.\n",
      "11.56348924210432 0.8451366330684713\n",
      "The training loss is 6.717585259793734 with std:11.392255361598686. The val loss is 17.16505844951293 with std:88.09854365732045.\n",
      "17.16505844951293 0.8451366330684713\n",
      "The training loss is 7.700632278517865 with std:14.43517516561807. The val loss is 7.347236588131656 with std:9.101316487514264.\n",
      "7.347236588131656 0.8451366330684713\n",
      "The training loss is 7.103499715262714 with std:11.052209175343373. The val loss is 10.176126547765678 with std:23.364569001787032.\n",
      "10.176126547765678 0.8451366330684713\n",
      "Evaluating for {'lmda': 0.8529644499741025} ...\n",
      "The training loss is 7.150320461963475 with std:13.29529381812346. The val loss is 11.589733531354563 with std:26.19461674296241.\n",
      "11.589733531354563 0.8529644499741025\n",
      "The training loss is 6.7194787476713635 with std:11.396657142503267. The val loss is 17.175349018617535 with std:88.19287308686886.\n",
      "17.175349018617535 0.8529644499741025\n",
      "The training loss is 7.706679387898281 with std:14.447488064558321. The val loss is 7.348026812488006 with std:9.10547393538602.\n",
      "7.348026812488006 0.8529644499741025\n",
      "The training loss is 7.108668535475129 with std:11.063507657257503. The val loss is 10.181357319315353 with std:23.372218434801777.\n",
      "10.181357319315353 0.8529644499741025\n",
      "Evaluating for {'lmda': 0.8608647696149244} ...\n",
      "The training loss is 7.155349657530411 with std:13.305584312871616. The val loss is 11.616271111729969 with std:26.393471346628665.\n",
      "11.616271111729969 0.8608647696149244\n",
      "The training loss is 6.721372877226607 with std:11.401089866920426. The val loss is 17.185552084820394 with std:88.28645048226683.\n",
      "17.185552084820394 0.8608647696149244\n",
      "The training loss is 7.7127254857316805 with std:14.459852807113224. The val loss is 7.3488176330055435 with std:9.10963520917971.\n",
      "7.3488176330055435 0.8608647696149244\n",
      "The training loss is 7.113832190397207 with std:11.074810539663192. The val loss is 10.186573877508119 with std:23.379946725283986.\n",
      "10.186573877508119 0.8608647696149244\n",
      "Evaluating for {'lmda': 0.8688382635251184} ...\n",
      "The training loss is 7.160384348049813 with std:13.315931667685406. The val loss is 11.643105542882573 with std:26.595320040494602.\n",
      "11.643105542882573 0.8688382635251184\n",
      "The training loss is 6.723267680824232 with std:11.405553690951805. The val loss is 17.19566727535926 with std:88.37927188360995.\n",
      "17.19566727535926 0.8688382635251184\n",
      "The training loss is 7.71877043594101 with std:14.472269037020203. The val loss is 7.349608868953462 with std:9.113799942717359.\n",
      "7.349608868953462 0.8688382635251184\n",
      "The training loss is 7.118990649533267 with std:11.086117702279587. The val loss is 10.191776184401576 with std:23.387753681347313.\n",
      "10.191776184401576 0.8688382635251184\n",
      "Evaluating for {'lmda': 0.8768856094587426} ...\n",
      "The training loss is 7.165424429347162 with std:13.326335462184442. The val loss is 11.67024038957244 with std:26.800197062559466.\n",
      "11.67024038957244 0.8768856094587426\n",
      "The training loss is 6.725163191809786 with std:11.410048772566835. The val loss is 17.205694221805032 with std:88.47133336756222.\n",
      "17.205694221805032 0.8768856094587426\n",
      "The training loss is 7.724814104898187 with std:14.484736397399711. The val loss is 7.350400342205189 with std:9.117967784162449.\n",
      "7.350400342205189 0.8768856094587426\n",
      "The training loss is 7.124143884213995 with std:11.097429028358986. The val loss is 10.196964210095455 with std:23.39563912133934.\n",
      "10.196964210095455 0.8768856094587426\n",
      "Evaluating for {'lmda': 0.8850074914473438} ...\n",
      "The training loss is 7.170469798017192 with std:13.336795272137724. The val loss is 11.697679221461025 with std:27.008136605766303.\n",
      "11.697679221461025 0.8850074914473438\n",
      "The training loss is 6.7270594445135234 with std:11.414575271631199. The val loss is 17.215632560024503 with std:88.56263104708732.\n",
      "17.215632560024503 0.8850074914473438\n",
      "The training loss is 7.730856361451014 with std:14.497254530891007. The val loss is 7.351191877306935 with std:9.122138396018517.\n",
      "7.351191877306935 0.8850074914473438\n",
      "The training loss is 7.129291867604249 with std:11.108744404731942. The val loss is 10.202137932734622 with std:23.403602874063512.\n",
      "10.202137932734622 0.8850074914473438\n",
      "Evaluating for {'lmda': 0.8932045998580969} ...\n",
      "The training loss is 7.17552035146065 with std:13.347310669611673. The val loss is 11.72542561292191 with std:27.219172812674756.\n",
      "11.72542561292191 0.8932045998580969\n",
      "The training loss is 6.728956474254332 with std:11.419133349936525. The val loss is 17.225481930137416 with std:88.65316107112282.\n",
      "17.225481930137416 0.8932045998580969\n",
      "The training loss is 7.736897076949706 with std:14.5098230797887. The val loss is 7.351983301545135 with std:9.126311455121378.\n",
      "7.351983301545135 0.8932045998580969\n",
      "The training loss is 7.134434574710134 with std:11.120063721850586. The val loss is 10.207297338508953 with std:23.411644778991967.\n",
      "10.207297338508953 0.8932045998580969\n",
      "Evaluating for {'lmda': 0.9014776314524917} ...\n",
      "The training loss is 7.180575987921041 with std:13.35788122311985. The val loss is 11.753483142836158 with std:27.433339770154163.\n",
      "11.753483142836158 0.9014776314524917\n",
      "The training loss is 6.730854317343936 with std:11.423723171230469. The val loss is 17.2352419764744 with std:88.74291962426496.\n",
      "17.2352419764744 0.9014776314524917\n",
      "The training loss is 7.742936125272534 with std:14.522441686177954. The val loss is 7.35277444501269 with std:9.130486652626512.\n",
      "7.35277444501269 0.9014776314524917\n",
      "The training loss is 7.13957198238655 with std:11.131386873833053. The val loss is 10.21244242165366 with std:23.419764686478015.\n",
      "10.21244242165366 0.9014776314524917\n",
      "Evaluating for {'lmda': 0.9098272894455558} ...\n",
      "The training loss is 7.185636606520714 with std:13.368506497772561. The val loss is 11.781855394367925 with std:27.650671504078478.\n",
      "11.781855394367925 0.9098272894455558\n",
      "The training loss is 6.732753011091107 with std:11.42834490124651. The val loss is 17.244912347536218 with std:88.83190292645963.\n",
      "17.244912347536218 0.9098272894455558\n",
      "The training loss is 7.748973382850662 with std:14.535109992069348. The val loss is 7.3535651406746085 with std:9.134663693991916.\n",
      "7.3535651406746085 0.9098272894455558\n",
      "The training loss is 7.144704069344119 with std:11.142713758505758. The val loss is 10.217573184446636 with std:23.42796245796202.\n",
      "10.217573184446636 0.9098272894455558\n",
      "Evaluating for {'lmda': 0.9182542835656282} ...\n",
      "The training loss is 7.190702107297706 with std:13.37918605542971. The val loss is 11.810545954748125 with std:27.871201974234914.\n",
      "11.810545954748125 0.9182542835656282\n",
      "The training loss is 6.734652593806004 with std:11.432998707734876. The val loss is 17.254492695949306 with std:88.92010723265949.\n",
      "17.254492695949306 0.9182542835656282\n",
      "The training loss is 7.7550087286928315 with std:14.54782763953457. The val loss is 7.3543552244331 with std:9.138842298956481.\n",
      "7.3543552244331 0.9182542835656282\n",
      "The training loss is 7.149830816156041 with std:11.154044277445669. The val loss is 10.222689637204672 with std:23.436237966172516.\n",
      "10.222689637204672 0.9182542835656282\n",
      "Evaluating for {'lmda': 0.9267593301146884} ...\n",
      "The training loss is 7.195772391241777 with std:13.389919454853283. The val loss is 11.83955841504256 with std:28.094965069276313.\n",
      "11.83955841504256 0.9267593301146884\n",
      "The training loss is 6.736553104804712 with std:11.437684760493239. The val loss is 17.26398267842369 with std:89.00752883250338.\n",
      "17.26398267842369 0.9267593301146884\n",
      "The training loss is 7.7610420444083825 with std:14.560594270839124. The val loss is 7.355144535190992 with std:9.143022201512334.\n",
      "7.355144535190992 0.9267593301146884\n",
      "The training loss is 7.154952205264991 with std:11.165378336022103. The val loss is 10.227791798278426 with std:23.44459109532342.\n",
      "10.227791798278426 0.9267593301146884\n",
      "Evaluating for {'lmda': 0.9353431520292387} ...\n",
      "The training loss is 7.200847360330323 with std:13.400706251860981. The val loss is 11.868896369906032 with std:28.321994601703285.\n",
      "11.868896369906032 0.9353431520292387\n",
      "The training loss is 6.738454584413816 with std:11.442403231397911. The val loss is 17.27338195570883 with std:89.0941640499729.\n",
      "17.27338195570883 0.9353431520292387\n",
      "The training loss is 7.7670732142307415 with std:14.573409528577551. The val loss is 7.355932914914841 with std:9.147203149873958.\n",
      "7.355932914914841 0.9353431520292387\n",
      "The training loss is 7.160068220989288 with std:11.17671584343669. The val loss is 10.232879694045248 with std:23.453021741306014.\n",
      "10.232879694045248 0.9353431520292387\n",
      "Evaluating for {'lmda': 0.9440064789417604} ...\n",
      "The training loss is 7.205926917565034 with std:13.411545999482687. The val loss is 11.898563417330315 with std:28.55232430296636.\n",
      "11.898563417330315 0.9440064789417604\n",
      "The training loss is 6.740357073975108 with std:11.447154294435691. The val loss is 17.282690192548948 with std:89.18000924304403.\n",
      "17.282690192548948 0.9440064789417604\n",
      "The training loss is 7.773102125039367 with std:14.586273055805284. The val loss is 7.35672020869694 with std:9.151384906442152.\n",
      "7.35672020869694 0.9440064789417604\n",
      "The training loss is 7.165178849529856 with std:11.188056712764796. The val loss is 10.237953358901649 with std:23.46152981187721.\n",
      "10.237953358901649 0.9440064789417604\n",
      "Evaluating for {'lmda': 0.952750047242729} ...\n",
      "The training loss is 7.2110109670069376 with std:13.42243824811469. The val loss is 11.928563158394464 with std:28.785987818739073.\n",
      "11.928563158394464 0.952750047242729\n",
      "The training loss is 6.742260615850591 with std:11.451938125735879. The val loss is 17.291907057639023 with std:89.2650608033399.\n",
      "17.291907057639023 0.952750047242729\n",
      "The training loss is 7.779128666381523 with std:14.59918449617169. The val loss is 7.35750626481628 with std:9.155567247763045.\n",
      "7.35750626481628 0.952750047242729\n",
      "The training loss is 7.170284078976212 with std:11.1994008609945. The val loss is 10.243012835253772 with std:23.470115226843422.\n",
      "10.243012835253772 0.952750047242729\n",
      "Evaluating for {'lmda': 0.9615746001432096} ...\n",
      "The training loss is 7.216099413812742 with std:13.433382545677837. The val loss is 11.958899196988897 with std:29.023018704140707.\n",
      "11.958899196988897 0.9615746001432096\n",
      "The training loss is 6.744165253427494 with std:11.456754903602658. The val loss is 17.301032223580023 with std:89.34931515578157.\n",
      "17.301032223580023 0.9615746001432096\n",
      "The training loss is 7.785152730493179 with std:14.61214349405152. The val loss is 7.358290934798664 with std:9.159749964483808.\n",
      "7.358290934798664 0.9615746001432096\n",
      "The training loss is 7.175383899312671 with std:11.210748209065152. The val loss is 10.24805817350557 with std:23.478777918235455.\n",
      "10.24805817350557 0.9615746001432096\n",
      "Evaluating for {'lmda': 0.9704808877380307} ...\n",
      "The training loss is 7.221192164270264 with std:13.444378437775157. The val loss is 11.989575139543396 with std:29.263450419140717.\n",
      "11.989575139543396 0.9704808877380307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 6.746071031123498 with std:11.461604808548024. The val loss is 17.31006536683339 with std:89.43276875822158.\n",
      "17.31006536683339 0.9704808877380307\n",
      "The training loss is 7.791174212319362 with std:14.62514969467597. The val loss is 7.359074073475882 with std:9.163932861302735.\n",
      "7.359074073475882 0.9704808877380307\n",
      "The training loss is 7.180478302424548 with std:11.222098681906317. The val loss is 10.253089432047782 with std:23.487517830486677.\n",
      "10.253089432047782 0.9704808877380307\n",
      "Evaluating for {'lmda': 0.9794696670695395} ...\n",
      "The training loss is 7.226289125833745 with std:13.455425467849665. The val loss is 12.020594594746749 with std:29.50731632406426.\n",
      "12.020594594746749 0.9794696670695395\n",
      "The training loss is 6.747977994392195 with std:11.466488023325173. The val loss is 17.319006167675212 with std:89.51541810107969.\n",
      "17.319006167675212 0.9794696670695395\n",
      "The training loss is 7.797193009533849 with std:14.638202744262886. The val loss is 7.3598555390439 with std:9.168115756916347.\n",
      "7.3598555390439 0.9794696670695395\n",
      "The training loss is 7.18556728210411 with std:11.233452208475038. The val loss is 10.258106677243761 with std:23.496334920598905.\n",
      "10.258106677243761 0.9794696670695395\n",
      "Evaluating for {'lmda': 0.9885417021919574} ...\n",
      "The training loss is 7.231390207159174 with std:13.466523177344083. The val loss is 12.051961173237238 with std:29.754649674998543.\n",
      "12.051961173237238 0.9885417021919574\n",
      "The training loss is 6.749886189728518 with std:11.471404732961933. The val loss is 17.327854310151295 with std:89.59725970698265.\n",
      "17.327854310151295 0.9885417021919574\n",
      "The training loss is 7.803209022557956 with std:14.651302290145349. The val loss is 7.360635193119946 with std:9.172298483961013.\n",
      "7.360635193119946 0.9885417021919574\n",
      "The training loss is 7.190650834056122 with std:11.24480872179223. The val loss is 10.26310998341404 with std:23.505229158306594.\n",
      "10.26310998341404 0.9885417021919574\n",
      "Evaluating for {'lmda': 0.9976977642363202} ...\n",
      "The training loss is 7.236495318139628 with std:13.477671105860727. The val loss is 12.083678487321768 with std:30.005483619602213.\n",
      "12.083678487321768 0.9976977642363202\n",
      "The training loss is 6.751795664674733 with std:11.47635512479507. The val loss is 17.336609482030624 with std:89.67829013038276.\n",
      "17.336609482030624 0.9976977642363202\n",
      "The training loss is 7.809222154579373 with std:14.664447980901116. The val loss is 7.361412900799054 with std:9.17648088895203.\n",
      "7.361412900799054 0.9976977642363202\n",
      "The training loss is 7.195728955904314 with std:11.256168158980207. The val loss is 10.26809943282109 with std:23.51420052623715.\n",
      "10.26809943282109 0.9976977642363202\n",
      "Evaluating for {'lmda': 1.0069386314760271} ...\n",
      "The training loss is 7.241604369940086 with std:13.488868791321442. The val loss is 12.11575015065067 with std:30.25985119269348.\n",
      "12.11575015065067 1.0069386314760271\n",
      "The training loss is 6.753706467826215 with std:11.481339388504912. The val loss is 17.34527137475992 with std:89.75850595719535.\n",
      "17.34527137475992 1.0069386314760271\n",
      "The training loss is 7.815232311569714 with std:14.67763946647951. The val loss is 7.362188530708816 with std:9.180662832216957.\n",
      "7.362188530708816 1.0069386314760271\n",
      "The training loss is 7.200801647196443 with std:11.267530461297133. The val loss is 10.273075115651558 with std:23.5232490200659.\n",
      "10.273075115651558 1.0069386314760271\n",
      "Evaluating for {'lmda': 1.0162650893929952} ...\n",
      "The training loss is 7.246717275032172 with std:13.500115770128543. The val loss is 12.148179777903511 with std:30.517785312084396.\n",
      "12.148179777903511 1.0162650893929952\n",
      "The training loss is 6.755618648837666 with std:11.486357716150051. The val loss is 17.353839683415714 with std:89.83790380439832.\n",
      "17.353839683415714 1.0162650893929952\n",
      "The training loss is 7.821239402301802 with std:14.690876398328022. The val loss is 7.362961955064059 with std:9.184844187826705.\n",
      "7.362961955064059 1.0162650893929952\n",
      "The training loss is 7.205868909410425 with std:11.278895574172676. The val loss is 10.278037129997006 with std:23.532374648662387.\n",
      "10.278037129997006 1.0162650893929952\n",
      "Evaluating for {'lmda': 1.025677930744422} ...\n",
      "The training loss is 7.25183394722897 with std:13.511411577325912. The val loss is 12.180970984460727 with std:30.77931877444171.\n",
      "12.180970984460727 1.025677930744422\n",
      "The training loss is 6.757532258429575 with std:11.491410302203247. The val loss is 17.362314106660545 with std:89.91648031967797.\n",
      "17.362314106660545 1.025677930744422\n",
      "The training loss is 7.8272433383661335 with std:14.704158429517246. The val loss is 7.36373304971991 with std:9.18902484352144.\n",
      "7.36373304971991 1.025677930744422\n",
      "The training loss is 7.2109307459597245 with std:11.290263447242273. The val loss is 10.282985581835218 with std:23.541577434240303.\n",
      "10.282985581835218 1.025677930744422\n",
      "Evaluating for {'lmda': 1.0351779556301763} ...\n",
      "The training loss is 7.256954301719493 with std:13.52275574676111. The val loss is 12.21412738606407 with std:31.044484251202896.\n",
      "12.21412738606407 1.0351779556301763\n",
      "The training loss is 6.7594473483946995 with std:11.496497343586999. The val loss is 17.370694346693856 with std:89.99423218101546.\n",
      "17.370694346693856 1.0351779556301763\n",
      "The training loss is 7.833244034187291 with std:14.717485214866722. The val loss is 7.364501694224046 with std:9.193204700633625.\n",
      "7.364501694224046 1.0351779556301763\n",
      "The training loss is 7.21598716219902 with std:11.301634034380589. The val loss is 10.287920585008514 with std:23.5508574124943.\n",
      "10.287920585008514 1.0351779556301763\n",
      "Evaluating for {'lmda': 1.0447659715608042} ...\n",
      "The training loss is 7.26207825510301 with std:13.53414781124671. The val loss is 12.247652598476524 with std:31.313314284629897.\n",
      "12.247652598476524 1.0447659715608042\n",
      "The training loss is 6.76136397160494 with std:11.501619039710107. The val loss is 17.378980109207923 with std:90.07115609632649.\n",
      "17.378980109207923 1.0447659715608042\n",
      "The training loss is 7.839241407039088 with std:14.730856411067. The val loss is 7.3652677718681 with std:9.197383674007133.\n",
      "7.3652677718681 1.0447659715608042\n",
      "The training loss is 7.2210381654297775 with std:11.313007293734984. The val loss is 10.292842261201608 with std:23.560214632736006.\n",
      "10.292842261201608 1.0447659715608042\n",
      "Evaluating for {'lmda': 1.0544427935261684} ...\n",
      "The training loss is 7.267205725423047 with std:13.54558730272221. The val loss is 12.281550237122474 with std:31.58584128383735.\n",
      "12.281550237122474 1.0544427935261684\n",
      "The training loss is 6.763282182018389 with std:11.506775592504564. The val loss is 17.387171103339305 with std:90.14724880304767.\n",
      "17.387171103339305 1.0544427935261684\n",
      "The training loss is 7.845235377059304 with std:14.744271676801718. The val loss is 7.36603116973748 with std:9.201561691912142.\n",
      "7.36603116973748 1.0544427935261684\n",
      "The training loss is 7.226083764905721 with std:11.324383187757782. The val loss is 10.297750739919023 with std:23.569649158026106.\n",
      "10.297750739919023 1.0544427935261684\n",
      "Evaluating for {'lmda': 1.0642092440647246} ...\n",
      "The training loss is 7.272336632201692 with std:13.557073752417024. The val loss is 12.3158239167299 with std:31.86209752097359.\n",
      "12.3158239167299 1.0642092440647246\n",
      "The training loss is 6.765202034686664 with std:11.511967206462648. The val loss is 17.39526704162399 with std:90.22250706776094.\n",
      "17.39526704162399 1.0642092440647246\n",
      "The training loss is 7.851225867264625 with std:14.757730672869998. The val loss is 7.3667917787612005 with std:9.205738695957821.\n",
      "7.3667917787612005 1.0642092440647246\n",
      "The training loss is 7.231123971838507 with std:11.33576168323811. The val loss is 10.302646158460181 with std:23.579161065298383.\n",
      "10.302646158460181 1.0642092440647246\n",
      "Evaluating for {'lmda': 1.0740661533334335} ...\n",
      "The training loss is 7.277470896473087 with std:13.568606691012226. The val loss is 12.35047725096016 with std:32.14211512743594.\n",
      "12.35047725096016 1.0740661533334335\n",
      "The training loss is 6.767123585762337 with std:11.517194088675165. The val loss is 17.40326763994961 with std:90.29692768578742.\n",
      "17.40326763994961 1.0740661533334335\n",
      "The training loss is 7.8572128035641375 with std:14.77123306230529. The val loss is 7.3675494937596975 with std:9.209914641000601.\n",
      "7.3675494937596975 1.0740661533334335\n",
      "The training loss is 7.23615879940319 with std:11.347142751333326. The val loss is 10.307528661894965 with std:23.588750445483374.\n",
      "10.307528661894965 1.0740661533334335\n",
      "Evaluating for {'lmda': 1.0840143591783309} ...\n",
      "The training loss is 7.282608440817398 with std:13.580185648803223. The val loss is 12.385513852022598 with std:32.425926090076395.\n",
      "12.385513852022598 1.0840143591783309\n",
      "The training loss is 6.769046892506893 with std:11.522456448869585. The val loss is 17.411172617510317 with std:90.37050748080141.\n",
      "17.411172617510317 1.0840143591783309\n",
      "The training loss is 7.863196114772401 with std:14.784778510494187. The val loss is 7.368304213491955 with std:9.214089495048729.\n",
      "7.368304213491955 1.0840143591783309\n",
      "The training loss is 7.241188262743912 with std:11.358526367599788. The val loss is 10.312398403036498 with std:23.59841740362161.\n",
      "10.312398403036498 1.0840143591783309\n",
      "Evaluating for {'lmda': 1.0940547072057425} ...\n",
      "The training loss is 7.287749189394 with std:13.591810155861975. The val loss is 12.420937330293325 with std:32.71356224758337.\n",
      "12.420937330293325 1.0940547072057425\n",
      "The training loss is 6.770972013298679 with std:11.527754499448712. The val loss is 17.418981696760053 with std:90.4432433044296.\n",
      "17.418981696760053 1.0940547072057425\n",
      "The training loss is 7.869175732622525 with std:14.798366685293672. The val loss is 7.369055840701931 with std:9.218263239165159.\n",
      "7.369055840701931 1.0940547072057425\n",
      "The training loss is 7.246212378979112 with std:11.369912512022035. The val loss is 10.317255542415483 with std:23.608162058981378.\n",
      "10.317255542415483 1.0940547072057425\n",
      "Evaluating for {'lmda': 1.1041880508541602} ...\n",
      "The training loss is 7.292893067975121 with std:13.603479742199578. The val loss is 12.456751293911575 with std:33.00505528678157.\n",
      "12.456751293911575 1.1041880508541602\n",
      "The training loss is 6.772899007641297 with std:11.53308845553048. The val loss is 17.42669460336598 with std:90.5151320358451.\n",
      "17.42669460336598 1.1041880508541602\n",
      "The training loss is 7.875151591778683 with std:14.811997257148144. The val loss is 7.369804282163657 with std:9.222435867365888.\n",
      "7.369804282163657 1.1041880508541602\n",
      "The training loss is 7.251231167207819 with std:11.381301169043647. The val loss is 10.322100248250509 with std:23.61798454515976.\n",
      "10.322100248250509 1.1041880508541602\n",
      "Evaluating for {'lmda': 1.1144152514667882} ...\n",
      "The training loss is 7.2980400039787465 with std:13.61519393792815. The val loss is 12.492959348377662 with std:33.30043673907424.\n",
      "12.492959348377662 1.1144152514667882\n",
      "The training loss is 6.774827936172247 with std:11.538458534987456. The val loss is 17.43431106616429 with std:90.58617058138555.\n",
      "17.43431106616429 1.1144152514667882\n",
      "The training loss is 7.881123629847483 with std:14.825669899203263. The val loss is 7.370549448725459 with std:9.226607386515775.\n",
      "7.370549448725459 1.1144152514667882\n",
      "The training loss is 7.256244648514764 with std:11.392692327594894. The val loss is 10.326932696420364 with std:23.627885010189953.\n",
      "10.326932696420364 1.1144152514667882\n",
      "Evaluating for {'lmda': 1.124737178364752} ...\n",
      "The training loss is 7.303189926501485 with std:13.626952273422216. The val loss is 12.529565096146387 with std:33.59973797695728.\n",
      "12.529565096146387 1.124737178364752\n",
      "The training loss is 6.776758860671727 with std:11.543864958487537. The val loss is 17.441830817111637 with std:90.6563558741256.\n",
      "17.441830817111637 1.124737178364752\n",
      "The training loss is 7.887091787389363 with std:14.839384287419612. The val loss is 7.371291255352675 with std:9.230777816220591.\n",
      "7.371291255352675 1.124737178364752\n",
      "The training loss is 7.261252845976576 with std:11.40408598112204. The val loss is 10.331753070433972 with std:23.637863616638974.\n",
      "10.331753070433972 1.124737178364752\n",
      "Evaluating for {'lmda': 1.135154708920999} ...\n",
      "The training loss is 7.308342766351867 with std:13.638754279481986. The val loss is 12.566572136193047 with std:33.90299021040528.\n",
      "12.566572136193047 1.135154708920999\n",
      "The training loss is 6.778691844071971 with std:11.549307949534606. The val loss is 17.449253591243373 with std:90.72568487350868.\n",
      "17.449253591243373 1.135154708920999\n",
      "The training loss is 7.893056007929896 with std:14.853140100685552. The val loss is 7.372029621170343 with std:9.234947188717722.\n",
      "7.372029621170343 1.135154708920999\n",
      "The training loss is 7.266255784667278 with std:11.415482127614254. The val loss is 10.336561561399506 with std:23.647920541700252.\n",
      "10.336561561399506 1.135154708920999\n",
      "Evaluating for {'lmda': 1.1456687286348715} ...\n",
      "The training loss is 7.313498456082496 with std:13.650599487493102. The val loss is 12.603984063593115 with std:34.21022448348689.\n",
      "12.603984063593115 1.1456687286348715\n",
      "The training loss is 6.780626950466708 with std:11.554787734510397. The val loss is 17.45657912662579 with std:90.79415456492228.\n",
      "17.45657912662579 1.1456687286348715\n",
      "The training loss is 7.899016237969747 with std:14.866937020927091. The val loss is 7.372764469503874 with std:9.239115548762381.\n",
      "7.372764469503874 1.1456687286348715\n",
      "The training loss is 7.27125349166448 with std:11.426880769631502. The val loss is 10.341358367993637 with std:23.658055977284917.\n",
      "10.341358367993637 1.1456687286348715\n",
      "Evaluating for {'lmda': 1.1562801312073754} ...\n",
      "The training loss is 7.318656930022661 with std:13.662487429588117. The val loss is 12.641804469070392 with std:34.52147167080791.\n",
      "12.641804469070392 1.1562801312073754\n",
      "The training loss is 6.782564245120953 with std:11.560304542716347. The val loss is 17.4638071643119 with std:90.86176195930025.\n",
      "17.4638071643119 1.1562801312073754\n",
      "The training loss is 7.904972426995138 with std:14.880774733218159. The val loss is 7.373495727919435 with std:9.24328295351238.\n",
      "7.373495727919435 1.1562801312073754\n",
      "The training loss is 7.276245996055408 with std:11.438281914330979. The val loss is 10.346143696429603 with std:23.66827013010684.\n",
      "10.346143696429603 1.1562801312073754\n",
      "Evaluating for {'lmda': 1.1669898186171475} ...\n",
      "The training loss is 7.323818124310948 with std:13.674417638808054. The val loss is 12.680036938569636 with std:34.83676247425938.\n",
      "12.680036938569636 1.1669898186171475\n",
      "The training loss is 6.78450379448118 with std:11.565858606416041. The val loss is 17.470937448297942 with std:90.92850409273393.\n",
      "17.470937448297942 1.1669898186171475\n",
      "The training loss is 7.910924527487116 with std:14.894652925887973. The val loss is 7.3742233282628264 with std:9.24744947240915.\n",
      "7.3742233282628264 1.1669898186171475\n",
      "The training loss is 7.281233328942829 with std:11.449685573493017. The val loss is 10.35091776042394 with std:23.6785632217631.\n",
      "10.35091776042394 1.1669898186171475\n",
      "Evaluating for {'lmda': 1.1777987011971192} ...\n",
      "The training loss is 7.3289819769268885 with std:13.686389649260782. The val loss is 12.71868505278336 with std:35.1561274194703.\n",
      "12.71868505278336 1.1777987011971192\n",
      "The training loss is 6.786445666185816 with std:11.571450160878776. The val loss is 17.477969725477955 with std:90.99437802605594.\n",
      "17.477969725477955 1.1777987011971192\n",
      "The training loss is 7.916872494930935 with std:14.90857129062777. The val loss is 7.374947206697635 with std:9.251615187057023.\n",
      "7.374947206697635 1.1777987011971192\n",
      "The training loss is 7.28621552345153 with std:11.461091763546998. The val loss is 10.355680781163421 with std:23.68893548880985.\n",
      "10.355680781163421 1.1777987011971192\n",
      "Evaluating for {'lmda': 1.1887076977119033} ...\n",
      "The training loss is 7.334148427723493 with std:13.69840299628231. The val loss is 12.757752386697549 with std:35.47959685250155.\n",
      "12.757752386697549 1.1887076977119033\n",
      "The training loss is 6.788389929075974 with std:11.577079444423141. The val loss is 17.48490374560116 with std:91.05938084445201.\n",
      "17.48490374560116 1.1887076977119033\n",
      "The training loss is 7.9228162878249 with std:14.922529522596097. The val loss is 7.375667303742247 with std:9.2557801910994.\n",
      "7.375667303742247 1.1887076977119033\n",
      "The training loss is 7.291192614734886 with std:11.472500505596786. The val loss is 10.360432987272025 with std:23.69938718283795.\n",
      "10.360432987272025 1.1887076977119033\n",
      "Evaluating for {'lmda': 1.1997177354358843} ...\n",
      "The training loss is 7.339317418458722 with std:13.710457216594712. The val loss is 12.797242509113396 with std:35.80720093642544.\n",
      "12.797242509113396 1.1997177354358843\n",
      "The training loss is 6.79033665320672 with std:11.582746698461198. The val loss is 17.491739261228357 with std:91.12350965705708.\n",
      "17.491739261228357 1.1997177354358843\n",
      "The training loss is 7.928755867688935 with std:14.936527320522455. The val loss is 7.376383564306103 with std:9.259944590093589.\n",
      "7.376383564306103 1.1997177354358843\n",
      "The training loss is 7.296164639981016 with std:11.483911825444597. The val loss is 10.365174614775388 with std:23.70991857053863.\n",
      "10.365174614775388 1.1997177354358843\n",
      "Evaluating for {'lmda': 1.2108297502320393} ...\n",
      "The training loss is 7.344488892827217 with std:13.722551848464516. The val loss is 12.837158982173644 with std:36.138969648020996.\n",
      "12.837158982173644 1.2108297502320393\n",
      "The training loss is 6.7922859098584105 with std:11.588452167543698. The val loss is 17.498476027688977 with std:91.18676159655655.\n",
      "17.498476027688977 1.2108297502320393\n",
      "The training loss is 7.934691199072412 with std:14.950564386809143. The val loss is 7.3770959377244925 with std:9.264108501381491.\n",
      "7.3770959377244925 1.2108297502320393\n",
      "The training loss is 7.301131638419909 with std:11.4953257536159. The val loss is 10.369905907066524 with std:23.720529933769548.\n",
      "10.369905907066524 1.2108297502320393\n",
      "Evaluating for {'lmda': 1.2220446866314887} ...\n",
      "The training loss is 7.349662796492084 with std:13.734686431861212. The val loss is 12.877505360872357 with std:36.47493277439493.\n",
      "12.877505360872357 1.2220446866314887\n",
      "The training loss is 6.794237771548715 with std:11.594196099405302. The val loss is 17.505113803038515 with std:91.24913381878598.\n",
      "17.505113803038515 1.2220446866314887\n",
      "The training loss is 7.940622249562403 with std:14.964640427632572. The val loss is 7.377804377793057 with std:9.2682720539603.\n",
      "7.377804377793057 1.2220446866314887\n",
      "The training loss is 7.306093651330175 with std:11.506742325382692. The val loss is 10.374627114870334 with std:23.731221569614675.\n",
      "10.374627114870334 1.2220446866314887\n",
      "Evaluating for {'lmda': 1.233363497913776} ...\n",
      "The training loss is 7.354839077116071 with std:13.7468605086134. The val loss is 12.918285192550961 with std:36.81511990955959.\n",
      "12.918285192550961 1.233363497913776\n",
      "The training loss is 6.796192312044766 with std:11.599978745010986. The val loss is 17.51165234801792 with std:91.31062350234662.\n",
      "17.51165234801792 1.233363497913776\n",
      "The training loss is 7.946548989790942 with std:14.978755153042277. The val loss is 7.3785088428004535 with std:9.272435388349322.\n",
      "7.3785088428004535 1.233363497913776\n",
      "The training loss is 7.311050722046205 with std:11.518161580787002. The val loss is 10.379338496207652 with std:23.741993790440976.\n",
      "10.379338496207652 1.233363497913776\n",
      "Evaluating for {'lmda': 1.244787146187906} ...\n",
      "The training loss is 7.36001768439277 with std:13.75907362256513. The val loss is 12.959502016405038 with std:37.15956045116876.\n",
      "12.959502016405038 1.244787146187906\n",
      "The training loss is 6.798149606375746 with std:11.605800358602481. The val loss is 17.518091426011416 with std:91.37122784820292.\n",
      "17.518091426011416 1.244787146187906\n",
      "The training loss is 7.952471393442141 with std:14.992908277058547. The val loss is 7.3792092955609725 with std:9.276598656455485.\n",
      "7.3792092955609725 1.244787146187906\n",
      "The training loss is 7.316002895965335 with std:11.529583564663206. The val loss is 10.384040316359222 with std:23.75284692395137.\n",
      "10.384040316359222 1.244787146187906\n",
      "Evaluating for {'lmda': 1.2563166024741201} ...\n",
      "The training loss is 7.365198570078109 with std:13.771325319731973. The val loss is 13.001159362964176 with std:37.508283597075845.\n",
      "13.001159362964176 1.2563166024741201\n",
      "The training loss is 6.80010973084592 with std:11.611661197745915. The val loss is 17.524430803005863 with std:91.43094407929068.\n",
      "17.524430803005863 1.2563166024741201\n",
      "The training loss is 7.95838943725925 with std:15.007099517768621. The val loss is 7.379905703445168 with std:9.28076202143679.\n",
      "7.379905703445168 1.2563166024741201\n",
      "The training loss is 7.320950220555592 with std:11.541008326661151. The val loss is 10.388732847829441 with std:23.763781313234603.\n",
      "10.388732847829441 1.2563166024741201\n",
      "Evaluating for {'lmda': 1.2679528467864338} ...\n",
      "The training loss is 7.370381688020934 with std:13.783615148454654. The val loss is 13.043260753574733 with std:37.86131834197835.\n",
      "13.043260753574733 1.2679528467864338\n",
      "The training loss is 6.802072763048008 with std:11.617561523379603. The val loss is 17.53067024755108 with std:91.48976944012888.\n",
      "17.53067024755108 1.2679528467864338\n",
      "The training loss is 7.964303101051404 with std:15.021328597421892. The val loss is 7.380598038410718 with std:9.284925657564532.\n",
      "7.380598038410718 1.2679528467864338\n",
      "The training loss is 7.325892745363292 with std:11.552435921267513. The val loss is 10.393416370309051 with std:23.77479731680939.\n",
      "10.393416370309051 1.2679528467864338\n",
      "Evaluating for {'lmda': 1.2796968682159415} ...\n",
      "The training loss is 7.375566994194 with std:13.795942659552951. The val loss is 13.085809699864665 with std:38.21869347395557.\n",
      "13.085809699864665 1.2796968682159415\n",
      "The training loss is 6.804038781876965 with std:11.623501599862692. The val loss is 17.536809530720465 with std:91.54770119643385.\n",
      "17.536809530720465 1.2796968682159415\n",
      "The training loss is 7.970212367699765 with std:15.035595242522593. The val loss is 7.381286277030981 with std:9.289089750081926.\n",
      "7.381286277030981 1.2796968682159415\n",
      "The training loss is 7.330830522021057 with std:11.563866407827604. The val loss is 10.398091170638416 with std:23.785895308667154.\n",
      "10.398091170638416 1.2796968682159415\n",
      "Evaluating for {'lmda': 1.291549665014884} ...\n",
      "The training loss is 7.380754446724835 with std:13.80830740647867. The val loss is 13.128809703216636 with std:38.58043757112738.\n",
      "13.128809703216636 1.291549665014884\n",
      "The training loss is 6.806007867544127 with std:11.629481695024259. The val loss is 17.54284842607214 with std:91.60473663473309.\n",
      "17.54284842607214 1.291549665014884\n",
      "The training loss is 7.976117223164045 with std:15.049899183921946. The val loss is 7.381970400524101 with std:9.293254495063975.\n",
      "7.381970400524101 1.291549665014884\n",
      "The training loss is 7.3357636042561 with std:11.575299850566607. The val loss is 10.402757542770395 with std:23.797075678310403.\n",
      "10.402757542770395 1.291549665014884\n",
      "Evaluating for {'lmda': 1.3035122446815088} ...\n",
      "The training loss is 7.385944005926196 with std:13.820708945467018. The val loss is 13.172264254214253 with std:38.946578998121076.\n",
      "13.172264254214253 1.3035122446815088\n",
      "The training loss is 6.807980101591866 with std:11.63550208021335. The val loss is 17.548786709610866 with std:91.66087306198091.\n",
      "17.548786709610866 1.3035122446815088\n",
      "The training loss is 7.982017656488275 with std:15.064240156908143. The val loss is 7.382650394779745 with std:9.297420099272689.\n",
      "7.382650394779745 1.3035122446815088\n",
      "The training loss is 7.34069204789863 with std:11.58673631861003. The val loss is 10.40741578773243 with std:23.808338830786603.\n",
      "10.40741578773243 1.3035122446815088\n",
      "Evaluating for {'lmda': 1.3155856240457038} ...\n",
      "The training loss is 7.391135634326721 with std:13.833146835687545. The val loss is 13.216176832096943 with std:39.31714590265522.\n",
      "13.216176832096943 1.3155856240457038\n",
      "The training loss is 6.8099555669086405 with std:11.641563030349598. The val loss is 17.554624159750798 with std:91.71610780517952.\n",
      "17.554624159750798 1.3155856240457038\n",
      "The training loss is 7.987913659806662 with std:15.078617901295232. The val loss is 7.383326250385801 with std:9.301586780012913.\n",
      "7.383326250385801 1.3155856240457038\n",
      "The training loss is 7.345615910890756 with std:11.598175886004967. The val loss is 10.412066213589735 with std:23.819685186721674.\n",
      "10.412066213589735 1.3155856240457038\n",
      "Evaluating for {'lmda': 1.3277708293554291} ...\n",
      "The training loss is 7.396329296701167 with std:13.845620639393223. The val loss is 13.26055090419566 with std:39.69216621198768.\n",
      "13.26055090419566 1.3277708293554291\n",
      "The training loss is 6.811934347744288 with std:11.647664823973917. The val loss is 17.560360557279 with std:91.77043821100594.\n",
      "17.560360557279 1.3277708293554291\n",
      "The training loss is 7.993805228349319 with std:15.0930321615102. The val loss is 7.383997962654158 with std:9.305754764986226.\n",
      "7.383997962654158 1.3277708293554291\n",
      "The training loss is 7.350535253295405 with std:11.609618631739114. The val loss is 10.416709135407164 with std:23.831115182346835.\n",
      "10.416709135407164 1.3277708293554291\n",
      "Evaluating for {'lmda': 1.3400688963639507} ...\n",
      "The training loss is 7.4015249601008914 with std:13.858129922069816. The val loss is 13.30538992536465 with std:40.07166762936261.\n",
      "13.30538992536465 1.3400688963639507\n",
      "The training loss is 6.813916529726203 with std:11.653807743300991. The val loss is 17.56599568531955 with std:91.82386164543367.\n",
      "17.56599568531955 1.3400688963639507\n",
      "The training loss is 7.999692360447551 with std:15.10748268667849. The val loss is 7.384665531644999 with std:9.309924292143009.\n",
      "7.384665531644999 1.3400688963639507\n",
      "The training loss is 7.355450137305936 with std:11.621064639761796. The val loss is 10.421344875212053 with std:23.84262926952519.\n",
      "10.421344875212053 1.3400688963639507\n",
      "Evaluating for {'lmda': 1.3524808704178755} ...\n",
      "The training loss is 7.406722593883826 with std:13.870674252582027. The val loss is 13.350697337405508 with std:40.455677630419544.\n",
      "13.350697337405508 1.3524808704178755\n",
      "The training loss is 6.815902199875436 with std:11.659992074271152. The val loss is 17.571529329298606 with std:91.8763754933659.\n",
      "17.571529329298606 1.3524808704178755\n",
      "The training loss is 8.005575057539424 with std:15.121969230708798. The val loss is 7.385328962190695 with std:9.31409560953408.\n",
      "7.385328962190695 1.3524808704178755\n",
      "The training loss is 7.360360627255416 with std:11.632513999002247. The val loss is 10.425973761956191 with std:23.854227915772324.\n",
      "10.425973761956191 1.3524808704178755\n",
      "Evaluating for {'lmda': 1.3650078065460138} ...\n",
      "The training loss is 7.411922169744905 with std:13.883253203321164. The val loss is 13.396476568489831 with std:40.844223459609154.\n",
      "13.396476568489831 1.3650078065460138\n",
      "The training loss is 6.817891446623603 with std:11.666218106604012. The val loss is 17.57696127691007 with std:91.92797715826859.\n",
      "17.57696127691007 1.3650078065460138\n",
      "The training loss is 8.011453324174893 with std:15.136491552375167. The val loss is 7.385988263918792 with std:9.318268975160983.\n",
      "7.385988263918792 1.3650078065460138\n",
      "The training loss is 7.365266789627107 with std:11.643966803389922. The val loss is 10.43059613147856 with std:23.865911604276636.\n",
      "10.43059613147856 1.3650078065460138\n",
      "Evaluating for {'lmda': 1.3776507695490536} ...\n",
      "The training loss is 7.417123661745869 with std:13.895866350348921. The val loss is 13.44273103256267 with std:41.23733212646458.\n",
      "13.44273103256267 1.3776507695490536\n",
      "The training loss is 6.819884359830154 with std:11.67248613385198. The val loss is 17.582291318082106 with std:91.9786640618034.\n",
      "17.582291318082106 1.3776507695490536\n",
      "The training loss is 8.017327168020811 with std:15.151049415398264. The val loss is 7.386643451273832 with std:9.322444656824725.\n",
      "7.386643451273832 1.3776507695490536\n",
      "The training loss is 7.370168693064463 with std:11.65542315187268. The val loss is 10.435212326468228 with std:23.87768083391656.\n",
      "10.435212326468228 1.3776507695490536\n",
      "Evaluating for {'lmda': 1.3904108340900696} ...\n",
      "The training loss is 7.422327046345215 with std:13.908513273541713. The val loss is 13.489464128744665 with std:41.63503040187735.\n",
      "13.489464128744665 1.3904108340900696\n",
      "The training loss is 6.821881030800079 with std:11.678796453454844. The val loss is 17.58751924494555 with std:92.02843364347912.\n",
      "17.58751924494555 1.3904108340900696\n",
      "The training loss is 8.023196599866342 with std:15.1656425885257. The val loss is 7.387294543538678 with std:9.326622931974843.\n",
      "7.387294543538678 1.3904108340900696\n",
      "The training loss is 7.375066408381906 with std:11.666883148435776. The val loss is 10.439822696426576 with std:23.889536119272066.\n",
      "10.439822696426576 1.3904108340900696\n",
      "Evaluating for {'lmda': 1.4032890847858732} ...\n",
      "The training loss is 7.427532302428313 with std:13.921193556733247. The val loss is 13.53667924073037 with std:42.03734481435138.\n",
      "13.53667924073037 1.4032890847858732\n",
      "The training loss is 6.8238815523021605 with std:11.685149366795187. The val loss is 17.59264485180128 with std:92.07728336028778.\n",
      "17.59264485180128 1.4032890847858732\n",
      "The training loss is 8.029061633627602 with std:15.180270845609458. The val loss is 7.3879415648545335 with std:9.330804087556169.\n",
      "7.3879415648545335 1.4032890847858732\n",
      "The training loss is 7.379960008576088 with std:11.678346902120518. The val loss is 10.444427597630584 with std:23.901477990636973.\n",
      "10.444427597630584 1.4032890847858732\n",
      "Evaluating for {'lmda': 1.4162866162991987} ...\n",
      "The training loss is 7.4327394113369865 with std:13.933906787855475. The val loss is 13.584379736171844 with std:42.44430164613774.\n",
      "13.584379736171844 1.4162866162991987\n",
      "The training loss is 6.825886018587694 with std:11.691545179254128. The val loss is 17.597667935090286 with std:92.12521068635897.\n",
      "17.597667935090286 1.4162866162991987\n",
      "The training loss is 8.034922286352964 with std:15.19493396568366. The val loss is 7.388584544240806 with std:9.334988419856458.\n",
      "7.388584544240806 1.4162866162991987\n",
      "The training loss is 7.384849568837068 with std:11.689814527042255. The val loss is 10.449027393096133 with std:23.913506994027312.\n",
      "10.449027393096133 1.4162866162991987\n",
      "Evaluating for {'lmda': 1.429404533431761} ...\n",
      "The training loss is 7.437948356899501 with std:13.94665255907877. The val loss is 13.632568966060804 with std:42.85592692936561.\n",
      "13.632568966060804 1.429404533431761\n",
      "The training loss is 6.8278945254096355 with std:11.69798420026783. The val loss is 17.602588293363365 with std:92.17221311260911.\n",
      "17.602588293363365 1.429404533431761\n",
      "The training loss is 8.040778578227636 with std:15.209631733038737. The val loss is 7.389223515613312 with std:9.339176234351656.\n",
      "7.389223515613312 1.429404533431761\n",
      "The training loss is 7.3897351665602145 with std:11.701286142408108. The val loss is 10.45362245254131 with std:23.925623691186633.\n",
      "10.45362245254131 1.429404533431761\n",
      "Evaluating for {'lmda': 1.4426439512181572} ...\n",
      "The training loss is 7.443159125460156 with std:13.959430466950117. The val loss is 13.68125026410585 with std:43.272246442132335.\n",
      "13.68125026410585 1.4426439512181572\n",
      "The training loss is 6.829907170042387 with std:11.704466743384952. The val loss is 17.607405727253767 with std:92.21828814641422.\n",
      "17.607405727253767 1.4426439512181572\n",
      "The training loss is 8.046630532578993 with std:15.224363937296511. The val loss is 7.389858517802433 with std:9.343367845552724.\n",
      "7.389858517802433 1.4426439512181572\n",
      "The training loss is 7.394616881358496 with std:11.712761872535534. The val loss is 10.458213152350577 with std:23.937828659589943.\n",
      "10.458213152350577 1.4426439512181572\n",
      "Evaluating for {'lmda': 1.4560059950206485} ...\n",
      "The training loss is 7.448371705909043 with std:13.97224011253076. The val loss is 13.730426946080758 with std:43.69328570433474.\n",
      "13.730426946080758 1.4560059950206485\n",
      "The training loss is 6.831924051301962 with std:11.710993126324531. The val loss is 17.6121200394475 with std:92.26343331125214.\n",
      "17.6121200394475 1.4560059950206485\n",
      "The training loss is 8.052478175881474 with std:15.239130373482228. The val loss is 7.390489594570233 with std:9.347563576850314.\n",
      "7.390489594570233 1.4560059950206485\n",
      "The training loss is 7.399494795075107 with std:11.724241846869536. The val loss is 10.462799875538503 with std:23.950122492443988.\n",
      "10.462799875538503 1.4560059950206485\n",
      "Evaluating for {'lmda': 1.4694918006248172} ...\n",
      "The training loss is 7.453586089711903 with std:13.985081101532096. The val loss is 13.780102309212873 with std:44.1190699738618.\n",
      "13.780102309212873 1.4694918006248172\n",
      "The training loss is 6.833945269566776 with std:11.717563671034908. The val loss is 17.61673103465793 with std:92.30764614638568.\n",
      "17.61673103465793 1.4694918006248172\n",
      "The training loss is 8.05832153776144 with std:15.25393084209567. The val loss is 7.391116794626352 with std:9.351763760360074.\n",
      "7.391116794626352 1.4694918006248172\n",
      "The training loss is 7.4043689917965265 with std:11.735726200000395. The val loss is 10.467383011715187 with std:23.96250579868768.\n",
      "10.467383011715187 1.4694918006248172\n",
      "Evaluating for {'lmda': 1.4831025143361045} ...\n",
      "The training loss is 7.4588022709393025 with std:13.997953044448987. The val loss is 13.830279631509068 with std:44.54962424222338.\n",
      "13.830279631509068 1.4831025143361045\n",
      "The training loss is 6.8359709267988915 with std:11.724178703752823. The val loss is 17.62123851960035 with std:92.35092420653723.\n",
      "17.62123851960035 1.4831025143361045\n",
      "The training loss is 8.064160651002387 with std:15.268765149180672. The val loss is 7.391740171643809 with std:9.355968736767332.\n",
      "7.391740171643809 1.4831025143361045\n",
      "The training loss is 7.409239557866037 with std:11.747215071681008. The val loss is 10.471962957051312 with std:23.974979202988578.\n",
      "10.471962957051312 1.4831025143361045\n",
      "Evaluating for {'lmda': 1.4968392930772556} ...\n",
      "The training loss is 7.464020246296878 with std:14.01085555669436. The val loss is 13.880962171111713 with std:44.984973230439465.\n",
      "13.880962171111713 1.4968392930772556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 6.838001126565914 with std:11.730838555063873. The val loss is 17.62564230296622 with std:92.39326506155642.\n",
      "17.62564230296622 1.4968392930772556\n",
      "The training loss is 8.069995551550223 with std:15.28363310639406. The val loss is 7.392359784273747 with std:9.360178855172041.\n",
      "7.392359784273747 1.4968392930772556\n",
      "The training loss is 7.4141065818974665 with std:11.758708606843904. The val loss is 10.476540114242919 with std:23.987543345735933.\n",
      "10.476540114242919 1.4968392930772556\n",
      "Evaluating for {'lmda': 1.510703304486654} ...\n",
      "The training loss is 7.469240015154508 with std:14.023788258729434. The val loss is 13.932153165640306 with std:45.425141384792525.\n",
      "13.932153165640306 1.510703304486654\n",
      "The training loss is 6.840035974063255 with std:11.73754355996326. The val loss is 17.629942195400883 with std:92.43466629611547.\n",
      "17.629942195400883 1.510703304486654\n",
      "The training loss is 8.075826278518354 with std:15.29853453107204. The val loss is 7.392975696159362 with std:9.3643944729339.\n",
      "7.392975696159362 1.510703304486654\n",
      "The training loss is 7.41897015479009 with std:11.770206955619326. The val loss is 10.481114892479033 with std:24.00019888303607.\n",
      "10.481114892479033 1.510703304486654\n",
      "Evaluating for {'lmda': 1.524695727017573} ...\n",
      "The training loss is 7.474461579576401 with std:14.036750776194825. The val loss is 13.983855831522243 with std:45.8701528724721.\n",
      "13.983855831522243 1.524695727017573\n",
      "The training loss is 6.842075576137116 with std:11.744294057917458. The val loss is 17.634138009480832 with std:92.47512550939746.\n",
      "17.634138009480832 1.524695727017573\n",
      "The training loss is 8.081652874193319 with std:15.313469246296581. The val loss is 7.393587975949386 with std:9.368615955516749.\n",
      "7.393587975949386 1.524695727017573\n",
      "The training loss is 7.423830369743015 with std:11.781710273351258. The val loss is 10.485687707407807 with std:24.01294648670261.\n",
      "10.485687707407807 1.524695727017573\n",
      "Evaluating for {'lmda': 1.5388177500383464} ...\n",
      "The training loss is 7.47968494435019 with std:14.04974274003761. The val loss is 14.03607336331417 with std:46.320031577124915.\n",
      "14.03607336331417 1.5388177500383464\n",
      "The training loss is 6.84412004130796 with std:11.751090392926367. The val loss is 17.638229559691737 with std:92.51464031478164.\n",
      "17.638229559691737 1.5388177500383464\n",
      "The training loss is 8.087475384040127 with std:15.328437080959047. The val loss is 7.3941966973105755 with std:9.372843676334266.\n",
      "7.3941966973105755 1.5388177500383464\n",
      "The training loss is 7.4286873222708305 with std:11.79321872061533. The val loss is 10.490258981104049 with std:24.02578684424492.\n",
      "10.490258981104049 1.5388177500383464\n",
      "Evaluating for {'lmda': 1.55307057393346} ...\n",
      "The training loss is 7.484910117017232 with std:14.06276378664014. The val loss is 14.088808933025518 with std:46.77480109439945.\n",
      "14.088808933025518 1.55307057393346\n",
      "The training loss is 6.846169479794576 with std:11.757932913586682. The val loss is 17.642216662409233 with std:92.55320833955582.\n",
      "17.642216662409233 1.55307057393346\n",
      "The training loss is 8.093293856708188 with std:15.34343786982398. The val loss is 7.394801938939808 with std:9.377078016595307.\n",
      "7.394801938939808 1.55307057393346\n",
      "The training loss is 7.433541110219455 with std:11.804732463235602. The val loss is 10.494829142038212 with std:24.038720658858946.\n",
      "10.494829142038212 1.55307057393346\n",
      "Evaluating for {'lmda': 1.567455410205595} ...\n",
      "The training loss is 7.490137107901954 with std:14.075813557944416. The val loss is 14.142065689433077 with std:47.23448472740266.\n",
      "14.142065689433077 1.567455410205595\n",
      "The training loss is 6.848224003538776 with std:11.76482197315555. The val loss is 17.646099135880522 with std:92.59082722462782.\n",
      "17.646099135880522 1.567455410205595\n",
      "The training loss is 8.099108344037006 with std:15.358471453590326. The val loss is 7.3954037845751674 with std:9.381319365149803.\n",
      "7.3954037845751674 1.567455410205595\n",
      "The training loss is 7.438391833782531 with std:11.816251672301556. The val loss is 10.499398625044455 with std:24.051748649409348.\n",
      "10.499398625044455 1.567455410205595\n",
      "Evaluating for {'lmda': 1.5819734815786} ...\n",
      "The training loss is 7.495365930141795 with std:14.088891701576564. The val loss is 14.195846757388125 with std:47.69910548206347.\n",
      "14.195846757388125 1.5819734815786\n",
      "The training loss is 6.850283726230517 with std:11.771757929615056. The val loss is 17.64987680020483 with std:92.62749462422043.\n",
      "17.64987680020483 1.5819734815786\n",
      "The training loss is 8.104918901062558 with std:15.37353767895267. The val loss is 7.396002323006893 with std:9.385568118335804.\n",
      "7.396002323006893 1.5819734815786\n",
      "The training loss is 7.443239595518135 with std:11.827776524184982. The val loss is 10.503967871291275 with std:24.064871550417987.\n",
      "10.503967871291275 1.5819734815786\n",
      "Evaluating for {'lmda': 1.5966260221014252} ...\n",
      "The training loss is 7.500596599716977 with std:14.101997870969127. The val loss is 14.250155237117276 with std:48.168686062423454.\n",
      "14.250155237117276 1.5966260221014252\n",
      "The training loss is 6.852348763333829 with std:11.77874114573769. The val loss is 17.653549477318286 with std:92.66320820560469.\n",
      "17.653549477318286 1.5966260221014252\n",
      "The training loss is 8.11072558602329 with std:15.388636398659807. The val loss is 7.396597648087033 with std:9.389824679825768.\n",
      "7.396597648087033 1.5966260221014252\n",
      "The training loss is 7.44808450036663 with std:11.839307200557622. The val loss is 10.508537328252121 with std:24.078090112046425.\n",
      "10.508537328252121 1.5966260221014252\n",
      "Evaluating for {'lmda': 1.6114142772530198} ...\n",
      "The training loss is 7.505829135480506 with std:14.115131725482252. The val loss is 14.304994203513875 with std:48.64324886581436.\n",
      "14.304994203513875 1.6114142772530198\n",
      "The training loss is 6.85441923211322 with std:11.785771989152348. The val loss is 17.657116990978835 with std:92.69796564882753.\n",
      "17.657116990978835 1.6114142772530198\n",
      "The training loss is 8.116528460367155 with std:15.403767471573792. The val loss is 7.397189858739287 with std:9.394089460475811.\n",
      "7.397189858739287 1.6114142772530198\n",
      "The training loss is 7.452926655668253 with std:11.85084388840749. The val loss is 10.513107449676317 with std:24.091405100076827.\n",
      "10.513107449676317 1.6114142772530198\n",
      "Evaluating for {'lmda': 1.6263395040481923} ...\n",
      "The training loss is 7.511063559188051 with std:14.128292930522909. The val loss is 14.360366705446234 with std:49.12281597816878.\n",
      "14.360366705446234 1.6263395040481923\n",
      "The training loss is 6.856495251660701 with std:11.792850832411062. The val loss is 17.66057916675134 with std:92.7317646464347.\n",
      "17.66057916675134 1.6263395040481923\n",
      "The training loss is 8.122327588758122 with std:15.41893076272656. The val loss is 7.397779058967537 with std:9.398362878173742.\n",
      "7.397779058967537 1.6263395040481923\n",
      "The training loss is 7.457766171181892 with std:11.862386780056628. The val loss is 10.517678695562422 with std:24.104817295895625.\n",
      "10.517678695562422 1.6263395040481923\n",
      "Evaluating for {'lmda': 1.6414029711444664} ...\n",
      "The training loss is 7.5162998955280385 with std:14.141481157662614. The val loss is 14.416275765016957 with std:49.60740916885915.\n",
      "14.416275765016957 1.6414029711444664\n",
      "The training loss is 6.858576942923595 with std:11.79997805305685. The val loss is 17.663935831995953 with std:92.76460290322123.\n",
      "17.663935831995953 1.6414029711444664\n",
      "The training loss is 8.12812303908348 with std:15.43412614337551. The val loss is 7.398365357864246 with std:9.402645357689138.\n",
      "7.398365357864246 1.6414029711444664\n",
      "The training loss is 7.462603159104229 with std:11.87393607317826. The val loss is 10.522251532131444 with std:24.11832749647248.\n",
      "10.522251532131444 1.6414029711444664\n",
      "Evaluating for {'lmda': 1.6566059589499136} ...\n",
      "The training loss is 7.521538172151866 with std:14.15469608475368. The val loss is 14.472724376866575 with std:50.09704988591248.\n",
      "14.472724376866575 1.6566059589499136\n",
      "The training loss is 6.860664428732762 with std:11.807154033691758. The val loss is 17.667186815856045 with std:92.79647813596979.\n",
      "17.667186815856045 1.6566059589499136\n",
      "The training loss is 8.133914882461289 with std:15.449353491058941. The val loss is 7.398948869618245 with std:9.406937330524796.\n",
      "7.398948869618245 1.6566059589499136\n",
      "The training loss is 7.467437734089292 with std:11.885491970813817. The val loss is 10.526826431801222 with std:24.13193651433983.\n",
      "10.526826431801222 1.6566059589499136\n",
      "Evaluating for {'lmda': 1.6719497597319901} ...\n",
      "The training loss is 7.526778419704214 with std:14.167937396044435. The val loss is 14.52971550743118 with std:50.591759250779944.\n",
      "14.52971550743118 1.6719497597319901\n",
      "The training loss is 6.8627578338315125 with std:11.814379162046. The val loss is 17.67033194924813 with std:92.82738807320338.\n",
      "17.67033194924813 1.6719497597319901\n",
      "The training loss is 8.139703193248334 with std:15.464612689649442. The val loss is 7.399529713521792 with std:9.411239234768484.\n",
      "7.399529713521792 1.6719497597319901\n",
      "The training loss is 7.472270013268909 with std:11.897054681390662. The val loss is 10.53140387316226 with std:24.145645177571158.\n",
      "10.53140387316226 1.6719497597319901\n",
      "Evaluating for {'lmda': 1.6874356777273758} ...\n",
      "The training loss is 7.532020671853353 with std:14.18120478229123. The val loss is 14.587252094222716 with std:51.09155805328089.\n",
      "14.587252094222716 1.6874356777273758\n",
      "The training loss is 6.864857284905381 with std:11.821653831048048. The val loss is 17.67337106485329 with std:92.85733045494688.\n",
      "17.67337106485329 1.6874356777273758\n",
      "The training loss is 8.145488049047989 with std:15.479903629406254. The val loss is 7.400108013977445 with std:9.41555151494675.\n",
      "7.400108013977445 1.6874356777273758\n",
      "The training loss is 7.477100116273579 with std:11.908624418739919. The val loss is 10.535984340954471 with std:24.15945432975958.\n",
      "10.535984340954471 1.6874356777273758\n",
      "Evaluating for {'lmda': 1.7030650292528444} ...\n",
      "The training loss is 7.537264965321767 with std:14.194497940871152. The val loss is 14.645337045091221 with std:51.59646674635686.\n",
      "14.645337045091221 1.7030650292528444\n",
      "The training loss is 6.866962910612329 with std:11.828978438895033. The val loss is 17.67630399711071 with std:92.88630303250231.\n",
      "17.67630399711071 1.7030650292528444\n",
      "The training loss is 8.151269530719015 with std:15.49522620702739. The val loss is 7.400683900504101 with std:9.419874621880018.\n",
      "7.400683900504101 1.7030650292528444\n",
      "The training loss is 7.481928165253914 with std:11.920201402113603. The val loss is 10.540568326044522 with std:24.173364829993034.\n",
      "10.540568326044522 1.7030650292528444\n",
      "Evaluating for {'lmda': 1.7188391428171457} ...\n",
      "The training loss is 7.542511339916813 with std:14.207816575890932. The val loss is 14.703973237482227 with std:52.106505440744925.\n",
      "14.703973237482227 1.7188391428171457\n",
      "The training loss is 6.869074841613622 with std:11.836353389124222. The val loss is 17.67913058220958 with std:92.9143035682024.\n",
      "17.67913058220958 1.7188391428171457\n",
      "The training loss is 8.157047722384373 with std:15.51058032569993. The val loss is 7.401257507742957 with std:9.424209012538993.\n",
      "7.401257507742957 1.7188391428171457\n",
      "The training loss is 7.486754284902957 with std:11.93178585620325. The val loss is 10.545156325405964 with std:24.18737755283352.\n",
      "10.545156325405964 1.7188391428171457\n",
      "Evaluating for {'lmda': 1.7347593592339308} ...\n",
      "The training loss is 7.547759838561589 with std:14.22116039829593. The val loss is 14.763163517700965 with std:52.62169389968485.\n",
      "14.763163517700965 1.7347593592339308\n",
      "The training loss is 6.871193210605647 with std:11.843779090685286. The val loss is 17.681850658087374 with std:92.94132983521882.\n",
      "17.681850658087374 1.7347593592339308\n",
      "The training loss is 8.162822711440693 with std:15.525965895150042. The val loss is 7.401828975462726 with std:9.428555149903305.\n",
      "7.401828975462726 1.7347593592339308\n",
      "The training loss is 7.491578602478917 with std:11.94337801115761. The val loss is 10.549748842098676 with std:24.20149338829092.\n",
      "10.549748842098676 1.7347593592339308\n",
      "Evaluating for {'lmda': 1.750827031735725} ...\n",
      "The training loss is 7.55301050732589 with std:14.23452912597637. The val loss is 14.822910700154894 with std:53.14205153339396.\n",
      "14.822910700154894 1.750827031735725\n",
      "The training loss is 6.8733181523520726 with std:11.851255958013144. The val loss is 17.684464064424386 with std:92.96737961732974.\n",
      "17.684464064424386 1.750827031735725\n",
      "The training loss is 8.168594588567963 with std:15.541382831691301. The val loss is 7.402398448564547 with std:9.432913502821162.\n",
      "7.402398448564547 1.750827031735725\n",
      "The training loss is 7.4964012478285476 with std:11.954978102601315. The val loss is 10.554346385250746 with std:24.215713241799445.\n",
      "10.554346385250746 1.750827031735725\n",
      "Evaluating for {'lmda': 1.7670435260889465} ...\n",
      "The training loss is 7.558263395457499 with std:14.247922483873058. The val loss is 14.883217566607705 with std:53.667597393612574.\n",
      "14.883217566607705 1.7670435260889465\n",
      "The training loss is 6.875449803716991 with std:11.85878441110173. The val loss is 17.68697064264324 with std:92.99245070872608.\n",
      "17.68697064264324 1.7670435260889465\n",
      "The training loss is 8.174363447739859 with std:15.55683105827289. The val loss is 7.402966077086641 with std:9.437284545871595.\n",
      "7.402966077086641 1.7670435260889465\n",
      "The training loss is 7.501222353411418 with std:11.966586371653074. The val loss is 10.558949470041595 with std:24.230038034193175.\n",
      "10.558949470041595 1.7670435260889465\n",
      "Evaluating for {'lmda': 1.783410220710008} ...\n",
      "The training loss is 7.563518555413595 with std:14.261340204080666. The val loss is 14.944086865423689 with std:54.1983501680287.\n",
      "14.944086865423689 1.783410220710008\n",
      "The training loss is 6.877588303698606 with std:11.866364875578526. The val loss is 17.689370235908246 with std:93.0165409138149.\n",
      "17.689370235908246 1.783410220710008\n",
      "The training loss is 8.180129386234348 with std:15.572310504526257. The val loss is 7.4035320162081835 with std:9.441668759228223.\n",
      "7.4035320162081835 1.783410220710008\n",
      "The training loss is 7.506042054324631 with std:11.978203064945069. The val loss is 10.563558617686114 with std:24.24446870168118.\n",
      "10.563558617686114 1.783410220710008\n",
      "Evaluating for {'lmda': 1.7999285067824764} ...\n",
      "The training loss is 7.568776042892189 with std:14.274782025949534. The val loss is 15.00552131080259 with std:54.73432817457969.\n",
      "15.00552131080259 1.7999285067824764\n",
      "The training loss is 6.879733793463708 with std:11.873997782779892. The val loss is 17.691662689126815 with std:93.03964804702812.\n",
      "17.691662689126815 1.7999285067824764\n",
      "The training loss is 8.185892504645253 with std:15.587821106811706. The val loss is 7.40409642625348 with std:9.446066628525672.\n",
      "7.40409642625348 1.7999285067824764\n",
      "The training loss is 7.51086048832838 with std:11.98982843464177. The val loss is 10.56817435542043 with std:24.25900619582197.\n",
      "10.56817435542043 1.7999285067824764\n",
      "Evaluating for {'lmda': 1.8165997883753267} ...\n",
      "The training loss is 7.574035916864169 with std:14.288247696186545. The val loss is 15.067523582031182 with std:55.27554935587239.\n",
      "15.067523582031182 1.8165997883753267\n",
      "The training loss is 6.881886416382885 with std:11.881683569827027. The val loss is 17.693847848952803 with std:93.06176993265359.\n",
      "17.693847848952803 1.8165997883753267\n",
      "The training loss is 8.191652906893767 with std:15.603362808263606. The val loss is 7.4046594726952 with std:9.45047864472757.\n",
      "7.4046594726952 1.8165997883753267\n",
      "The training loss is 7.515677795872063 with std:12.001462738459363. The val loss is 10.572797216489354 with std:24.273651483499744.\n",
      "10.572797216489354 1.8165997883753267\n",
      "Evaluating for {'lmda': 1.8334254825622907} ...\n",
      "The training loss is 7.5792982396052215 with std:14.30173696895357. The val loss is 15.13009632269631 with std:55.822031273212325.\n",
      "15.13009632269631 1.8334254825622907\n",
      "The training loss is 6.88404631806647 with std:11.889422679702978. The val loss is 17.69592556378968 with std:93.08290440464654.\n",
      "17.69592556378968 1.8334254825622907\n",
      "The training loss is 8.197410700240885 with std:15.618935558834924. The val loss is 7.405221326157678 with std:9.454905303997489.\n",
      "7.405221326157678 1.8334254825622907\n",
      "The training loss is 7.520494120121344 with std:12.013106239685738. The val loss is 10.57742774013456 with std:24.28840554689773.\n",
      "10.57742774013456 1.8334254825622907\n",
      "Evaluating for {'lmda': 1.8504070195423021} ...\n",
      "The training loss is 7.584563076728208 with std:14.31524960596505. The val loss is 15.193242139935418 with std:56.37379110094219.\n",
      "15.193242139935418 1.8504070195423021\n",
      "The training loss is 6.886213646401077 with std:11.89721556132998. The val loss is 17.69789568379683 with std:93.10304930647533.\n",
      "17.69789568379683 1.8504070195423021\n",
      "The training loss is 8.203165995300193 with std:15.634539315340733. The val loss is 7.405782162419841 with std:9.459347107571974.\n",
      "7.405782162419841 1.8504070195423021\n",
      "The training loss is 7.5253096069855925 with std:12.024759207200207. The val loss is 10.582066471585287 with std:24.303269383474866.\n",
      "10.582066471585287 1.8504070195423021\n",
      "Evaluating for {'lmda': 1.867545842761076} ...\n",
      "The training loss is 7.589830497215714 with std:14.328785376583768. The val loss is 15.256963603658981 with std:56.93084562048757.\n",
      "15.256963603658981 1.867545842761076\n",
      "The training loss is 6.888388551587284 with std:11.90506266964853. The val loss is 17.69975806089742 with std:93.12220249096588.\n",
      "17.69975806089742 1.867545842761076\n",
      "The training loss is 8.208918906051535 with std:15.650174041502192. The val loss is 7.406342162417846 with std:9.463804561636248.\n",
      "7.406342162417846 1.867545842761076\n",
      "The training loss is 7.530124405146438 with std:12.036421915494422. The val loss is 10.586713962049828 with std:24.318244005940027.\n",
      "10.586713962049828 1.867545842761076\n",
      "Evaluating for {'lmda': 1.8848434090337953} ...\n",
      "The training loss is 7.595100573452905 with std:14.342344057914834. The val loss is 15.321263245766746 with std:57.49321121430399.\n",
      "15.321263245766746 1.8848434090337953\n",
      "The training loss is 6.89057118617771 with std:11.912964465695707. The val loss is 17.701512548787388 with std:93.14036182015808.\n",
      "17.701512548787388 1.8848434090337953\n",
      "The training loss is 8.21466954985497 with std:15.66583970798854. The val loss is 7.4069015122476305 with std:9.468278177202695.\n",
      "7.4069015122476305 1.8848434090337953\n",
      "The training loss is 7.534938666086787 with std:12.048094644692686. The val loss is 10.591370768709238 with std:24.33333044222869.\n",
      "10.591370768709238 1.8848434090337953\n",
      "Evaluating for {'lmda': 1.9023011886689438} ...\n",
      "The training loss is 7.600373381260707 with std:14.355925434898584. The val loss is 15.386143559393952 with std:58.06090386006714.\n",
      "15.386143559393952 1.9023011886689438\n",
      "The training loss is 6.8927617051159595 with std:11.920921416685971. The val loss is 17.703159002944187 with std:93.15752516514634.\n",
      "17.703159002944187 1.9023011886689438\n",
      "The training loss is 8.220418047465634 with std:15.681536292459127. The val loss is 7.407460403167109 with std:9.472768469992026.\n",
      "7.407460403167109 1.9023011886689438\n",
      "The training loss is 7.539752544120818 with std:12.059777680573658. The val loss is 10.596037454712121 with std:24.348529735478056.\n",
      "10.596037454712121 1.9023011886689438\n",
      "Evaluating for {'lmda': 1.9199206655932848} ...\n",
      "The training loss is 7.605648999929156 with std:14.369529300401725. The val loss is 15.45160699810598 with std:58.63393912434977.\n",
      "15.45160699810598 1.9199206655932848\n",
      "The training loss is 6.894960265776578 with std:11.928933996091457. The val loss is 17.70469728064208 with std:93.17369040598118.\n",
      "17.70469728064208 1.9199206655932848\n",
      "The training loss is 8.226164523049011 with std:15.69726377960473. The val loss is 7.4080190315983465 with std:9.477275960317458.\n",
      "7.4080190315983465 1.9199206655932848\n",
      "The training loss is 7.5445661964245945 with std:12.07147131459208. The val loss is 10.60071458917145 with std:24.36384294400351.\n",
      "10.60071458917145 1.9199206655932848\n",
      "Evaluating for {'lmda': 1.9377033374779888} ...\n",
      "The training loss is 7.610927512251339 with std:14.38315545530706. The val loss is 15.517655975146937 with std:59.21233215675538.\n",
      "15.517655975146937 1.9377033374779888\n",
      "The training loss is 6.897167028005505 with std:11.937002683723803. The val loss is 17.706127240963248 with std:93.18885543151956.\n",
      "17.706127240963248 1.9377033374779888\n",
      "The training loss is 8.231909104197156 with std:15.713022161188562. The val loss is 7.408577599129502 with std:9.481801172971634.\n",
      "7.408577599129502 1.9377033374779888\n",
      "The training loss is 7.5493797830674465 with std:12.08317584390008. The val loss is 10.60540274716311 with std:24.379271141276107.\n",
      "10.60540274716311 1.9377033374779888\n",
      "Evaluating for {'lmda': 1.955650715865949} ...\n",
      "The training loss is 7.616209004557288 with std:14.396803708601334. The val loss is 15.584292862636353 with std:59.796097683541305.\n",
      "15.584292862636353 1.955650715865949\n",
      "The training loss is 6.899382154161519 with std:11.945127965816376. The val loss is 17.70744874481438 with std:93.20301813932319.\n",
      "17.70744874481438 1.955650715865949\n",
      "The training loss is 8.237651921945293 with std:15.72881143608605. The val loss is 7.409136312516713 with std:9.486344637117124.\n",
      "7.409136312516713 1.955650715865949\n",
      "The training loss is 7.554193467044308 with std:12.094891571370416. The val loss is 10.61010250972619 with std:24.394815415899444.\n",
      "10.61010250972619 1.955650715865949\n",
      "Evaluating for {'lmda': 1.9737643263002553} ...\n",
      "The training loss is 7.621493566748615 with std:14.410473877462485. The val loss is 15.651519990801011 with std:60.385250001522905.\n",
      "15.651519990801011 1.9737643263002553\n",
      "The training loss is 6.901605809158487 with std:11.95331033510752. The val loss is 17.70866165494356 with std:93.21617643555183.\n",
      "17.70866165494356 1.9737643263002553\n",
      "The training loss is 8.243393110789523 with std:15.744631610325104. The val loss is 7.409695383685872 with std:9.49090688617981.\n",
      "7.409695383685872 1.9737643263002553\n",
      "The training loss is 7.559007414308763 with std:12.106618805619416. The val loss is 10.614814463865267 with std:24.410476871589132.\n",
      "10.614814463865267 1.9737643263002553\n",
      "Evaluating for {'lmda': 1.9920457084538692} ...\n",
      "The training loss is 7.6267812923331455 with std:14.424165787344217. The val loss is 15.719339647182826 with std:60.979802971722854.\n",
      "15.719339647182826 1.9920457084538692\n",
      "The training loss is 6.903838160508433 with std:11.961550290924448. The val loss is 17.709765835959338 with std:93.2283282348696.\n",
      "17.709765835959338 1.9920457084538692\n",
      "The training loss is 8.249132808704845 with std:15.760482697125287. The val loss is 7.410255029734347 with std:9.495488457745378.\n",
      "7.410255029734347 1.9920457084538692\n",
      "The training loss is 7.563821793806954 with std:12.118357861030345. The val loss is 10.6195392025535 with std:24.426256627148685.\n",
      "10.6195392025535 1.9920457084538692\n",
      "Evaluating for {'lmda': 2.0104964162604992} ...\n",
      "The training loss is 7.632072278460361 with std:14.43787927206054. The val loss is 15.787754075865912 with std:61.57977001315721.\n",
      "15.787754075865912 2.0104964162604992\n",
      "The training loss is 6.906079378365381 with std:11.969848339267854. The val loss is 17.710761154350493 with std:93.23947146035003.\n",
      "17.710761154350493 2.0104964162604992\n",
      "The training loss is 8.254871157164407 with std:15.776364716936842. The val loss is 7.410815472932778 with std:9.500089893460405.\n",
      "7.410815472932778 2.0104964162604992\n",
      "The training loss is 7.568636777512128 with std:12.13010905777725. The val loss is 10.624277324739412 with std:24.4421558164526.\n",
      "10.624277324739412 2.0104964162604992\n",
      "Evaluating for {'lmda': 2.029118018046678} ...\n",
      "The training loss is 7.637366625956769 with std:14.451614173867583. The val loss is 15.8567654766824 with std:62.185164096390295.\n",
      "15.8567654766824 2.029118018046678\n",
      "The training loss is 6.9083296355702615 with std:11.97820499289767. The val loss is 17.71164747850929 with std:93.24960404340881.\n",
      "17.71164747850929 2.029118018046678\n",
      "The training loss is 8.260608301159108 with std:15.792277697479244. The val loss is 7.411376940726548 with std:9.504711738934986.\n",
      "7.411376940726548 2.029118018046678\n",
      "The training loss is 7.573452540460452 with std:12.141872721849973. The val loss is 10.629029435354036 with std:24.458175588424673.\n",
      "10.629029435354036 2.029118018046678\n",
      "Evaluating for {'lmda': 2.0479120966650854} ...\n",
      "The training loss is 7.642664439361962 with std:14.465370343544556. The val loss is 15.926376004434571 with std:62.79599773720019.\n",
      "15.926376004434571 2.0479120966650854\n",
      "The training loss is 6.910589107696402 with std:11.986620771419101. The val loss is 17.712424678754253 with std:93.25872392372399.\n",
      "17.712424678754253 2.0479120966650854\n",
      "The training loss is 8.266344389218297 with std:15.808221673779576. The val loss is 7.411939665737836 with std:9.509354543651211.\n",
      "7.411939665737836 2.0479120966650854\n",
      "The training loss is 7.5782692607872475 with std:12.153649185078837. The val loss is 10.633796145320607 with std:24.474317107019868.\n",
      "10.633796145320607 2.0479120966650854\n",
      "Evaluating for {'lmda': 2.066880249629082} ...\n",
      "The training loss is 7.64796582696517 with std:14.479147640473743. The val loss is 15.996587768112661 with std:63.41228299016889.\n",
      "15.996587768112661 2.066880249629082\n",
      "The training loss is 6.912857973096151 with std:11.995096201369945. The val loss is 17.713092627356772 with std:93.26682904918277.\n",
      "17.713092627356772 2.066880249629082\n",
      "The training loss is 8.272079573431242 with std:15.82419668821041. The val loss is 7.412503885767275 with std:9.514018860873874.\n",
      "7.412503885767275 2.066880249629082\n",
      "The training loss is 7.5830871197642855 with std:12.165438785160607. The val loss is 10.63857807156618 with std:24.490581551206496.\n",
      "10.63857807156618 2.066880249629082\n",
      "Evaluating for {'lmda': 2.086024089248503} ...\n",
      "The training loss is 7.653270900841951 with std:14.492945932718088. The val loss is 16.067402830104967 with std:64.03403144216252.\n",
      "16.067402830104967 2.086024089248503\n",
      "The training loss is 6.915136412948251 with std:12.003631816308129. The val loss is 17.71365119856795 with std:93.27391737582637.\n",
      "17.71365119856795 2.086024089248503\n",
      "The training loss is 8.277814009469312 with std:15.840202790527579. The val loss is 7.413069843795704 with std:9.518705247565467.\n",
      "7.413069843795704 2.086024089248503\n",
      "The training loss is 7.587906301838044 with std:12.177241865684675. The val loss is 10.643375837035396 with std:24.506970114949787.\n",
      "10.643375837035396 2.086024089248503\n",
      "Evaluating for {'lmda': 2.105345242766706} ...\n",
      "The training loss is 7.658579776891684 with std:14.506765097098613. The val loss is 16.138823205428128 with std:64.66125420596053.\n",
      "16.138823205428128 2.105345242766706\n",
      "The training loss is 6.917424611306164 with std:12.012228156900832. The val loss is 17.714100268648025 with std:93.27998686780819.\n",
      "17.714100268648025 2.105345242766706\n",
      "The training loss is 8.283547856609314 with std:15.856240037907959. The val loss is 7.4136377879865245 with std:9.523414264305535.\n",
      "7.4136377879865245 2.105345242766706\n",
      "The training loss is 7.592726994668694 with std:12.189058776159786. The val loss is 10.648190070706237 with std:24.523484007196572.\n",
      "10.648190070706237 2.105345242766706\n",
      "Evaluating for {'lmda': 2.124845352498883} ...\n",
      "The training loss is 7.663892574875341 with std:14.520605019269592. The val loss is 16.21085086093273 with std:65.29396191362213.\n",
      "16.21085086093273 2.124845352498883\n",
      "The training loss is 6.919722755147314 with std:12.020885771013345. The val loss is 17.71443971589758 with std:93.28503549736429.\n",
      "17.71443971589758 2.124845352498883\n",
      "The training loss is 8.289281277757421 with std:15.872308494986708. The val loss is 7.414207971687582 with std:9.5281464752132.\n",
      "7.414207971687582 2.124845352498883\n",
      "The training loss is 7.59754938917012 with std:12.200889872041909. The val loss is 10.653021407607513 with std:24.540124451860237.\n",
      "10.653021407607513 2.124845352498883\n",
      "Evaluating for {'lmda': 2.1445260759716676} ...\n",
      "The training loss is 7.669209418453807 with std:14.534465593793032. The val loss is 16.283487714536314 with std:65.93216471006511.\n",
      "16.283487714536314 2.1445260759716676\n",
      "The training loss is 6.922031034423241 with std:12.029605213799694. The val loss is 17.71466942068974 with std:93.2890612447836.\n",
      "17.71466942068974 2.1445260759716676\n",
      "The training loss is 8.29501443947429 with std:15.888408233894479. The val loss is 7.414780653433805 with std:9.53290244787511.\n",
      "7.414780653433805 2.1445260759716676\n",
      "The training loss is 7.602373679550721 with std:12.212735514761897. The val loss is 10.657870488839118 with std:24.55689268780798.\n",
      "10.657870488839118 2.1445260759716676\n",
      "Evaluating for {'lmda': 2.1643890860640203} ...\n",
      "The training loss is 7.674530435226545 with std:14.548346724210699. The val loss is 16.356735634439865 with std:66.57587224646059.\n",
      "16.356735634439865 2.1643890860640203\n",
      "The training loss is 6.92434964211071 with std:12.03838704779347. The val loss is 17.714789265505864 with std:93.29206209840179.\n",
      "17.714789265505864 2.1643890860640203\n",
      "The training loss is 8.300747512000914 with std:15.904539334294695. The val loss is 7.41535609694953 with std:9.53768275327602.\n",
      "7.41535609694953 2.1643890860640203\n",
      "The training loss is 7.607200063355404 with std:12.224596071755101. The val loss is 10.662737961593567 with std:24.5737899688479.\n",
      "10.662737961593567 2.1643890860640203\n",
      "Evaluating for {'lmda': 2.184436071149426} ...\n",
      "The training loss is 7.679855756771263 with std:14.562248323117222. The val loss is 16.43059643835294 with std:67.22509367366455.\n",
      "16.43059643835294 2.184436071149426\n",
      "The training loss is 6.9266787742637765 with std:12.047231842999473. The val loss is 17.714799134970825 with std:93.29403605458765.\n",
      "17.714799134970825 2.184436071149426\n",
      "The training loss is 8.306480669285536 with std:15.920701883420673. The val loss is 7.415934571151542 with std:9.542487965735226.\n",
      "7.415934571151542 2.184436071149426\n",
      "The training loss is 7.612028741508109 with std:12.236471916489958. The val loss is 10.6676244791809 with std:24.59081756371958.\n",
      "10.6676244791809 2.184436071149426\n",
      "Evaluating for {'lmda': 2.2046687352394096} ...\n",
      "The training loss is 7.685185518683544 with std:14.576170312229321. The val loss is 16.50507189272425 with std:67.8798376356699.\n",
      "16.50507189272425 2.2046687352394096\n",
      "The training loss is 6.929018630066851 with std:12.056140176986426. The val loss is 17.71469891589265 with std:93.2949811177573.\n",
      "17.71469891589265 2.2046687352394096\n",
      "The training loss is 8.31221408901151 with std:15.936895976112474. The val loss is 7.416516350152124 with std:9.547318662846317.\n",
      "7.416516350152124 2.2046687352394096\n",
      "The training loss is 7.616859918355757 with std:12.248363428498973. The val loss is 10.672530701054539 with std:24.607976756084025.\n",
      "10.672530701054539 2.2046687352394096\n",
      "Evaluating for {'lmda': 2.2250887981283696} ...\n",
      "The training loss is 7.690519860617483 with std:14.590112622455642. The val loss is 16.58016371197262 with std:68.54011226302151.\n",
      "16.58016371197262 2.2250887981283696\n",
      "The training loss is 6.931369411888597 with std:12.065112634980075. The val loss is 17.7144884973016 with std:93.29489530037843.\n",
      "17.7144884973016 2.2250887981283696\n",
      "The training loss is 8.317947952626318 with std:15.95312171485471. The val loss is 7.41710171326243 with std:9.552175425421934.\n",
      "7.41710171326243 2.2250887981283696\n",
      "The training loss is 7.621693801712775 with std:12.260270993409131. The val loss is 10.677457292840145 with std:24.625268844515322.\n",
      "10.677457292840145 2.2250887981283696\n",
      "Evaluating for {'lmda': 2.245697995539774} ...\n",
      "The training loss is 7.695858926326622 with std:14.604075193964423. The val loss is 16.655873557719488 with std:69.20592516621194.\n",
      "16.655873557719488 2.245697995539774\n",
      "The training loss is 6.933731325337061 with std:12.074149809957378. The val loss is 17.714167770493244 with std:93.29377662300182.\n",
      "17.714167770493244 2.245697995539774\n",
      "The training loss is 8.323682445371459 with std:15.969379209813155. The val loss is 7.4176909449965684 with std:9.557058837443444.\n",
      "7.4176909449965684 2.245697995539774\n",
      "The training loss is 7.626530602906861 with std:12.27219500297407. The val loss is 10.682404926366592 with std:24.642695142494652.\n",
      "10.682404926366592 2.245697995539774\n",
      "Evaluating for {'lmda': 2.266498079273693} ...\n",
      "The training loss is 7.701202863705648 with std:14.618057976250528. The val loss is 16.73220303802692 with std:69.8772834290921.\n",
      "16.73220303802692 2.266498079273693\n",
      "The training loss is 6.936104579315569 with std:12.08325230274125. The val loss is 17.713736629071633 with std:93.29162311428811.\n",
      "17.713736629071633 2.266498079273693\n",
      "The training loss is 8.329417756313253 with std:15.985668578872188. The val loss is 7.418284335075376 with std:9.56196948601419.\n",
      "7.418284335075376 2.266498079273693\n",
      "The training loss is 7.631370536825629 with std:12.28413585510614. The val loss is 10.687374279699362 with std:24.660256978405425.\n",
      "10.687374279699362 2.266498079273693\n",
      "Evaluating for {'lmda': 2.287490817355702} ...\n",
      "The training loss is 7.70655182483279 with std:14.632060928201383. The val loss is 16.809153706638057 with std:70.55419360226678.\n",
      "16.809153706638057 2.287490817355702\n",
      "The training loss is 6.938489386079864 with std:12.092420722096158. The val loss is 17.713194968996625 with std:93.28843281106062.\n",
      "17.713194968996625 2.287490817355702\n",
      "The training loss is 8.335154078375194 with std:16.00198994767247. The val loss is 7.418882178431272 with std:9.56690796131871.\n",
      "7.418882178431272 2.287490817355702\n",
      "The training loss is 7.636213821964219 with std:12.296093953909798. The val loss is 10.6923660371755 with std:24.677955695528315.\n",
      "10.6923660371755 2.287490817355702\n",
      "Evaluating for {'lmda': 2.30867799418717} ...\n",
      "The training loss is 7.711905966012436 with std:14.646084018160952. The val loss is 16.886727062223713 with std:71.23666169651804.\n",
      "16.886727062223713 2.30867799418717\n",
      "The training loss is 6.940885961296014 with std:12.101655684824411. The val loss is 17.712542688630013 with std:93.28420375834357.\n",
      "17.712542688630013 2.30867799418717\n",
      "The training loss is 8.340891608370942 with std:16.018343449648444. The val loss is 7.419484775213391 with std:9.57187485658627.\n",
      "7.419484775213391 2.30867799418717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 7.641060680474126 with std:12.308069709715836. The val loss is 10.697380889442499 with std:24.69579265204148.\n",
      "10.697380889442499 2.30867799418717\n",
      "Evaluating for {'lmda': 2.3300614106969246} ...\n",
      "The training loss is 7.717265447819128 with std:14.660127223994312. The val loss is 16.964924547637164 with std:71.92469317626016.\n",
      "16.964924547637164 2.3300614106969246\n",
      "The training loss is 6.94329452409971 with std:12.110957815863353. The val loss is 17.711779688787466 with std:93.27893400944112.\n",
      "17.711779688787466 2.3300614106969246\n",
      "The training loss is 8.346630547038602 with std:16.034729226066343. The val loss is 7.420092430792857 with std:9.576870768058864.\n",
      "7.420092430792857 2.3300614106969246\n",
      "The training loss is 7.645911338212854 with std:12.320063539115916. The val loss is 10.702419533497682 with std:24.71376922101802.\n",
      "10.702419533497682 2.3300614106969246\n",
      "Evaluating for {'lmda': 2.3516428844943484} ...\n",
      "The training loss is 7.722630435141466 with std:14.674190533149293. The val loss is 17.043747549164955 with std:72.61829295293377.\n",
      "17.043747549164955 2.3516428844943484\n",
      "The training loss is 6.945715297156208 with std:12.120327748382987. The val loss is 17.710905872788505 with std:93.27262162599379.\n",
      "17.710905872788505 2.3516428844943484\n",
      "The training loss is 8.35237109907629 with std:16.051147426062194. The val loss is 7.420705455769134 with std:9.581896294965237.\n",
      "7.420705455769134 2.3516428844943484\n",
      "The training loss is 7.650766024794634 with std:12.332075864998398. The val loss is 10.707482672731395 with std:24.731886790428376.\n",
      "10.707482672731395 2.3516428844943484\n",
      "Evaluating for {'lmda': 2.373424250023866} ...\n",
      "The training loss is 7.728001097227207 with std:14.688273942718174. The val loss is 17.123197395789187 with std:73.31746537846429.\n",
      "17.123197395789187 2.373424250023866\n",
      "The training loss is 6.948148506721773 with std:12.129766123884746. The val loss is 17.709921146511657 with std:93.26526467807167.\n",
      "17.709921146511657 2.373424250023866\n",
      "The training loss is 8.358113473178525 with std:16.06759820668027. The val loss is 7.421324165976451 with std:9.58695203949884.\n",
      "7.421324165976451 2.373424250023866\n",
      "The training loss is 7.655624973642333 with std:12.344107116584789. The val loss is 10.712571016971973 with std:24.750146763142812.\n",
      "10.712571016971973 2.373424250023866\n",
      "Evaluating for {'lmda': 2.395407358720877} ...\n",
      "The training loss is 7.7333776077289995 with std:14.70237745949866. The val loss is 17.203275358460225 with std:74.02221423878377.\n",
      "17.203275358460225 2.395407358720877\n",
      "The training loss is 6.950594382705885 with std:12.139273592300727. The val loss is 17.708825418449706 with std:93.2568612442581.\n",
      "17.708825418449706 2.395407358720877\n",
      "The training loss is 8.363857882074253 with std:16.08408173291213. The val loss is 7.4219488824912085 with std:9.592038606801783.\n",
      "7.4219488824912085 2.395407358720877\n",
      "The training loss is 7.660488422040508 with std:12.356157729467622. The val loss is 10.717685282533607 with std:24.768550556936727.\n",
      "10.717685282533607 2.395407358720877\n",
      "Evaluating for {'lmda': 2.4175940791691306} ...\n",
      "The training loss is 7.738760144750689 with std:14.716501100052874. The val loss is 17.283982649359427 with std:74.73254274723163.\n",
      "17.283982649359427 2.4175940791691306\n",
      "The training loss is 6.953053158734686 with std:12.148850812093661. The val loss is 17.707618599768065 with std:93.24740941176016.\n",
      "17.707618599768065 2.4175940791691306\n",
      "The training loss is 8.369604542565623 with std:16.100598177735463. The val loss is 7.422579931639589 with std:9.597156604953286.\n",
      "7.422579931639589 2.4175940791691306\n",
      "The training loss is 7.66535661118908 with std:12.368228145648088. The val loss is 10.722826192266051 with std:24.787099604495566.\n",
      "10.722826192266051 2.4175940791691306\n",
      "Evaluating for {'lmda': 2.43998629725955} ...\n",
      "The training loss is 7.744148890894646 with std:14.730644890766595. The val loss is 17.365320421190148 with std:75.44845353817541.\n",
      "17.365320421190148 2.43998629725955\n",
      "The training loss is 6.955525072215597 with std:12.158498450357873. The val loss is 17.706300604364362 with std:93.23690727651187.\n",
      "17.706300604364362 2.43998629725955\n",
      "The training loss is 8.375353675568396 with std:16.117147722153828. The val loss is 7.423217645006402 with std:9.602306644964061.\n",
      "7.423217645006402 2.43998629725955\n",
      "The training loss is 7.670229786258723 with std:12.380318813575581. The val loss is 10.727994475607348 with std:24.805795353423886.\n",
      "10.727994475607348 2.43998629725955\n",
      "Evaluating for {'lmda': 2.462585916350547} ...\n",
      "The training loss is 7.749544033309563 with std:14.744808867906377. The val loss is 17.447289766457963 with std:76.16994866050696.\n",
      "17.447289766457963 2.462585916350547\n",
      "The training loss is 6.958010364402907 with std:12.168217182920493. The val loss is 17.70487134893044 with std:93.2253529432984.\n",
      "17.70487134893044 2.462585916350547\n",
      "The training loss is 8.381105506153347 with std:16.133730555236806. The val loss is 7.423862359444146 with std:9.607489340775329.\n",
      "7.423862359444146 2.462585916350547\n",
      "The training loss is 7.6751081964469625 with std:12.392430188187907. The val loss is 10.73319086863923 with std:24.824639266255367.\n",
      "10.73319086863923 2.462585916350547\n",
      "Evaluating for {'lmda': 2.4853948574297986} ...\n",
      "The training loss is 7.754945763739357 with std:14.758993077676994. The val loss is 17.529891716770326 with std:76.8970295712793.\n",
      "17.529891716770326 2.4853948574297986\n",
      "The training loss is 6.960509280464693 with std:12.178007694443984. The val loss is 17.70333075301671 with std:93.21274452588871.\n",
      "17.70333075301671 2.4853948574297986\n",
      "The training loss is 8.386860263589142 with std:16.150346874160487. The val loss is 7.424514417082943 with std:9.612705309263555.\n",
      "7.424514417082943 2.4853948574297986\n",
      "The training loss is 7.679992095035605 with std:12.404562730952415. The val loss is 10.738416114144307 with std:24.84363282046368.\n",
      "10.738416114144307 2.4853948574297986\n",
      "Evaluating for {'lmda': 2.5084150592775387} ...\n",
      "The training loss is 7.760354278572729 with std:14.773197576277415. The val loss is 17.61312724213586 with std:77.62969712930926.\n",
      "17.61312724213586 2.5084150592775387\n",
      "The training loss is 6.963022069550753 with std:12.187870678528832. The val loss is 17.70167873909828 with std:93.19908014717664.\n",
      "17.70167873909828 2.5084150592775387\n",
      "The training loss is 8.392618181386249 with std:16.1669968842483. The val loss is 7.425174165341533 with std:9.617955170250777.\n",
      "7.425174165341533 2.5084150592775387\n",
      "The training loss is 7.684881739449126 with std:12.41671690990771. The val loss is 10.743670961666883 with std:24.862777508476807.\n",
      "10.743670961666883 2.5084150592775387\n",
      "Evaluating for {'lmda': 2.5316484786313556} ...\n",
      "The training loss is 7.765769778893583 with std:14.787422429955956. The val loss is 17.69699725027416 with std:78.3679515888462.\n",
      "17.69699725027416 2.5316484786313556\n",
      "The training loss is 6.965548984861839 with std:12.197806837817415. The val loss is 17.699915232642795 with std:93.18435793932827.\n",
      "17.699915232642795 2.5316484786313556\n",
      "The training loss is 8.398379497342596 with std:16.183680799013192. The val loss is 7.425841956938598 with std:9.623239546519661.\n",
      "7.425841956938598 2.5316484786313556\n",
      "The training loss is 7.689777391314492 with std:12.428893199707176. The val loss is 10.74895616757538 with std:24.882074837692112.\n",
      "10.74895616757538 2.5316484786313556\n",
      "Evaluating for {'lmda': 2.555097090352507} ...\n",
      "The training loss is 7.771192470532374 with std:14.80166771506508. The val loss is 17.78150258593818 with std:79.11179259332319.\n",
      "17.78150258593818 2.555097090352507\n",
      "The training loss is 6.968090283719943 with std:12.207816884097927. The val loss is 17.698040162181556 with std:93.1685760439493.\n",
      "17.698040162181556 2.555097090352507\n",
      "The training loss is 8.404144453590025 with std:16.200398840199124. The val loss is 7.42651814990547 with std:9.628559063834956.\n",
      "7.42651814990547 2.555097090352507\n",
      "The training loss is 7.694679316521775 with std:12.441092081662521. The val loss is 10.754272495127896 with std:24.901526330492665.\n",
      "10.754272495127896 2.555097090352507\n",
      "Evaluating for {'lmda': 2.578762887593801} ...\n",
      "The training loss is 7.776622564118327 with std:14.81593351811526. The val loss is 17.866644030245116 with std:79.86121916915462.\n",
      "17.866644030245116 2.578762887593801\n",
      "The training loss is 6.970646227640006 with std:12.217901538410002. The val loss is 17.696053459381556 with std:93.15173261225053.\n",
      "17.696053459381556 2.578762887593801\n",
      "The training loss is 8.409913296642769 with std:16.217151237824936. The val loss is 7.427203107599207 with std:9.63391435096976.\n",
      "7.427203107599207 2.578762887593801\n",
      "The training loss is 7.699587785286412 with std:12.453314043789183. The val loss is 10.759620714540683 with std:24.92113352426857.\n",
      "10.759620714540683 2.578762887593801\n",
      "Evaluating for {'lmda': 2.6026478819690047} ...\n",
      "The training loss is 7.782060275132486 with std:14.830219935827781. The val loss is 17.952422300002006 with std:80.61622971945533.\n",
      "17.952422300002006 2.6026478819690047\n",
      "The training loss is 6.973217082402579 with std:12.22806153114982. The val loss is 17.693955059120196 with std:93.1338258052322.\n",
      "17.693955059120196 2.6026478819690047\n",
      "The training loss is 8.415686277446659 with std:16.23393823022682. The val loss is 7.427897198717157 with std:9.63930603973788.\n",
      "7.427897198717157 2.6026478819690047\n",
      "The training loss is 7.70450307221226 with std:12.465559580852311. The val loss is 10.765001603058359 with std:24.940897971435522.\n",
      "10.765001603058359 2.6026478819690047\n",
      "Evaluating for {'lmda': 2.6267541037238358} ...\n",
      "The training loss is 7.787505823961893 with std:14.844527075187788. The val loss is 18.038838047071906 with std:81.37682201810111.\n",
      "18.038838047071906 2.6267541037238358\n",
      "The training loss is 6.975803118128043 with std:12.238297602177003. The val loss is 17.691744899562686 with std:93.11485379387652.\n",
      "17.691744899562686 2.6267541037238358\n",
      "The training loss is 8.421463651430448 with std:16.250760064103975. The val loss is 7.428600797312219 with std:9.64473476503169.\n",
      "7.428600797312219 2.6267541037238358\n",
      "The training loss is 7.709425456356127 with std:12.477829194413852. The val loss is 10.770415945028091 with std:24.960821239459932.\n",
      "10.770415945028091 2.6267541037238358\n",
      "Evaluating for {'lmda': 2.651083601908539} ...\n",
      "The training loss is 7.792959435954557 with std:14.858855053496166. The val loss is 18.125891857717534 with std:82.14299320354924.\n",
      "18.125891857717534 2.651083601908539\n",
      "The training loss is 6.9784046093517755 with std:12.248610500921364. The val loss is 17.68942292224001 with std:93.09481475934113.\n",
      "17.68942292224001 2.651083601908539\n",
      "The training loss is 8.427245678557856 with std:16.267616994562403. The val loss is 7.429314282808794 with std:9.650201164864962.\n",
      "7.429314282808794 2.651083601908539\n",
      "The training loss is 7.71435522129337 with std:12.490123392880635. The val loss is 10.775864531975104 with std:24.98090491088254.\n",
      "10.775864531975104 2.651083601908539\n",
      "Evaluating for {'lmda': 2.675638444552045} ...\n",
      "The training loss is 7.798421341475435 with std:14.873203998420342. The val loss is 18.213584251977707 with std:82.91473977293431.\n",
      "18.213584251977707 2.675638444552045\n",
      "The training loss is 6.981021835100829 with std:12.259000986490708. The val loss is 17.686989072131418 with std:93.07370689318027.\n",
      "17.686989072131418 2.675638444552045\n",
      "The training loss is 8.433032623381651 with std:16.284509285161295. The val loss is 7.430038040020645 with std:9.655705880422596.\n",
      "7.430038040020645 2.675638444552045\n",
      "The training loss is 7.719292655185018 with std:12.502442691554153. The val loss is 10.781348162681905 with std:25.00115058334536.\n",
      "10.781348162681905 2.675638444552045\n",
      "Evaluating for {'lmda': 2.7004207188377727} ...\n",
      "The training loss is 7.803891775963558 with std:14.887574048046108. The val loss is 18.301915683041877 with std:83.69205757611104.\n",
      "18.301915683041877 2.7004207188377727\n",
      "The training loss is 6.983655078971813 with std:12.26946982777937. The val loss is 17.684443297746924 with std:93.05152839755839.\n",
      "17.684443297746924 2.7004207188377727\n",
      "The training loss is 8.438824755098985 with std:16.301437207960088. The val loss is 7.430772459168945 with std:9.66124955611499.\n",
      "7.430772459168945 2.7004207188377727\n",
      "The training loss is 7.724238050845751 with std:12.514787612680436. The val loss is 10.786867643270282 with std:25.021559869622656.\n",
      "10.786867643270282 2.7004207188377727\n",
      "Evaluating for {'lmda': 2.7254325312810277} ...\n",
      "The training loss is 7.809370979989938 with std:14.90196535092702. The val loss is 18.39088653664961 with std:84.47494180990226.\n",
      "18.39088653664961 2.7254325312810277\n",
      "The training loss is 6.986304629210059 with std:12.280017803577127. The val loss is 17.68178555121383 with std:93.02827748548756.\n",
      "17.68178555121383 2.7254325312810277\n",
      "The training loss is 8.444622347608187 with std:16.318401043565274. The val loss is 7.431517935902079 with std:9.666832839638403.\n",
      "7.431517935902079 2.7254325312810277\n",
      "The training loss is 7.729191705813444 with std:12.52715868550159. The val loss is 10.792423787284712 with std:25.04213439764777.\n",
      "10.792423787284712 2.7254325312810277\n",
      "Evaluating for {'lmda': 2.7506760079080648} ...\n",
      "The training loss is 7.814859199316923 with std:14.91637806613521. The val loss is 18.480497130486484 with std:85.2633870122763.\n",
      "18.480497130486484 2.7506760079080648\n",
      "The training loss is 6.988970778790186 with std:12.290645702678828. The val loss is 17.679015788363767 with std:93.00395238106091.\n",
      "17.679015788363767 2.7506760079080648\n",
      "The training loss is 8.450425679567363 with std:16.335401081179636. The val loss is 7.432274871316409 with std:9.672456382041563.\n",
      "7.432274871316409 2.7506760079080648\n",
      "The training loss is 7.734153922420086 with std:12.539556446308996. The val loss is 10.798017415780832 with std:25.062875810550935.\n",
      "10.798017415780832 2.7506760079080648\n",
      "Evaluating for {'lmda': 2.776153294436801} ...\n",
      "The training loss is 7.820356684958117 with std:14.930812363310002. The val loss is 18.57074771360366 with std:86.05738705672118.\n",
      "18.57074771360366 2.776153294436801\n",
      "The training loss is 6.991653825497976 with std:12.301354323994852. The val loss is 17.67613396882373 with std:92.97855131971028.\n",
      "17.67613396882373 2.776153294436801\n",
      "The training loss is 8.456235034454151 with std:16.352437618650864. The val loss is 7.43304367197816 with std:9.678120837796985.\n",
      "7.43304367197816 2.776153294436801\n",
      "The training loss is 7.739125007863757 with std:12.551981438496671. The val loss is 10.80364935741437 with std:25.083785766690244.\n",
      "10.80364935741437 2.776153294436801\n",
      "Evaluating for {'lmda': 2.801866556459195} ...\n",
      "The training loss is 7.825863693239916 with std:14.94526842270757. The val loss is 18.661638465849062 with std:86.85693514668716.\n",
      "18.661638465849062 2.801866556459195\n",
      "The training loss is 6.994354072013599 with std:12.312144476661866. The val loss is 17.673140056108046 with std:92.95207254846474.\n",
      "17.673140056108046 2.801866556459195\n",
      "The training loss is 8.462050700627541 with std:16.369510962522842. The val loss is 7.43382474994698 with std:9.683826864879185.\n",
      "7.43382474994698 2.801866556459195\n",
      "The training loss is 7.744105274281954 with std:12.56443421261611. The val loss is 10.809320448534338 with std:25.10486593969033.\n",
      "10.809320448534338 2.801866556459195\n",
      "Evaluating for {'lmda': 2.8278179796253413} ...\n",
      "The training loss is 7.831380485863934 with std:14.959746435250018. The val loss is 18.753169497306512 with std:87.66202381007979.\n",
      "18.753169497306512 2.8278179796253413\n",
      "The training loss is 6.997071825996259 with std:12.323016980154572. The val loss is 17.67003401771341 with std:92.92451432622096.\n",
      "17.67003401771341 2.8278179796253413\n",
      "The training loss is 8.467872971390879 with std:16.386621428086134. The val loss is 7.434618522800159 with std:9.689575124847568.\n",
      "7.434618522800159 2.8278179796253413\n",
      "The training loss is 7.749095038826682 with std:12.576915326433012. The val loss is 10.81503153327873 with std:25.126118018481048.\n",
      "10.81503153327873 2.8278179796253413\n",
      "Evaluating for {'lmda': 2.8540097698292373} ...\n",
      "The training loss is 7.836907329970522 with std:14.974246602573096. The val loss is 18.845340847751185 with std:88.47264489386973.\n",
      "18.845340847751185 2.8540097698292373\n",
      "The training loss is 6.999807400170241 with std:12.333972664397779. The val loss is 17.666815825216094 with std:92.8958749240274.\n",
      "17.666815825216094 2.8540097698292373\n",
      "The training loss is 8.473702145056755 with std:16.403769339430816. The val loss is 7.435425413658908 with std:9.695366282935915.\n",
      "7.435425413658908 2.8540097698292373\n",
      "The training loss is 7.754094623740237 with std:12.589425344984228. The val loss is 10.820783463672948 with std:25.147543707339167.\n",
      "10.820783463672948 2.8540097698292373\n",
      "Evaluating for {'lmda': 2.8804441533962977} ...\n",
      "The training loss is 7.842444498203724 with std:14.988769137075465. The val loss is 18.938152486124437 with std:89.28878955885334.\n",
      "18.938152486124437 2.8804441533962977\n",
      "The training loss is 7.002561112412309 with std:12.34501236987894. The val loss is 17.663485454370896 with std:92.86615262536884.\n",
      "17.663485454370896 2.8804441533962977\n",
      "The training loss is 8.479538525013522 with std:16.420955029500202. The val loss is 7.436245851215431 with std:9.701201008146779.\n",
      "7.436245851215431 2.8804441533962977\n",
      "The training loss is 7.7591043564330215 with std:12.601964840636589. The val loss is 10.826577099730885 with std:25.169144725929762.\n",
      "10.826577099730885 2.8804441533962977\n",
      "Evaluating for {'lmda': 2.9071233772725784} ...\n",
      "The training loss is 7.84799226877708 with std:15.003314261965587. The val loss is 19.031604310011616 with std:90.11044827441174.\n",
      "19.031604310011616 2.9071233772725784\n",
      "The training loss is 7.005333285840726 with std:12.356136947761875. The val loss is 17.660042885213414 with std:92.83534572647316.\n",
      "17.660042885213414 2.9071233772725784\n",
      "The training loss is 8.485382419793478 with std:16.43817884014519. The val loss is 7.4370802697619975 with std:9.707079973352702.\n",
      "7.4370802697619975 2.9071233772725784\n",
      "The training loss is 7.764124569562364 with std:12.614534393146846. The val loss is 10.83241330955955 with std:25.19092280935349.\n",
      "10.83241330955955 2.9071233772725784\n",
      "Evaluating for {'lmda': 2.934049709215787} ...\n",
      "The training loss is 7.853550925541222 with std:15.017882211310837. The val loss is 19.125696145148034 with std:90.93761081349592.\n",
      "19.125696145148034 2.934049709215787\n",
      "The training loss is 7.008124248905468 with std:12.367347260000344. The val loss is 17.65648810216396 with std:92.80345253661993.\n",
      "17.65648810216396 2.934049709215787\n",
      "The training loss is 8.491234143142634 with std:16.45544112217979. The val loss is 7.437929109220839 with std:9.713003855401666.\n",
      "7.437929109220839 2.934049709215787\n",
      "The training loss is 7.769155601112817 with std:12.627134589722433. The val loss is 10.838292969465469 with std:25.212879708191018.\n",
      "10.838292969465469 2.934049709215787\n",
      "Evaluating for {'lmda': 2.9612254379880345} ...\n",
      "The training loss is 7.859120758052134 with std:15.032473230084433. The val loss is 19.220427744933943 with std:91.77026624767376.\n",
      "19.220427744933943 2.9612254379880345\n",
      "The training loss is 7.010934335480156 with std:12.378644179452591. The val loss is 17.65282109413396 with std:92.77047137845777.\n",
      "17.65282109413396 2.9612254379880345\n",
      "The training loss is 8.49709401409254 with std:16.472742235438698. The val loss is 7.438792815176282 with std:9.718973335229652.\n",
      "7.438792815176282 2.9612254379880345\n",
      "The training loss is 7.77419779447808 with std:12.639766025084313. The val loss is 10.844216964065332 with std:25.235017188554252.\n",
      "10.844216964065332 2.9612254379880345\n",
      "Evaluating for {'lmda': 2.988652873550383} ...\n",
      "The training loss is 7.864702061641096 with std:15.047087574213466. The val loss is 19.31579878995807 with std:92.60840294221852.\n",
      "19.31579878995807 2.988652873550383\n",
      "The training loss is 7.013763884955335 with std:12.390028589996325. The val loss is 17.649041854635225 with std:92.73640058834137.\n",
      "17.649041854635225 2.988652873550383\n",
      "The training loss is 8.502962357033484 with std:16.490082548834867. The val loss is 7.439671838907981 with std:9.724989097978124.\n",
      "7.439671838907981 2.988652873550383\n",
      "The training loss is 7.7792514985441255 with std:12.65242930153042. The val loss is 10.85018618639849 with std:25.257337032136952.\n",
      "10.85018618639849 2.988652873550383\n",
      "Evaluating for {'lmda': 3.0163343472591975} ...\n",
      "The training loss is 7.870295137485908 with std:15.061725510626593. The val loss is 19.411808887555964 with std:93.45200855149442.\n",
      "19.411808887555964 3.0163343472591975\n",
      "The training loss is 7.016613242333417 with std:12.401501386644396. The val loss is 17.645150381890748 with std:92.70123851666338.\n",
      "17.645150381890748 3.0163343472591975\n",
      "The training loss is 8.508839501789792 with std:16.507462440419168. The val loss is 7.440566637425805 with std:9.731051833117057.\n",
      "7.440566637425805 3.0163343472591975\n",
      "The training loss is 7.784317067774046 with std:12.665125029000823. The val loss is 10.856201538043008 with std:25.279841036268092.\n",
      "10.856201538043008 3.0163343472591975\n",
      "Evaluating for {'lmda': 3.044272212064303} ...\n",
      "The training loss is 7.875900292683333 with std:15.076387317301561. The val loss is 19.508457571363163 with std:94.30107001426302.\n",
      "19.508457571363163 3.044272212064303\n",
      "The training loss is 7.019482758325015 with std:12.413063475660769. The val loss is 17.641146678949045 with std:92.66498352821215.\n",
      "17.641146678949045 3.044272212064303\n",
      "The training loss is 8.514725783696726 with std:16.52488229744103. The val loss is 7.4414776735066805 with std:9.737162234573926.\n",
      "7.4414776735066805 3.044272212064303\n",
      "The training loss is 7.789394862294067 with std:12.677853825143709. The val loss is 10.86226392923413 with std:25.302531013966362.\n",
      "10.86226392923413 3.044272212064303\n",
      "Evaluating for {'lmda': 3.0724688427090037} ...\n",
      "The training loss is 7.881517840323074 with std:15.091073283313076. The val loss is 19.605744300907052 with std:95.15557354931849.\n",
      "19.605744300907052 3.0724688427090037\n",
      "The training loss is 7.022372789446934 with std:12.424715774676976. The val loss is 17.637030753799586 with std:92.62763400252258.\n",
      "17.637030753799586 3.0724688427090037\n",
      "The training loss is 8.520621543679376 with std:16.542342516410425. The val loss is 7.44240541573275 with std:9.743321000868146.\n",
      "7.44240541573275 3.0724688427090037\n",
      "The training loss is 7.7944852479816165 with std:12.690616315383732. The val loss is 10.868374278986016 with std:25.32540879399755.\n",
      "10.868374278986016 3.0724688427090037\n",
      "Evaluating for {'lmda': 3.1009266359319265} ...\n",
      "The training loss is 7.88714809956305 with std:15.1057837088805. The val loss is 19.703668461195793 with std:96.01550465106608.\n",
      "19.703668461195793 3.1009266359319265\n",
      "The training loss is 7.025283698121685 with std:12.436459212809423. The val loss is 17.632802619491798 with std:92.58918833425022.\n",
      "17.632802619491798 3.1009266359319265\n",
      "The training loss is 8.52652712833331 with std:16.559843503161083. The val loss is 7.443350338531644 with std:9.749528835251045.\n",
      "7.443350338531644 3.1009266359319265\n",
      "The training loss is 7.79958859655422 with std:12.70341313299047. The val loss is 10.874533515216834 with std:25.348476220935368.\n",
      "10.874533515216834 3.1009266359319265\n",
      "Evaluating for {'lmda': 3.1296480106707505} ...\n",
      "The training loss is 7.892791395706255 with std:15.12051890541596. The val loss is 19.80222936234319 with std:96.88084808540319.\n",
      "19.80222936234319 3.1296480106707505\n",
      "The training loss is 7.028215852778708 with std:12.448294730776725. The val loss is 17.628462294256607 with std:92.54964493355286.\n",
      "17.628462294256607 3.1296480106707505\n",
      "The training loss is 8.532442890007179 with std:16.5773856729154. The val loss is 7.444312922218245 with std:9.755786445851154.\n",
      "7.444312922218245 3.1296480106707505\n",
      "The training loss is 7.804705285660649 with std:12.71624491914924. The val loss is 10.880742574875804 with std:25.371735155220694.\n",
      "10.880742574875804 3.1296480106707505\n",
      "Evaluating for {'lmda': 3.158635408267819} ...\n",
      "The training loss is 7.898448060278874 with std:15.135279195572153. The val loss is 19.901426239193803 with std:97.75158788557476.\n",
      "19.901426239193803 3.158635408267819\n",
      "The training loss is 7.031169627957093 with std:12.460223281017988. The val loss is 17.62400980162897 with std:92.50900222646978.\n",
      "17.62400980162897 3.158635408267819\n",
      "The training loss is 8.538369186887165 with std:16.594969450350376. The val loss is 7.4452936530384894 with std:9.762094545825107.\n",
      "7.4452936530384894 3.158635408267819\n",
      "The training loss is 7.809835698973033 with std:12.729112323032545. The val loss is 10.887002404074249 with std:25.395187473226347.\n",
      "10.887002404074249 3.158635408267819\n",
      "Evaluating for {'lmda': 3.1878912926776457} ...\n",
      "The training loss is 7.9041184311102 with std:15.150064913291168. The val loss is 20.00125825098899 with std:98.62770734837494.\n",
      "20.00125825098899 3.1878912926776457\n",
      "The training loss is 7.034145404410021 with std:12.47224582781113. The val loss is 17.619445170574675 with std:92.46725865532464.\n",
      "17.619445170574675 3.1878912926776457\n",
      "The training loss is 8.54430638308343 with std:16.61259526966504. The val loss is 7.44629302321511 with std:9.768453853514151.\n",
      "7.44629302321511 3.1878912926776457\n",
      "The training loss is 7.8149802262811106 with std:12.742016001873784. The val loss is 10.893313958218926 with std:25.41883506732158.\n",
      "10.893313958218926 3.1878912926776457\n",
      "Evaluating for {'lmda': 3.2174181506763717} ...\n",
      "The training loss is 7.9098028524138115 with std:15.164876403852706. The val loss is 20.101724481030278 with std:99.50918903029752.\n",
      "20.101724481030278 3.2174181506763717\n",
      "The training loss is 7.037143569210888 with std:12.484363347391756. The val loss is 17.614768435619034 with std:92.42441267913519.\n",
      "17.614768435619034 3.2174181506763717\n",
      "The training loss is 8.550254848718545 with std:16.630263574649685. The val loss is 7.44731153099491 with std:9.774865092605113.\n",
      "7.44731153099491 3.2174181506763717\n",
      "The training loss is 7.820139263587666 with std:12.75495662104168. The val loss is 10.899678202148861 with std:25.44267984593976.\n",
      "10.899678202148861 3.2174181506763717\n",
      "Evaluating for {'lmda': 3.2472184920731286} ...\n",
      "The training loss is 7.91550167487044 with std:15.179714023923285. The val loss is 20.202823936374667 with std:100.3960147439497.\n",
      "20.202823936374667 3.2472184920731286\n",
      "The training loss is 7.040164515861066 with std:12.4965768280727. The val loss is 17.60997963697737 with std:92.3804627740226.\n",
      "17.60997963697737 3.2472184920731286\n",
      "The training loss is 8.556214960017895 with std:16.64797481875632. The val loss is 7.448349680698701 with std:9.781328992297924.\n",
      "7.448349680698701 3.2472184920731286\n",
      "The training loss is 7.825313213205815 with std:12.767934854116394. The val loss is 10.906096110275069 with std:25.46672373364834.\n",
      "10.906096110275069 3.2472184920731286\n",
      "Evaluating for {'lmda': 3.277294849923382} ...\n",
      "The training loss is 7.921215255712521 with std:15.194578141605371. The val loss is 20.304555547552035 with std:101.28816555464287.\n",
      "20.304555547552035 3.277294849923382\n",
      "The training loss is 7.043208644399451 with std:12.508887270363799. The val loss is 17.605078820689233 with std:92.33540743364279.\n",
      "17.605078820689233 3.277294849923382\n",
      "The training loss is 8.562187099402019 with std:16.66572946517041. The val loss is 7.449407982772597 with std:9.787846287477368.\n",
      "7.449407982772597 3.277294849923382\n",
      "The training loss is 7.830502483857997 with std:12.780951382967302. The val loss is 10.912568666722763 with std:25.49096867121855.\n",
      "10.912568666722763 3.277294849923382\n",
      "Evaluating for {'lmda': 3.3076497807442427} ...\n",
      "The training loss is 7.926943958810186 with std:15.209469136487336. The val loss is 20.406918168288463 with std:102.18562177700568.\n",
      "20.406918168288463 3.3076497807442427\n",
      "The training loss is 7.046276361513641 with std:12.521295687091888. The val loss is 17.600066038754182 with std:92.28924516961577.\n",
      "17.600066038754182 3.3076497807442427\n",
      "The training loss is 8.56817165558113 with std:16.683527986884698. The val loss is 7.450486953841803 with std:9.794417718890013.\n",
      "7.450486953841803 3.3076497807442427\n",
      "The training loss is 7.835707490776402 with std:12.794006897831835. The val loss is 10.919096865477625 with std:25.515416615700367.\n",
      "10.919096865477625 3.3076497807442427\n",
      "Evaluating for {'lmda': 3.338285864731761} ...\n",
      "The training loss is 7.932688154759049 with std:15.22438739969333. The val loss is 20.5099105752662 with std:103.0883629719089.\n",
      "20.5099105752662 3.338285864731761\n",
      "The training loss is 7.04936808065306 with std:12.533803103521578. The val loss is 17.59494134927107 with std:92.24197451197546.\n",
      "17.59494134927107 3.338285864731761\n",
      "The training loss is 8.574169023651676 with std:16.701370866774315. The val loss is 7.451587116766329 with std:9.8010440333269.\n",
      "7.451587116766329 3.338285864731761\n",
      "The training loss is 7.840928655805414 with std:12.807102097396337. The val loss is 10.92568171053428 with std:25.54006954049658.\n",
      "10.92568171053428 3.338285864731761\n",
      "Evaluating for {'lmda': 3.369205705980267} ...\n",
      "The training loss is 7.938448220969743 with std:15.23933333393433. The val loss is 20.613531467901367 with std:103.99636794352303.\n",
      "20.613531467901367 3.369205705980267\n",
      "The training loss is 7.052484222143641 with std:12.546410557475971. The val loss is 17.58970481657898 with std:92.19359400961753.\n",
      "17.58970481657898 3.369205705980267\n",
      "The training loss is 8.580179605194909 with std:16.719258597673434. The val loss is 7.452709000699064 with std:9.807725983810409.\n",
      "7.452709000699064 3.369205705980267\n",
      "The training loss is 7.84616640750563 with std:12.820237688878084. The val loss is 10.932324216048121 with std:25.56492943544059.\n",
      "10.932324216048121 3.369205705980267\n",
      "Evaluating for {'lmda': 3.400411932703706} ...\n",
      "The training loss is 7.944224541758925 with std:15.254307353558836. The val loss is 20.71777946813081 with std:104.9096147364308.\n",
      "20.71777946813081 3.400411932703706\n",
      "The training loss is 7.055625213304542 with std:12.559119099458211. The val loss is 17.584356511401715 with std:92.14410223076646.\n",
      "17.584356511401715 3.400411932703706\n",
      "The training loss is 8.586203808377752 with std:16.737191682453783. The val loss is 7.453853141145759 with std:9.814464329786507.\n",
      "7.453853141145759 3.400411932703706\n",
      "The training loss is 7.8514211812594175 with std:12.833414388108737. The val loss is 10.939025406490378 with std:25.589998306875646.\n",
      "10.939025406490378 3.400411932703706\n",
      "Evaluating for {'lmda': 3.4319071974590427} ...\n",
      "The training loss is 7.950017508442384 with std:15.269309884605129. The val loss is 20.82265312024221 with std:105.82808063310992.\n",
      "20.82265312024221 3.4319071974590427\n",
      "The training loss is 7.058791488566479 with std:12.571929792772757. The val loss is 17.5788965109938 with std:92.09349776343966.\n",
      "17.5788965109938 3.4319071974590427\n",
      "The training loss is 8.592242048055633 with std:16.755170634104427. The val loss is 7.45502008002767 with std:9.82125983732181.\n",
      "7.45502008002767 3.4319071974590427\n",
      "The training loss is 7.856693419378762 with std:12.846632919619905. The val loss is 10.945786316805743 with std:25.615278177734975.\n",
      "10.945786316805743 3.4319071974590427\n",
      "Evaluating for {'lmda': 3.4636941773717345} ...\n",
      "The training loss is 7.955827519429747 with std:15.284341364853637. The val loss is 20.92815089070259 with std:106.75174215137032.\n",
      "20.92815089070259 3.4636941773717345\n",
      "The training loss is 7.061983489592169 with std:12.584843713647617. The val loss is 17.573324899290103 with std:92.04177921593022.\n",
      "17.573324899290103 3.4636941773717345\n",
      "The training loss is 8.59829474587758 with std:16.77319597581371. The val loss is 7.456210365746115 with std:9.828113279304967.\n",
      "7.456210365746115 3.4636941773717345\n",
      "The training loss is 7.86198357121434 with std:12.859894016729514. The val loss is 10.952607992573826 with std:25.640771087625982.\n",
      "10.952607992573826 3.4636941773717345\n",
      "Evaluating for {'lmda': 3.4957755743632752} ...\n",
      "The training loss is 7.961654980320993 with std:15.299402243879818. The val loss is 21.03427116803038 with std:107.68057504216426.\n",
      "21.03427116803038 3.4957755743632752\n",
      "The training loss is 7.065201665398406 with std:12.597861951356437. The val loss is 17.567641767058113 with std:91.98894521730001.\n",
      "17.567641767058113 3.4957755743632752\n",
      "The training loss is 8.604362330393684 with std:16.791268241052727. The val loss is 7.457424553249776 with std:9.835025435653256.\n",
      "7.457424553249776 3.4957755743632752\n",
      "The training loss is 7.867292093266862 with std:12.873198421630486. The val loss is 10.959491490172766 with std:25.666479092913356.\n",
      "10.959491490172766 3.4957755743632752\n",
      "Evaluating for {'lmda': 3.528154115380883} ...\n",
      "The training loss is 7.967500304004955 with std:15.314492983108316. The val loss is 21.141012262677155 with std:108.61455428745587.\n",
      "21.141012262677155 3.528154115380883\n",
      "The training loss is 7.068446472480209 with std:12.610985608340894. The val loss is 17.561847212051717 with std:91.93499441786729.\n",
      "17.561847212051717 3.528154115380883\n",
      "The training loss is 8.610445237164553 with std:16.809387973660552. The val loss is 7.458663204104116 with std:9.84199709352334.\n",
      "7.458663204104116 3.528154115380883\n",
      "The training loss is 7.872619449299954 with std:12.88654688548061. The val loss is 10.9664378769472 with std:25.69240426680828.\n",
      "10.9664378769472 3.528154115380883\n",
      "Evaluating for {'lmda': 3.560832552629278} ...\n",
      "The training loss is 7.973363910759588 with std:15.329614055867333. The val loss is 21.248372406940938 with std:109.55365409835166.\n",
      "21.248372406940938 3.560832552629278\n",
      "The training loss is 7.0717183749368795 with std:12.624215800333651. The val loss is 17.555941339168953 with std:91.87992548971766.\n",
      "17.555941339168953 3.560832552629278\n",
      "The training loss is 8.616543908873147 with std:16.827555727930946. The val loss is 7.459926886563255 with std:9.84902904752632.\n",
      "7.459926886563255 3.560832552629278\n",
      "The training loss is 7.877966110455118 with std:12.899940168493947. The val loss is 10.97344823137795 with std:25.718548699454427.\n",
      "10.97344823137795 3.560832552629278\n",
      "Evaluating for {'lmda': 3.593813663804626} ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 7.979246228354405 with std:15.344765947444548. The val loss is 21.35634975489291 with std:110.4978479133226.\n",
      "21.35634975489291 3.593813663804626\n",
      "The training loss is 7.07501784459999 with std:12.637553656481316. The val loss is 17.549924260612315 with std:91.82373712722016.\n",
      "17.549924260612315 3.593813663804626\n",
      "The training loss is 8.622658795438927 with std:16.84577206870129. The val loss is 7.461216175644742 with std:9.856122099947836.\n",
      "7.461216175644742 3.593813663804626\n",
      "The training loss is 7.883332555368422 with std:12.913379040034311. The val loss is 10.980523643255733 with std:25.744914498019796.\n",
      "10.980523643255733 3.593813663804626\n",
      "Evaluating for {'lmda': 3.6271002523306484} ...\n",
      "The training loss is 7.985147692154566 with std:15.359949155142832. The val loss is 21.46494238234519 with std:111.44710839677855.\n",
      "21.46494238234519 3.6271002523306484\n",
      "The training loss is 7.078345361163392 with std:12.651000319467686. The val loss is 17.543796096050556 with std:91.76642804754228.\n",
      "17.543796096050556 3.6271002523306484\n",
      "The training loss is 8.628790354134313 with std:16.86403757144319. The val loss is 7.462531653206318 with std:9.863277060971589.\n",
      "7.462531653206318 3.6271002523306484\n",
      "The training loss is 7.888719270289288 with std:12.926864278710033. The val loss is 10.987665213857904 with std:25.77150378678885.\n",
      "10.987665213857904 3.6271002523306484\n",
      "Evaluating for {'lmda': 3.6606951475969023} ...\n",
      "The training loss is 7.991068745227153 with std:15.375164188338042. The val loss is 21.574148286817692 with std:112.40140743760035.\n",
      "21.574148286817692 3.6606951475969023\n",
      "The training loss is 7.081701412315178 with std:12.664556945636967. The val loss is 17.537556972784895 with std:91.7079969911857.\n",
      "17.537556972784895 3.6606951475969023\n",
      "The training loss is 8.634939049703464 with std:16.88235282235468. The val loss is 7.463873908025696 with std:9.870494748907472.\n",
      "7.463873908025696 3.6606951475969023\n",
      "The training loss is 7.894126749200979 with std:12.940396672470364. The val loss is 10.994874056127939 with std:25.798318707256044.\n",
      "10.994874056127939 3.6606951475969023\n",
      "Evaluating for {'lmda': 3.6946012051993025} ...\n",
      "The training loss is 7.997009838449581 with std:15.390411568536813. The val loss is 21.68396538755815 with std:113.36071614813228.\n",
      "21.68396538755815 3.6946012051993025\n",
      "The training loss is 7.085086493871808 with std:12.678224705117643. The val loss is 17.531207025917112 with std:91.64844272252591.\n",
      "17.531207025917112 3.6946012051993025\n",
      "The training loss is 8.641105354483638 with std:16.900718418455043. The val loss is 7.465243535882861 with std:9.87777599042357.\n",
      "7.465243535882861 3.6946012051993025\n",
      "The training loss is 7.899555493943367 with std:12.95397701870402. The val loss is 11.002151294858361 with std:25.825361418221707.\n",
      "11.002151294858361 3.6946012051993025\n",
      "Evaluating for {'lmda': 3.7288213071828338} ...\n",
      "The training loss is 8.002971430619883 with std:15.405691829436353. The val loss is 21.794391525567256 with std:114.32500486319263.\n",
      "21.794391525567256 3.7288213071828338\n",
      "The training loss is 7.088501109914166 with std:12.6920047819462. The val loss is 17.52474639851943 with std:91.58776403034742.\n",
      "17.52474639851943 3.7288213071828338\n",
      "The training loss is 8.647289748528511 with std:16.91913496768024. The val loss is 7.466641139645021 with std:9.885121620782096.\n",
      "7.466641139645021 3.7288213071828338\n",
      "The training loss is 7.9050060143374585 with std:12.96760612433884. The val loss is 11.009498066877121 with std:25.852634095890966.\n",
      "11.009498066877121 3.7288213071828338\n",
      "Evaluating for {'lmda': 3.7633583622865325} ...\n",
      "The training loss is 8.008953988569242 with std:15.421005516984094. The val loss is 21.90542446365389 with std:115.29424313932822.\n",
      "21.90542446365389 3.7633583622865325\n",
      "The training loss is 7.091945772925865 with std:12.705898374191. The val loss is 17.51817524181013 with std:91.5259597284173.\n",
      "17.51817524181013 3.7633583622865325\n",
      "The training loss is 8.653492719734512 with std:16.93760308898162. The val loss is 7.468067329354403 with std:9.89253248407913.\n",
      "7.468067329354403 3.7633583622865325\n",
      "The training loss is 7.910478828312032 with std:12.981284805943654. The val loss is 11.016915521236058 with std:25.880138933970557.\n",
      "11.016915521236058 3.7633583622865325\n",
      "Evaluating for {'lmda': 3.798215306190736} ...\n",
      "The training loss is 8.014957987276706 with std:15.436353189439743. The val loss is 22.01706188652154 with std:116.26839975432601.\n",
      "22.01706188652154 3.798215306190736\n",
      "The training loss is 7.09542100393349 with std:12.719906694076204. The val loss is 17.51149371532829 with std:91.46302865602799.\n",
      "17.51149371532829 3.798215306190736\n",
      "The training loss is 8.659714763969156 with std:16.95612341242546. The val loss is 7.469522722318749 with std:9.900009433487826.\n",
      "7.469522722318749 3.798215306190736\n",
      "The training loss is 7.915974462032393 with std:12.995013889831652. The val loss is 11.024404819404014 with std:25.907878143772393.\n",
      "11.024404819404014 3.798215306190736\n",
      "Evaluating for {'lmda': 3.8333951017666013} ...\n",
      "The training loss is 8.0209839099859 with std:15.451735417437169. The val loss is 22.12930140087189 with std:117.2474427068536.\n",
      "22.12930140087189 3.8333951017666013\n",
      "The training loss is 7.0989273326492155 with std:12.734030968106161. The val loss is 17.504701987113886 with std:91.39896967857771.\n",
      "17.504701987113886 3.8333951017666013\n",
      "The training loss is 8.665956385202163 with std:16.974696579295202. The val loss is 7.471007943204781 with std:9.907553331505499.\n",
      "7.471007943204781 3.8333951017666013\n",
      "The training loss is 7.921493450031032 with std:13.008794212165382. The val loss is 11.031967135461809 with std:25.935853954314933.\n",
      "11.031967135461809 3.8333951017666013\n",
      "Evaluating for {'lmda': 3.8689007393279757} ...\n",
      "The training loss is 8.0270322483243 with std:15.467152784048954. The val loss is 22.242140535541363 with std:118.23133921636887.\n",
      "22.242140535541363 3.8689007393279757\n",
      "The training loss is 7.102465297615422 with std:12.748272437189351. The val loss is 17.49780023388978 with std:91.33378168814339.\n",
      "17.49780023388978 3.8689007393279757\n",
      "The training loss is 8.672218095639078 with std:16.993323242195597. The val loss is 7.472523624134425 with std:9.915165050203827.\n",
      "7.472523624134425 3.8689007393279757\n",
      "The training loss is 7.927036335340643 with std:13.022626619063937. The val loss is 11.039603656301214 with std:25.96406861242816.\n",
      "11.039603656301214 3.8689007393279757\n",
      "Evaluating for {'lmda': 3.90473523688556} ...\n",
      "The training loss is 8.033103502424456 with std:15.482605884850846. The val loss is 22.355576741661604 with std:119.22005572321986.\n",
      "22.355576741661604 3.90473523688556\n",
      "The training loss is 7.1060354463516555 with std:12.762632356762929. The val loss is 17.490788641247004 with std:91.267463604069.\n",
      "17.490788641247004 3.90473523688556\n",
      "The training loss is 8.678500415857282 with std:17.012004065158393. The val loss is 7.474070404784121 with std:9.92284547148255.\n",
      "7.474070404784121 3.90473523688556\n",
      "The training loss is 7.932603669629022 with std:13.036511966711599. The val loss is 11.047315581826814 with std:25.9925243828599.\n",
      "11.047315581826814 3.90473523688556\n",
      "Evaluating for {'lmda': 3.940901640403448} ...\n",
      "The training loss is 8.039198181047698 with std:15.498095327988317. The val loss is 22.469607392845816 with std:120.21355788895221.\n",
      "22.469607392845816 3.940901640403448\n",
      "The training loss is 7.109638335503705 with std:12.777111996916625. The val loss is 17.483667403831618 with std:91.20001437354789.\n",
      "17.483667403831618 3.940901640403448\n",
      "The training loss is 8.684803874944775 with std:17.030739723750035. The val loss is 7.475648932487074 with std:9.93059548732605.\n",
      "7.475648932487074 3.940901640403448\n",
      "The training loss is 7.938196013336391 with std:13.050451121468285. The val loss is 11.055104125160891 with std:26.02122354838261.\n",
      "11.055104125160891 3.940901640403448\n",
      "Evaluating for {'lmda': 3.9774030240580367} ...\n",
      "The training loss is 8.045316801710163 with std:15.513621734244088. The val loss is 22.584229785403487 with std:121.21181059684297.\n",
      "22.584229785403487 3.9774030240580367\n",
      "The training loss is 7.113274530995132 with std:12.79171264251742. The val loss is 17.476436725536704 with std:91.13143297223392.\n",
      "17.476436725536704 3.9774030240580367\n",
      "The training loss is 8.691129010641605 with std:17.04953090518255. The val loss is 7.477259862338405 with std:9.93841600006326.\n",
      "7.477259862338405 3.9774030240580367\n",
      "The training loss is 7.943813935814669 with std:13.064444959982088. The val loss is 11.062970512852187 with std:26.05016840990491.\n",
      "11.062970512852187 3.9774030240580367\n",
      "Evaluating for {'lmda': 4.014242490499322} ...\n",
      "The training loss is 8.051459890811243 with std:15.529185737107635. The val loss is 22.699441138580546 with std:122.21477795263905.\n",
      "22.699441138580546 4.014242490499322\n",
      "The training loss is 7.1169446081809 with std:12.806435593333651. The val loss is 17.46909681969476 with std:91.0617184048355.\n",
      "17.46909681969476 4.014242490499322\n",
      "The training loss is 8.697476369483713 with std:17.06837830842479. The val loss is 7.478903857303636 with std:9.946307922630039.\n",
      "7.478903857303636 4.014242490499322\n",
      "The training loss is 7.949458015469192 with std:13.078494369303383. The val loss is 11.070915985086728 with std:26.07936128657996.\n",
      "11.070915985086728 4.014242490499322\n",
      "Evaluating for {'lmda': 4.051423171114648} ...\n",
      "The training loss is 8.057627983764341 with std:15.544787982845035. The val loss is 22.815238594831413 with std:123.22242328555511.\n",
      "22.815238594831413 4.051423171114648\n",
      "The training loss is 7.120649152003537 with std:12.821282164159319. The val loss is 17.461647909275126 with std:90.99086970573613.\n",
      "17.461647909275126 4.051423171114648\n",
      "The training loss is 8.703846506949809 with std:17.087282644317096. The val loss is 7.4805815883302795 with std:9.954272178834984.\n",
      "7.4805815883302795 4.051423171114648\n",
      "The training loss is 7.9551288399025335 with std:13.092600247000878. The val loss is 11.078941795903319 with std:26.108804515920667.\n",
      "11.078941795903319 4.051423171114648\n",
      "Evaluating for {'lmda': 4.08894822629486} ...\n",
      "The training loss is 8.06382162513022 with std:15.560429130571507. The val loss is 22.93161922010772 with std:124.23470914939298.\n",
      "22.93161922010772 4.08894822629486\n",
      "The training loss is 7.124388757151453 with std:12.83625368493804. The val loss is 17.454090227082247 with std:90.91888593960437.\n",
      "17.454090227082247 4.08894822629486\n",
      "The training loss is 8.710239987610693 with std:17.10624463568719. The val loss is 7.482293734462489 with std:9.962309703627149.\n",
      "7.482293734462489 4.08894822629486\n",
      "The training loss is 7.960827006060716 with std:13.106763501279756. The val loss is 11.087049213410891 with std:26.138500453911927.\n",
      "11.087049213410891 4.08894822629486\n",
      "Evaluating for {'lmda': 4.126820845702952} ...\n",
      "The training loss is 8.0700413687529 with std:15.576109852324926. The val loss is 23.048580004193102 with std:125.25159732405216.\n",
      "23.048580004193102 4.126820845702952\n",
      "The training loss is 7.128164028219889 with std:12.85135150088753. The val loss is 17.44642401595906 with std:90.84576620202712.\n",
      "17.44642401595906 4.126820845702952\n",
      "The training loss is 8.716657385281456 with std:17.125265017468937. The val loss is 7.48404098295892 with std:9.970421443366165.\n",
      "7.48404098295892 4.126820845702952\n",
      "The training loss is 7.966553120381666 with std:13.12098505110133. The val loss is 11.095239520009928 with std:26.16845147512724.\n",
      "11.095239520009928 4.126820845702952\n",
      "Evaluating for {'lmda': 4.165044248545185} ...\n",
      "The training loss is 8.076287777897903 with std:15.591830833140683. The val loss is 23.166117861045112 with std:126.27304881707092.\n",
      "23.166117861045112 4.165044248545185\n",
      "The training loss is 7.131975579874104 with std:12.866576972623228. The val loss is 17.438649528991665 with std:90.77150962013725.\n",
      "17.438649528991665 4.165044248545185\n",
      "The training loss is 8.723099283176504 with std:17.144344536822892. The val loss is 7.4858240294141485 with std:9.978608356095615.\n",
      "7.4858240294141485 4.165044248545185\n",
      "The training loss is 7.972307798946129 with std:13.135265826304757. The val loss is 11.103514012617119 with std:26.198659972846144.\n",
      "11.103514012617119 4.165044248545185\n",
      "Evaluating for {'lmda': 4.203621683844713} ...\n",
      "The training loss is 8.08256142539327 with std:15.607592771128468. The val loss is 23.284229629181485 with std:127.29902386554518.\n",
      "23.284229629181485 4.203621683844713\n",
      "The training loss is 7.135824037015111 with std:12.88193147628218. The val loss is 17.430767029717423 with std:90.69611535324943.\n",
      "17.430767029717423 4.203621683844713\n",
      "The training loss is 8.729566274067222 with std:17.163483953258364. The val loss is 7.487643577883197 with std:9.986871411817361.\n",
      "7.487643577883197 4.203621683844713\n",
      "The training loss is 7.978091667630788 with std:13.14960676773092. The val loss is 11.111874002892593 with std:26.229128359172236.\n",
      "11.111874002892593 4.203621683844713\n",
      "Evaluating for {'lmda': 4.242556430717777} ...\n",
      "The training loss is 8.088862893773301 with std:15.623396377551307. The val loss is 23.402912072087815 with std:128.3294819382078.\n",
      "23.402912072087815 4.242556430717777\n",
      "The training loss is 7.139710034947931 with std:12.897416403646874. The val loss is 17.422776792336514 with std:90.61958259350611.\n",
      "17.422776792336514 4.242556430717777\n",
      "The training loss is 8.73605896044277 with std:17.182684038759113. The val loss is 7.489500341009554 with std:9.995211592768873.\n",
      "7.489500341009554 4.242556430717777\n",
      "The training loss is 7.98390536226406 with std:13.164008827347924. The val loss is 11.120320817471264 with std:26.25985906515386.\n",
      "11.120320817471264 4.242556430717777\n",
      "Evaluating for {'lmda': 4.281851798652411} ...\n",
      "The training loss is 8.095192775424557 with std:15.639242376904697. The val loss is 23.522161878651552 with std:129.36438173771674.\n",
      "23.522161878651552 4.281851798652411\n",
      "The training loss is 7.143634219552296 with std:12.91303316226842. The val loss is 17.41467910192571 with std:90.54191056652529.\n",
      "17.41467910192571 4.281851798652411\n",
      "The training loss is 8.74257795467348 with std:17.20194557790974. The val loss is 7.4913950401565685 with std:10.003629893701763.\n",
      "7.4913950401565685 4.281851798652411\n",
      "The training loss is 7.989749528784055 with std:13.178472968378635. The val loss is 11.128855798196934 with std:26.290854540905727.\n",
      "11.128855798196934 4.281851798652411\n",
      "Evaluating for {'lmda': 4.321511127789762} ...\n",
      "The training loss is 8.101551672735154 with std:15.65513150699951. The val loss is 23.641975663630117 with std:130.40368120322327.\n",
      "23.641975663630117 4.321511127789762\n",
      "The training loss is 7.14759724745603 with std:12.928783175590242. The val loss is 17.406474254655866 with std:90.46309853205624.\n",
      "17.406474254655866 4.321511127789762\n",
      "The training loss is 8.749123879177342 with std:17.2212693680253. The val loss is 7.493328405542457 with std:10.012127322162451.\n",
      "7.493328405542457 4.321511127789762\n",
      "The training loss is 7.995624823399303 with std:13.193000165430554. The val loss is 11.137480302359803 with std:26.322117255731808.\n",
      "11.137480302359803 4.321511127789762\n",
      "Evaluating for {'lmda': 4.361537789208006} ...\n",
      "The training loss is 8.107940198246384 with std:15.671064519045348. The val loss is 23.762349968145802 with std:131.44733751315243.\n",
      "23.762349968145802 4.361537789208006\n",
      "The training loss is 7.151599786210873 with std:12.944667883070444. The val loss is 17.398162558011876 with std:90.38314578463877.\n",
      "17.398162558011876 4.361537789208006\n",
      "The training loss is 8.755697366589473 with std:17.24065621928261. The val loss is 7.495301176378675 with std:10.020704898774136.\n",
      "7.495301176378675 4.361537789208006\n",
      "The training loss is 8.001531912751748 with std:13.207591404626791. The val loss is 11.14619570293725 with std:26.353649698249896.\n",
      "11.14619570293725 4.361537789208006\n",
      "Evaluating for {'lmda': 4.401935185208875} ...\n",
      "The training loss is 8.114358974807311 with std:15.687042177736041. The val loss is 23.883281260208562 with std:132.4953070882029.\n",
      "23.883281260208562 4.401935185208875\n",
      "The training loss is 7.1556425144710945 with std:12.960688740305052. The val loss is 17.389744331015354 with std:90.30205165426241.\n",
      "17.389744331015354 4.401935185208875\n",
      "The training loss is 8.762299059934355 with std:17.260106954853978. The val loss is 7.497314101012022 with std:10.029363657520243.\n",
      "7.497314101012022 4.401935185208875\n",
      "The training loss is 8.00747147408246 with std:13.222247683740322. The val loss is 11.155003388838098 with std:26.385454376517604.\n",
      "11.155003388838098 4.401935185208875\n",
      "Evaluating for {'lmda': 4.442706749606883} ...\n",
      "The training loss is 8.120808635732258 with std:15.703065261337708. The val loss is 24.004765935267823 with std:133.547545594581.\n",
      "24.004765935267823 4.442706749606883\n",
      "The training loss is 7.1597261221747 with std:12.976847219150356. The val loss is 17.381219904452188 with std:90.21981550704939.\n",
      "17.381219904452188 4.442706749606883\n",
      "The training loss is 8.768929612801418 with std:17.27962241104322. The val loss is 7.499367937070357 with std:10.038104646029351.\n",
      "7.499367937070357 4.442706749606883\n",
      "The training loss is 8.013444195399902 with std:13.236970012329026. The val loss is 11.163904765149615 with std:26.417533818158436.\n",
      "11.163904765149615 4.442706749606883\n",
      "Evaluating for {'lmda': 4.483855948021186} ...\n",
      "The training loss is 8.127289824961034 with std:15.719134561777391. The val loss is 24.126800316795915 with std:134.60400794748918.\n",
      "24.126800316795915 4.483855948021186\n",
      "The training loss is 7.163851310727281 with std:12.993144807844644. The val loss is 17.37258962110065 with std:90.13643674591926.\n",
      "17.37258962110065 4.483855948021186\n",
      "The training loss is 8.775589689523347 with std:17.29920343742368. The val loss is 7.501463451611944 with std:10.046928925860822.\n",
      "7.501463451611944 4.483855948021186\n",
      "The training loss is 8.019450775650753 with std:13.251759411873518. The val loss is 11.172901253388744 with std:26.44989057049186.\n",
      "11.172901253388744 4.483855948021186\n",
      "Evaluating for {'lmda': 4.525386278170167} ...\n",
      "The training loss is 8.133803197222335 with std:15.735250884735654. The val loss is 24.24938065689379 with std:135.66464831478157.\n",
      "24.24938065689379 4.525386278170167\n",
      "The training loss is 7.168018793188747 with std:13.009583011130376. The val loss is 17.363853835964054 with std:90.05191481126963.\n",
      "17.363853835964054 4.525386278170167\n",
      "The training loss is 8.782279965357738 with std:17.31885089697888. The val loss is 7.503601421278652 with std:10.055837572791745.\n",
      "7.503601421278652 4.525386278170167\n",
      "The training loss is 8.025491924893439 with std:13.266616915916929. The val loss is 11.181994291755911 with std:26.482527200662165.\n",
      "11.181994291755911 4.525386278170167\n",
      "Evaluating for {'lmda': 4.5673012701687465} ...\n",
      "The training loss is 8.140349418199897 with std:15.751415049739231. The val loss is 24.37250313693662 with std:136.7294201209543.\n",
      "24.37250313693662 4.5673012701687465\n",
      "The training loss is 7.172229294462721 with std:13.02616335037532. The val loss is 17.35501291650684 with std:89.96624918166626.\n",
      "17.35501291650684 4.5673012701687465\n",
      "The training loss is 8.7890011266718 with std:17.338565666245426. The val loss is 7.505782632452868 with std:10.064831677104594.\n",
      "7.505782632452868 4.5673012701687465\n",
      "The training loss is 8.031568364474463 with std:13.281543570206615. The val loss is 11.191185335392268 with std:26.515446295768943.\n",
      "11.191185335392268 4.5673012701687465\n",
      "Evaluating for {'lmda': 4.609604486828434} ...\n",
      "The training loss is 8.146929164701808 with std:15.767627890257199. The val loss is 24.49616386824321 with std:137.79827605131453.\n",
      "24.49616386824321 4.609604486828434\n",
      "The training loss is 7.1764835514887215 with std:13.04288736369342. The val loss is 17.34606724289186 with std:89.87943937451946.\n",
      "17.34606724289186 4.609604486828434\n",
      "The training loss is 8.795753871130106 with std:17.358348635457695. The val loss is 7.508007881418097 with std:10.073912343875161.\n",
      "7.508007881418097 4.609604486828434\n",
      "The training loss is 8.037680827207216 with std:13.296540432837842. The val loss is 11.200475856640809 with std:26.54865046300058.\n",
      "11.200475856640809 4.609604486828434\n",
      "Evaluating for {'lmda': 4.652299523960189} ...\n",
      "The training loss is 8.153543124832884 with std:15.78389025379859. The val loss is 24.620358892767058 with std:138.87116805631325.\n",
      "24.620358892767058 4.652299523960189\n",
      "The training loss is 7.1807823134373505 with std:13.059756606065468. The val loss is 17.33701720822385 with std:89.79148494679255.\n",
      "17.33701720822385 4.652299523960189\n",
      "The training loss is 8.80253890788575 with std:17.37820070869572. The val loss is 7.510277974523861 with std:10.08308069326158.\n",
      "7.510277974523861 4.652299523960189\n",
      "The training loss is 8.043830057553816 with std:13.311608574399896. The val loss is 11.209867345310192 with std:26.582142329766615.\n",
      "11.209867345310192 4.652299523960189\n",
      "Evaluating for {'lmda': 4.695390010680058} ...\n",
      "The training loss is 8.16019199817013 with std:15.800203002012273. The val loss is 24.745084183833683 with std:139.94804735626604.\n",
      "24.745084183833683 4.695390010680058\n",
      "The training loss is 7.185126341908168 with std:13.07677264945883. The val loss is 17.32786321879213 with std:89.70238549567839.\n",
      "17.32786321879213 4.695390010680058\n",
      "The training loss is 8.809356957774545 with std:17.39812280403425. The val loss is 7.512593728354067 with std:10.092337860793169.\n",
      "7.512593728354067 4.695390010680058\n",
      "The training loss is 8.050016811809567 with std:13.326749078124175. The val loss is 11.219361308942085 with std:26.615924543832303.\n",
      "11.219361308942085 4.695390010680058\n",
      "Evaluating for {'lmda': 4.738879609717651} ...\n",
      "The training loss is 8.166876495941468 with std:15.816567010789536. The val loss is 24.870335646892727 with std:141.02886444617255.\n",
      "24.870335646892727 4.738879609717651\n",
      "The training loss is 7.189516411130669 with std:13.093937082947324. The val loss is 17.318605694320706 with std:89.6121406593177.\n",
      "17.318605694320706 4.738879609717651\n",
      "The training loss is 8.816208753512683 with std:17.418115853695703. The val loss is 7.514955969899693 with std:10.101684997659683.\n",
      "7.514955969899693 4.738879609717651\n",
      "The training loss is 8.056241858290411 with std:13.341963040034344. The val loss is 11.228959273082484 with std:26.649999773455306.\n",
      "11.228959273082484 4.738879609717651\n",
      "Evaluating for {'lmda': 4.782772017727486} ...\n",
      "The training loss is 8.17359734120747 with std:15.832983170368172. The val loss is 24.996109120308255 with std:142.1135691008494.\n",
      "24.996109120308255 4.782772017727486\n",
      "The training loss is 7.193953308168087 with std:13.111251512829798. The val loss is 17.309245068217734 with std:89.52075011748676.\n",
      "17.309245068217734 4.782772017727486\n",
      "The training loss is 8.82309503989751 with std:17.438180804203757. The val loss is 7.517365536735346 with std:10.111123271000572.\n",
      "7.517365536735346 4.782772017727486\n",
      "The training loss is 8.062505977523136 with std:13.357251569098644. The val loss is 11.238662781555572 with std:26.684370707521882.\n",
      "11.238662781555572 4.782772017727486\n",
      "Evaluating for {'lmda': 4.827070965603183} ...\n",
      "The training loss is 8.180355269046585 with std:15.84945238543944. The val loss is 25.12240037616918 with std:143.20211038020818.\n",
      "25.12240037616918 4.827070965603183\n",
      "The training loss is 7.198437833124288 with std:13.12871756274883. The val loss is 17.299781787831044 with std:89.42821359231473.\n",
      "17.299781787831044 4.827070965603183\n",
      "The training loss is 8.830016574011761 with std:17.458318616540456. The val loss is 7.519823277200006 with std:10.120653864193994.\n",
      "7.519823277200006 4.827070965603183\n",
      "The training loss is 8.0688099624386 with std:13.372615787384442. The val loss is 11.248473396741462 with std:26.719040055684673.\n",
      "11.248473396741462 4.827070965603183\n",
      "Evaluating for {'lmda': 4.871780218794631} ...\n",
      "The training loss is 8.187151026743525 with std:15.865975575257254. The val loss is 25.24920512114061 with std:144.29443663487416.\n",
      "25.24920512114061 4.871780218794631\n",
      "The training loss is 7.202970799353647 with std:13.146336873808464. The val loss is 17.290216314704654 with std:89.33453084898198.\n",
      "17.290216314704654 4.871780218794631\n",
      "The training loss is 8.836974125431198 with std:17.478530266305274. The val loss is 7.522330050581727 with std:10.130277977145415.\n",
      "7.522330050581727 4.871780218794631\n",
      "The training loss is 8.075154618567815 with std:13.388056830214905. The val loss is 11.258392699857197 with std:26.754010548501743.\n",
      "11.258392699857197 4.871780218794631\n",
      "Evaluating for {'lmda': 4.916903577628026} ...\n",
      "The training loss is 8.193985373981103 with std:15.882553673749767. The val loss is 25.376518997338792 with std:145.39049551198042.\n",
      "25.376518997338792 4.916903577628026\n",
      "The training loss is 7.20755303367406 with std:13.164111104691246. The val loss is 17.280549124840338 with std:89.23970169644116.\n",
      "17.280549124840338 4.916903577628026\n",
      "The training loss is 8.843968476435647 with std:17.498816743876848. The val loss is 7.524886727306787 with std:10.139996826576148.\n",
      "7.524886727306787 4.916903577628026\n",
      "The training loss is 8.081540764241199 with std:13.403575846327952. The val loss is 11.268422291241379 with std:26.78928493757688.\n",
      "11.268422291241379 4.916903577628026\n",
      "Evaluating for {'lmda': 4.962444877628913} ...\n",
      "The training loss is 8.200859083035391 with std:15.899187629633309. The val loss is 25.504337583229972 with std:146.49023396114316.\n",
      "25.504337583229972 4.962444877628913\n",
      "The training loss is 7.212185376583018 with std:13.182041931774924. The val loss is 17.270780708961105 with std:89.14372598812429.\n",
      "17.270780708961105 4.962444877628913\n",
      "The training loss is 8.851000422223414 with std:17.519179054576377. The val loss is 7.527494189132833 with std:10.149811646310928.\n",
      "7.527494189132833 4.962444877628913\n",
      "The training loss is 8.087969230790687 with std:13.419173998037286. The val loss is 11.278563790642092 with std:26.824865995700716.\n",
      "11.278563790642092 4.962444877628913\n",
      "Evaluating for {'lmda': 5.008407989848212} ...\n",
      "The training loss is 8.20777293897438 with std:15.915878406528913. The val loss is 25.632656394570475 with std:147.59359824077686.\n",
      "25.632656394570475 5.008407989848212\n",
      "The training loss is 7.216868682476924 with std:13.20013104924826. The val loss is 17.260911572778625 with std:89.04660362266253.\n",
      "17.260911572778625 5.008407989848212\n",
      "The training loss is 8.858070771129352 with std:17.539618218834367. The val loss is 7.530153329346412 with std:10.159723687564664.\n",
      "7.530153329346412 5.008407989848212\n",
      "The training loss is 8.094440862754983 with std:13.434852461395668. The val loss is 11.288818837508064 with std:26.86075651699183.\n",
      "11.288818837508064 5.008407989848212\n",
      "Evaluating for {'lmda': 5.054796821191241} ...\n",
      "The training loss is 8.214727739860134 with std:15.932626983081487. The val loss is 25.76147088536572 with std:148.70053392454352.\n",
      "25.76147088536572 5.054796821191241\n",
      "The training loss is 7.22160381987352 with std:13.218380169226084. The val loss is 17.250942237263583 with std:88.94833454460301.\n",
      "17.250942237263583 5.054796821191241\n",
      "The training loss is 8.865180344846301 with std:17.560135272358856. The val loss is 7.532865052964862 with std:10.169734219228626.\n",
      "7.532865052964862 5.054796821191241\n",
      "The training loss is 8.100956518088111 with std:13.450612426360568. The val loss is 11.29918909128378 with std:26.896959317040704.\n",
      "11.29918909128378 5.054796821191241\n",
      "Evaluating for {'lmda': 5.101615314749834} ...\n",
      "The training loss is 8.221724296954422 with std:15.949434353081067. The val loss is 25.890776448860713 with std:149.8109859080508.\n",
      "25.890776448860713 5.101615314749834\n",
      "The training loss is 7.226391671637624 with std:13.23679102186372. The val loss is 17.240873238919292 with std:88.84891874512684.\n",
      "17.240873238919292 5.101615314749834\n",
      "The training loss is 8.872329978650276 with std:17.58073126630687. The val loss is 7.535630276942481 with std:10.17984452815512.\n",
      "7.535630276942481 5.101615314749834\n",
      "The training loss is 8.107517068370734 with std:13.466455096961694. The val loss is 11.309676231707693 with std:26.93347723305275.\n",
      "11.309676231707693 5.101615314749834\n",
      "Evaluating for {'lmda': 5.148867450137493} ...\n",
      "The training loss is 8.228763434928139 with std:15.966301525587445. The val loss is 26.020568418565073 with std:150.92489841582574.\n",
      "26.020568418565073 5.148867450137493\n",
      "The training loss is 7.231233135210165 with std:13.255365355470625. The val loss is 17.23070513005924 with std:88.74835626277611.\n",
      "17.23070513005924 5.148867450137493\n",
      "The training loss is 8.879520521629187 with std:17.601407267457713. The val loss is 7.5384499303812245 with std:10.190055919441313.\n",
      "7.5384499303812245 5.148867450137493\n",
      "The training loss is 8.11412339902516 with std:13.482381691471394. The val loss is 11.320281959114068 with std:26.970313123993517.\n",
      "11.320281959114068 5.148867450137493\n",
      "Evaluating for {'lmda': 5.196557243827657} ...\n",
      "The training loss is 8.235845992074129 with std:15.983229525056702. The val loss is 26.15084206929699 with std:152.04221500842223.\n",
      "26.15084206929699 5.196557243827657\n",
      "The training loss is 7.236129122840455 with std:13.27410493662291. The val loss is 17.22043847908679 with std:88.64664718416975.\n",
      "17.22043847908679 5.196557243827657\n",
      "The training loss is 8.886752836915113 with std:17.622164358388805. The val loss is 7.541324954745736 with std:10.20036971671124.\n",
      "7.541324954745736 5.196557243827657\n",
      "The training loss is 8.12077640953318 with std:13.498393442576646. The val loss is 11.331007994738147 with std:27.007469870733274.\n",
      "11.331007994738147 5.196557243827657\n",
      "Evaluating for {'lmda': 5.244688749495119} ...\n",
      "The training loss is 8.242972820523867 with std:16.000219391471056. The val loss is 26.281592618265815 with std:153.16287858984333.\n",
      "26.281592618265815 5.244688749495119\n",
      "The training loss is 7.241080561821996 with std:13.293011550275711. The val loss is 17.210073870779294 with std:88.54379164473102.\n",
      "17.210073870779294 5.244688749495119\n",
      "The training loss is 8.894027801920563 with std:17.64300363765408. The val loss is 7.544256304083188 with std:10.210787262396844.\n",
      "7.544256304083188 5.244688749495119\n",
      "The training loss is 8.12747701365753 with std:13.514491597553866. The val loss is 11.341856081025462 with std:27.044950376194876.\n",
      "11.341856081025462 5.244688749495119\n",
      "Evaluating for {'lmda': 5.293266058360562} ...\n",
      "The training loss is 8.250144786467876 with std:16.017272180470915. The val loss is 26.412815226179166 with std:154.2868314151446.\n",
      "26.412815226179166 5.293266058360562\n",
      "The training loss is 7.246088394731563 with std:13.31208699987392. The val loss is 17.199611906575026 with std:88.43978982941299.\n",
      "17.199611906575026 5.293266058360562\n",
      "The training loss is 8.901346308578114 with std:17.663926219964566. The val loss is 7.547244945247319 with std:10.22130991801672.\n",
      "7.547244945247319 5.293266058360562\n",
      "The training loss is 8.134226139666431 with std:13.530677418445539. The val loss is 11.352827981943683 with std:27.082757565499143.\n",
      "11.352827981943683 5.293266058360562\n",
      "Evaluating for {'lmda': 5.342293299538352} ...\n",
      "The training loss is 8.257362770379823 with std:16.034388963489906. The val loss is 26.544504998371256 with std:155.41401509819153.\n",
      "26.544504998371256 5.342293299538352\n",
      "The training loss is 7.251153579671803 with std:13.331333107462335. The val loss is 17.189053204863352 with std:88.3346419734191.\n",
      "17.189053204863352 5.342293299538352\n",
      "The training loss is 8.908709263584194 with std:17.684933236371776. The val loss is 7.550291858127329 with std:10.231939064453309.\n",
      "7.550291858127329 5.342293299538352\n",
      "The training loss is 8.141024730561668 with std:13.546952182239648. The val loss is 11.363925483298928 with std:27.12089438611371.\n",
      "11.363925483298928 5.342293299538352\n",
      "Evaluating for {'lmda': 5.3917746403875} ...\n",
      "The training loss is 8.264627667244618 with std:16.051570827892977. The val loss is 26.676656985975068 with std:156.54437061977876.\n",
      "26.676656985975068 5.3917746403875\n",
      "The training loss is 7.256277090517324 with std:13.350751713795047. The val loss is 17.178398401278653 with std:88.22834836293175.\n",
      "17.178398401278653 5.3917746403875\n",
      "The training loss is 8.916117588646358 with std:17.706025834452944. The val loss is 7.55339803588144 with std:10.242676102228211.\n",
      "7.55339803588144 5.3917746403875\n",
      "The training loss is 8.147873744310052 with std:13.563317181051065. The val loss is 11.375150393055156 with std:27.15936380800125.\n",
      "11.375150393055156 5.3917746403875\n",
      "Evaluating for {'lmda': 5.441714286865893} ...\n",
      "The training loss is 8.27194038679034 with std:16.068818877116957. The val loss is 26.809266187105795 with std:157.6778383358055.\n",
      "26.809266187105795 5.441714286865893\n",
      "The training loss is 7.2614599171644185 with std:13.370344678443555. The val loss is 17.16764814899756 with std:88.12090933583745.\n",
      "17.16764814899756 5.441714286865893\n",
      "The training loss is 8.923572220734709 with std:17.727205178499382. The val loss is 7.556564485175096 with std:10.253522451775213.\n",
      "7.556564485175096 5.441714286865893\n",
      "The training loss is 8.154774154078327 with std:13.579773722305339. The val loss is 11.386504541657235 with std:27.1981688237684.\n",
      "11.386504541657235 5.441714286865893\n",
      "Evaluating for {'lmda': 5.49211648388779} ...\n",
      "The training loss is 8.279301853724103 with std:16.086134230814114. The val loss is 26.942327548086336 with std:158.81435798579932.\n",
      "26.942327548086336 5.49211648388779\n",
      "The training loss is 7.266703065784264 with std:13.390113879903987. The val loss is 17.15680311903923 with std:88.01232528245033.\n",
      "17.15680311903923 5.49211648388779\n",
      "The training loss is 8.931074112337006 with std:17.748472449706483. The val loss is 7.559792226423979 with std:10.264479553711022.\n",
      "7.559792226423979 5.49211648388779\n",
      "The training loss is 8.16172694847164 with std:13.596323128924924. The val loss is 11.39798978235779 with std:27.237312448816116.\n",
      "11.39798978235779 5.49211648388779\n",
      "Evaluating for {'lmda': 5.542985515684669} ...\n",
      "The training loss is 8.28671300797185 with std:16.103518024998515. The val loss is 27.07583596468608 with std:159.95386870151475.\n",
      "27.07583596468608 5.542985515684669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 7.272007559079844 with std:13.410061215703331. The val loss is 17.14586400056962 with std:87.90259664623791.\n",
      "17.14586400056962 5.542985515684669\n",
      "The training loss is 8.938624231717851 with std:17.7698288463669. The val loss is 7.563082294041706 with std:10.275548869103476.\n",
      "7.563082294041706 5.542985515684669\n",
      "The training loss is 8.16873313177547 with std:13.612966739517452. The val loss is 11.409607991547324 with std:27.27679772148938.\n",
      "11.409607991547324 5.542985515684669\n",
      "Evaluating for {'lmda': 5.594325706169378} ...\n",
      "The training loss is 8.294174804922397 with std:16.120971412195786. The val loss is 27.20978628340298 with std:161.0963090158876.\n",
      "27.20978628340298 5.594325706169378\n",
      "The training loss is 7.277374436546587 with std:13.430188602504636. The val loss is 17.134831501208833 with std:87.79172392454501.\n",
      "17.134831501208833 5.594325706169378\n",
      "The training loss is 8.946223563181892 with std:17.791275584065765. The val loss is 7.566435736692677 with std:10.286731879737625.\n",
      "7.566435736692677 5.594325706169378\n",
      "The training loss is 8.17579372420136 with std:13.629705908566914. The val loss is 11.421361069088443 with std:27.3166277032287.\n",
      "11.421361069088443 5.594325706169378\n",
      "Evaluating for {'lmda': 5.6461414193036665} ...\n",
      "The training loss is 8.301688215675458 with std:16.13849556159525. The val loss is 27.344173302758385 with std:162.24161687205856.\n",
      "27.344173302758385 5.6461414193036665\n",
      "The training loss is 7.282804754736613 with std:13.450497976210734. The val loss is 17.123706347341777 with std:87.67970766931468.\n",
      "17.123706347341777 5.6461414193036665\n",
      "The training loss is 8.953873107340897 with std:17.812813895878147. The val loss is 7.569853617549508 with std:10.29803008837848.\n",
      "7.569853617549508 5.6461414193036665\n",
      "The training loss is 8.182909762136024 with std:13.646542006626445. The val loss is 11.433250938653206 with std:27.356805478721373.\n",
      "11.433250938653206 5.6461414193036665\n",
      "Evaluating for {'lmda': 5.698437059469142} ...\n",
      "The training loss is 8.309254227293728 with std:16.15609165920563. The val loss is 27.478991774629023 with std:163.3897296326988.\n",
      "27.478991774629023 5.698437059469142\n",
      "The training loss is 7.288299587526989 with std:13.47099129206754. The val loss is 17.112489284432883 with std:87.56654848781103.\n",
      "17.112489284432883 5.698437059469142\n",
      "The training loss is 8.961573881385148 with std:17.834445032569533. The val loss is 7.5733370145556576 with std:10.309445019031536.\n",
      "7.5733370145556576 5.698437059469142\n",
      "The training loss is 8.190082298394497 with std:13.66347642051434. The val loss is 11.445279548064624 with std:27.397334156053432.\n",
      "11.445279548064624 5.698437059469142\n",
      "Evaluating for {'lmda': 5.751217071841614} ...\n",
      "The training loss is 8.3168738430594 with std:16.173760908013158. The val loss is 27.614236405597143 with std:164.54058408945184.\n",
      "27.614236405597143 5.751217071841614\n",
      "The training loss is 7.293860026391592 with std:13.491670524765514. The val loss is 17.10118107734401 with std:87.4522470433424.\n",
      "17.10118107734401 5.751217071841614\n",
      "The training loss is 8.969326919358744 with std:17.85617026279806. The val loss is 7.5768870206928955 with std:10.320978217199821.\n",
      "7.5768870206928955 5.751217071841614\n",
      "The training loss is 8.197312402476712 with std:13.680510553511663. The val loss is 11.457448869641537 with std:27.438216866862167.\n",
      "11.457448869641537 5.751217071841614\n",
      "Evaluating for {'lmda': 5.804485942768978} ...\n",
      "The training loss is 8.324548082734726 with std:16.191504528143522. The val loss is 27.749901858335015 with std:165.6941164726483.\n",
      "27.749901858335015 5.804485942768978\n",
      "The training loss is 7.299487180677014 with std:13.512537668540201. The val loss is 17.089782510655446 with std:87.33680405597268.\n",
      "17.089782510655446 5.804485942768978\n",
      "The training loss is 8.977133272439183 with std:17.87799087331962. The val loss is 7.580504744253937 with std:10.332631250138045.\n",
      "7.580504744253937 5.804485942768978\n",
      "The training loss is 8.204601160828002 with std:13.697645825562482. The val loss is 11.469760900547303 with std:27.479456766488685.\n",
      "11.469760900547303 5.804485942768978\n",
      "Evaluating for {'lmda': 5.858248200152536} ...\n",
      "The training loss is 8.332277982827048 with std:16.209323757026258. The val loss is 27.88598275300563 with std:166.85026246112264.\n",
      "27.88598275300563 5.858248200152536\n",
      "The training loss is 7.305182177882316 with std:13.533594737271532. The val loss is 17.07829438899121 with std:87.22022030324311.\n",
      "17.07829438899121 5.858248200152536\n",
      "The training loss is 8.984994009221175 with std:17.899908169195218. The val loss is 7.584191309120109 with std:10.344405707103608.\n",
      "7.584191309120109 5.858248200152536\n",
      "The training loss is 8.211949677103586 with std:13.714883673476947. The val loss is 11.482217663142288 with std:27.52105703413143.\n",
      "11.482217663142288 5.858248200152536\n",
      "Evaluating for {'lmda': 5.912508413831875} ...\n",
      "The training loss is 8.34006459685814 with std:16.227219849562793. The val loss is 28.022473668702183 with std:168.00895719234074.\n",
      "28.022473668702183 5.912508413831875\n",
      "The training loss is 7.310946163942825 with std:13.554843764581898. The val loss is 17.066717537347657 with std:87.1024966208861.\n",
      "17.066717537347657 5.912508413831875\n",
      "The training loss is 8.99291021600463 with std:17.921923474000568. The val loss is 7.587947855044198 with std:10.356303199603904.\n",
      "7.587947855044198 5.912508413831875\n",
      "The training loss is 8.21935907243668 with std:13.73222555113596. The val loss is 11.494821205339981 with std:27.563020872999378.\n",
      "11.494821205339981 5.912508413831875\n",
      "Evaluating for {'lmda': 5.967271195973317} ...\n",
      "The training loss is 8.347908995637955 with std:16.245194078297295. The val loss is 28.159369144899856 with std:169.1701352725898.\n",
      "28.159369144899856 5.967271195973317\n",
      "The training loss is 7.316780303517974 with std:13.576286803932945. The val loss is 17.0550528014254 with std:86.98363390353475.\n",
      "17.0550528014254 5.967271195973317\n",
      "The training loss is 9.000882997087086 with std:17.944038130038678. The val loss is 7.591775537938506 with std:10.368325361640641.\n",
      "7.591775537938506 5.967271195973317\n",
      "The training loss is 8.226830485710758 with std:13.749672929698717. The val loss is 11.507573600967019 with std:27.605351510466278.\n",
      "11.507573600967019 5.967271195973317\n",
      "Evaluating for {'lmda': 6.0225412014619275} ...\n",
      "The training loss is 8.355812267542815 with std:16.263247733590767. The val loss is 28.29666368294001 with std:170.333730787425.\n",
      "28.29666368294001 6.0225412014619275\n",
      "The training loss is 7.322685780283129 with std:13.597925928720821. The val loss is 17.043301047964743 with std:86.86363310542978.\n",
      "17.043301047964743 6.0225412014619275\n",
      "The training loss is 9.008913475060298 with std:17.96625349855388. The val loss is 7.595675530168103 with std:10.38047384994986.\n",
      "7.595675530168103 6.0225412014619275\n",
      "The training loss is 8.234365073835745 with std:13.767227297812902. The val loss is 11.52047695012681 with std:27.648052198224303.\n",
      "11.52047695012681 6.0225412014619275\n",
      "Evaluating for {'lmda': 6.07832312829723} ...\n",
      "The training loss is 8.363775518798235 with std:16.281382123798572. The val loss is 28.43435174753955 with std:171.4996773122956.\n",
      "28.43435174753955 6.07832312829723\n",
      "The training loss is 7.32866379722569 with std:13.619763232370346. The val loss is 17.03146316508577 with std:86.74249524113151.\n",
      "17.03146316508577 6.07832312829723\n",
      "The training loss is 9.017002791111457 with std:17.98857095994918. The val loss is 7.599649020849387 with std:10.3927503442388.\n",
      "7.599649020849387 6.07832312829723\n",
      "The training loss is 8.24196401202823 with std:13.784890161826423. The val loss is 11.53353337956719 with std:27.691126212438473.\n",
      "11.53353337956719 6.07832312829723\n",
      "Evaluating for {'lmda': 6.134621717992506} ...\n",
      "The training loss is 8.371799873766276 with std:16.29959857545118. The val loss is 28.57242776831448 with std:172.6679079232517.\n",
      "28.57242776831448 6.134621717992506\n",
      "The training loss is 7.33471557694529 with std:13.641800828427694. The val loss is 17.01954006263064 with std:86.62022138621565.\n",
      "17.01954006263064 6.134621717992506\n",
      "The training loss is 9.025152105328672 with std:18.010991914005487. The val loss is 7.6036972161539635 with std:10.405156547418448.\n",
      "7.6036972161539635 6.134621717992506\n",
      "The training loss is 8.249628494095742 with std:13.802663046002372. The val loss is 11.546745043051692 with std:27.734576853901103.\n",
      "11.546745043051692 6.134621717992506\n",
      "Evaluating for {'lmda': 6.191441755977841} ...\n",
      "The training loss is 8.379886475237473 with std:16.31789843343782. The val loss is 28.71088614134324 with std:173.8383552079739.\n",
      "28.71088614134324 6.191441755977841\n",
      "The training loss is 7.340842361958294 with std:13.664040850651492. The val loss is 17.00753267251057 with std:86.49681267797567.\n",
      "17.00753267251057 6.191441755977841\n",
      "The training loss is 9.033362597010997 with std:18.033517780103168. The val loss is 7.607821339617961 with std:10.417694185832408.\n",
      "7.607821339617961 6.191441755977841\n",
      "The training loss is 8.257359732725282 with std:13.820547492735935. The val loss is 11.560114121734946 with std:27.77840744818664.\n",
      "11.560114121734946 6.191441755977841\n",
      "Evaluating for {'lmda': 6.248788072006894} ...\n",
      "The training loss is 8.388036484727522 with std:16.336283061193846. The val loss is 28.849721230743313 with std:175.01095127687162.\n",
      "28.849721230743313 6.248788072006894\n",
      "The training loss is 7.347045415006543 with std:13.686485453102769. The val loss is 16.995441949056133 with std:86.37227031611708.\n",
      "16.995441949056133 6.248788072006894\n",
      "The training loss is 9.041635464983118 with std:18.056149997446315. The val loss is 7.612022632456818 with std:10.43036500948156.\n",
      "7.612022632456818 6.248788072006894\n",
      "The training loss is 8.265158959776013 with std:13.83854506277404. The val loss is 11.573642824541658 with std:27.822621345805665.\n",
      "11.573642824541658 6.248788072006894\n",
      "Evaluating for {'lmda': 6.306665540567406} ...\n",
      "The training loss is 8.396251082778743 with std:16.354753840891313. The val loss is 28.988927370269916 with std:176.1856277743344.\n",
      "28.988927370269916 6.306665540567406\n",
      "The training loss is 7.353326019370519 with std:13.709136810233272. The val loss is 16.983268869371194 with std:86.24659556344652.\n",
      "16.983268869371194 6.306665540567406\n",
      "The training loss is 9.049971927914333 with std:18.078890025288764. The val loss is 7.616302353885273 with std:10.44317079224421.\n",
      "7.616302353885273 6.306665540567406\n",
      "The training loss is 8.273027426576084 with std:13.856657335436807. The val loss is 11.587333388549773 with std:27.867221922360436.\n",
      "11.587333388549773 6.306665540567406\n",
      "Evaluating for {'lmda': 6.365079081295571} ...\n",
      "The training loss is 8.404531469266272 with std:16.37331217363269. The val loss is 29.128498864940163 with std:177.3623158901716.\n",
      "29.128498864940163 6.365079081295571\n",
      "The training loss is 7.359685479186833 with std:13.731997116972172. The val loss is 16.97101443369102 with std:86.11978974656171.\n",
      "16.97101443369102 6.365079081295571\n",
      "The training loss is 9.058373224642624 with std:18.10173934316336. The val loss is 7.620661781443324 with std:10.456113332092457.\n",
      "7.620661781443324 6.365079081295571\n",
      "The training loss is 8.280966404223884 with std:13.87488590884211. The val loss is 11.60118807937715 with std:27.912212578698654.\n",
      "11.60118807937715 6.365079081295571\n",
      "Evaluating for {'lmda': 6.4240336593941905} ...\n",
      "The training loss is 8.412878863709139 with std:16.391959479648293. The val loss is 29.26842999268545 with std:178.5409463712586.\n",
      "29.26842999268545 6.4240336593941905\n",
      "The training loss is 7.366125119770278 with std:13.755068588811389. The val loss is 16.95867966574367 with std:85.99185425653191.\n",
      "16.95867966574367 6.4240336593941905\n",
      "The training loss is 9.066840614502953 with std:18.124699451112523. The val loss is 7.625102211327358 with std:10.46919445130356.\n",
      "7.625102211327358 6.4240336593941905\n",
      "The training loss is 8.288977183893632 with std:13.893232400131978. The val loss is 11.615209191572879 with std:27.957596741069178.\n",
      "11.615209191572879 6.4240336593941905\n",
      "Evaluating for {'lmda': 6.483534286054721} ...\n",
      "The training loss is 8.421294505586243 with std:16.41069719849665. The val loss is 29.4087150060126 with std:179.72144953321302.\n",
      "29.4087150060126 6.483534286054721\n",
      "The training loss is 7.3726462879402614 with std:13.778353461889253. The val loss is 16.94626561311519 with std:85.86279054957329.\n",
      "16.94626561311519 6.483534286054721\n",
      "The training loss is 9.07537537766064 with std:18.147771869921872. The val loss is 7.629624958727119 with std:10.482415996667072.\n",
      "7.629624958727119 6.483534286054721\n",
      "The training loss is 8.29706107714534 with std:13.911698445701656. The val loss is 11.629399049011752 with std:28.003377861275478.\n",
      "11.629399049011752 6.483534286054721\n",
      "Evaluating for {'lmda': 6.543586018883236} ...\n",
      "The training loss is 8.429779654657377 with std:16.42952678926877. The val loss is 29.54934813369645 with std:180.90375527230952.\n",
      "29.54934813369645 6.543586018883236\n",
      "The training loss is 7.379250352351955 with std:13.80185399307263. The val loss is 16.933773347619407 with std:85.73260014772538.\n",
      "16.933773347619407 6.543586018883236\n",
      "The training loss is 9.083978815449248 with std:18.170958141355616. The val loss is 7.634231358168254 with std:10.495779839687211.\n",
      "7.634231358168254 6.543586018883236\n",
      "The training loss is 8.30521941623919 with std:13.930285701430877. The val loss is 11.643760005293329 with std:28.049559416830494.\n",
      "11.643760005293329 6.543586018883236\n",
      "Evaluating for {'lmda': 6.604193962330306} ...\n",
      "The training loss is 8.438335591289137 with std:16.448449730795275. The val loss is 29.690323582495033 with std:182.08779307755285.\n",
      "29.690323582495033 6.604193962330306\n",
      "The training loss is 7.385938703832024 with std:13.825572460037495. The val loss is 16.921203965670045 with std:85.60128463951276.\n",
      "16.921203965670045 6.604193962330306\n",
      "The training loss is 9.092652250713638 with std:18.194259828394742. The val loss is 7.63892276386064 with std:10.509287876780474.\n",
      "7.63892276386064 6.604193962330306\n",
      "The training loss is 8.313453554454577 with std:13.948995842917785. The val loss is 11.658294444144943 with std:28.096144911111583.\n",
      "11.658294444144943 6.604193962330306\n",
      "Evaluating for {'lmda': 6.6653632681249135} ...\n",
      "The training loss is 8.44696361678599 with std:16.467467521857337. The val loss is 29.83163553887082 with std:183.273492042751.\n",
      "29.83163553887082 6.6653632681249135\n",
      "The training loss is 7.392712755718901 with std:13.849511161347305. The val loss is 16.90855858865839 with std:85.46884568061034.\n",
      "16.90855858865839 6.6653632681249135\n",
      "The training loss is 9.101397028157526 with std:18.217678515476763. The val loss is 7.64370055005244 with std:10.522942029468373.\n",
      "7.64370055005244 6.6653632681249135\n",
      "The training loss is 8.321764866413535 with std:13.967830565714808. The val loss is 11.673004779828204 with std:28.143137873512764.\n",
      "11.673004779828204 6.6653632681249135\n",
      "Evaluating for {'lmda': 6.727099135712336} ...\n",
      "The training loss is 8.455665053726545 with std:16.486581681401038. The val loss is 29.973278170751737 with std:184.46078087890564.\n",
      "29.973278170751737 6.727099135712336\n",
      "The training loss is 7.399573944208139 with std:13.87367241653086. The val loss is 16.895838363333834 with std:85.33528499449841.\n",
      "16.895838363333834 6.727099135712336\n",
      "The training loss is 9.110214514696366 with std:18.24121580873874. The val loss is 7.648566111389855 with std:10.53674424456519.\n",
      "7.648566111389855 6.727099135712336\n",
      "The training loss is 8.330154748408978 with std:13.986791585567275. The val loss is 11.687893457550706 with std:28.190541859600504.\n",
      "11.687893457550706 6.727099135712336\n",
      "Evaluating for {'lmda': 6.789406812696106} ...\n",
      "The training loss is 8.464441246304869 with std:16.505793748755003. The val loss is 30.11524562929482 with std:185.64958792657197.\n",
      "30.11524562929482 6.789406812696106\n",
      "The training loss is 7.406523728702156 with std:13.898058566156973. The val loss is 16.883044462188586 with std:85.20060437311139.\n",
      "16.883044462188586 6.789406812696106\n",
      "The training loss is 9.119106099814873 with std:18.26487333626107. The val loss is 7.653520863283011 with std:10.550696494360835.\n",
      "7.653520863283011 6.789406812696106\n",
      "The training loss is 8.338624618737352 with std:14.005880638653974. The val loss is 11.702962953880462 with std:28.23836045126528.\n",
      "11.702962953880462 6.789406812696106\n",
      "Evaluating for {'lmda': 6.852291595284065} ...\n",
      "The training loss is 8.47329356067716 with std:16.525105283851452. The val loss is 30.25753205068001 with std:186.83984116844468.\n",
      "30.25753205068001 6.852291595284065\n",
      "The training loss is 7.413563592165168 with std:13.922671971908464. The val loss is 16.870178083846202 with std:85.06480567748056.\n",
      "16.870178083846202 6.852291595284065\n",
      "The training loss is 9.128073195929916 with std:18.288652748315116. The val loss is 7.658566242277395 with std:10.564800776798323.\n",
      "7.658566242277395 6.852291595284065\n",
      "The training loss is 8.347175918036402 with std:14.025099481830543. The val loss is 11.718215777165573 with std:28.286597256875478.\n",
      "11.718215777165573 6.852291595284065\n",
      "Evaluating for {'lmda': 6.915758828738525} ...\n",
      "The training loss is 8.482223385313693 with std:16.54451786745104. The val loss is 30.400131557917177 with std:188.03146824201022.\n",
      "30.400131557917177 6.915758828738525\n",
      "The training loss is 7.420695041482883 with std:13.947515016654187. The val loss is 16.857240453454505 with std:84.92789083837303.\n",
      "16.857240453454505 6.915758828738525\n",
      "The training loss is 9.137117238758165 with std:18.312555717611406. The val loss is 7.663703706431536 with std:10.579059115646059.\n",
      "7.663703706431536 6.915758828738525\n",
      "The training loss is 8.355810109627306 with std:14.044449892874749. The val loss is 11.733654467957578 with std:28.335255911430032.\n",
      "11.733654467957578 6.915758828738525\n",
      "Evaluating for {'lmda': 6.9798139078306605} ...\n",
      "The training loss is 8.491232131356101 with std:16.564033101370807. The val loss is 30.54303826267021 with std:189.22439645230483.\n",
      "30.54303826267021 6.9798139078306605\n",
      "The training loss is 7.4279196078272065 with std:13.97259010451881. The val loss is 16.844232823081963 with std:84.78986185691957.\n",
      "16.844232823081963 6.9798139078306605\n",
      "The training loss is 9.14623968768919 with std:18.33658393955125. The val loss is 7.668934735700442 with std:10.593473560665085.\n",
      "7.668934735700442 6.9798139078306605\n",
      "The training loss is 8.364528679861978 with std:14.063933670734032. The val loss is 11.749281599438781 with std:28.384340076709265.\n",
      "11.749281599438781 6.9798139078306605\n",
      "Evaluating for {'lmda': 7.044462277299037} ...\n",
      "The training loss is 8.500321232980165 with std:16.58365260871585. The val loss is 30.68624626710438 with std:190.41855278482956.\n",
      "30.68624626710438 7.044462277299037\n",
      "The training loss is 7.435238847026003 with std:13.9978996609514. The val loss is 16.831156472118835 with std:84.65072080524205.\n",
      "16.831156472118835 7.044462277299037\n",
      "The training loss is 9.155442026163607 with std:18.36073913247981. The val loss is 7.674260832325314 with std:10.608046187770475.\n",
      "7.674260832325314 7.044462277299037\n",
      "The training loss is 8.373333138475141 with std:14.083552635775957. The val loss is 11.765099777854628 with std:28.433853441428536.\n",
      "11.765099777854628 7.044462277299037\n",
      "Evaluating for {'lmda': 7.109709432312432} ...\n",
      "The training loss is 8.509492147763968 with std:16.603378034114368. The val loss is 30.82974966574423 with std:191.61386391851102.\n",
      "30.82974966574423 7.109709432312432\n",
      "The training loss is 7.442654339937952 with std:14.023446132791562. The val loss is 16.818012707681383 with std:84.51046982706436.\n",
      "16.818012707681383 7.109709432312432\n",
      "The training loss is 9.164725762056495 with std:18.385023037941526. The val loss is 7.679683521229115 with std:10.622779099187678.\n",
      "7.679683521229115 7.109709432312432\n",
      "The training loss is 8.382225018941345 with std:14.103308630040074. The val loss is 11.781111642948977 with std:28.483799721386585.\n",
      "11.781111642948977 7.109709432312432\n",
      "Evaluating for {'lmda': 7.175560918936928} ...\n",
      "The training loss is 8.518746357061623 with std:16.62321104395607. The val loss is 30.973542547350682 with std:192.81025623878142.\n",
      "30.973542547350682 7.175560918936928\n",
      "The training loss is 7.4501676928325455 with std:14.049231988334295. The val loss is 16.804802865021294 with std:84.36911113832582.\n",
      "16.804802865021294 7.175560918936928\n",
      "The training loss is 9.174092428066059 with std:18.409437420937117. The val loss is 7.685204350418441 with std:10.637674423602865.\n",
      "7.685204350418441 7.175560918936928\n",
      "The training loss is 8.391205878837035 with std:14.123203517492588. The val loss is 11.797319868405172 with std:28.53418265961786.\n",
      "11.797319868405172 7.175560918936928\n",
      "Evaluating for {'lmda': 7.242022334607316} ...\n",
      "The training loss is 8.528085366382696 with std:16.64315332663446. The val loss is 31.117618996816667 with std:194.0076558507742.\n",
      "31.117618996816667 7.242022334607316\n",
      "The training loss is 7.4577805377753466 with std:14.075259717392301. The val loss is 16.791528307938233 with std:84.22664702778019.\n",
      "16.791528307938233 7.242022334607316\n",
      "The training loss is 9.183543582107875 with std:18.43398407018363. The val loss is 7.690824891391479 with std:10.652734316308075.\n",
      "7.690824891391479 7.242022334607316\n",
      "The training loss is 8.400277300207735 with std:14.143239184283264. The val loss is 11.813727162290151 with std:28.585006026540857.\n",
      "11.813727162290151 7.242022334607316\n",
      "Evaluating for {'lmda': 7.309099328602911} ...\n",
      "The training loss is 8.537510705777041 with std:16.663206592792005. The val loss is 31.261973097070797 with std:195.20598859254073.\n",
      "31.261973097070797 7.309099328602911\n",
      "The training loss is 7.465494533018494 with std:14.101531831357056. The val loss is 16.778190429197522 with std:84.08307985759123.\n",
      "16.778190429197522 7.309099328602911\n",
      "The training loss is 9.193080807714265 with std:18.458664798375455. The val loss is 7.6965467395523 with std:10.66796095934041.\n",
      "7.6965467395523 7.309099328602911\n",
      "The training loss is 8.409440889940235 with std:14.163417539004108. The val loss is 11.83033626750366 with std:28.636273620107293.\n",
      "11.83033626750366 7.309099328602911\n",
      "Evaluating for {'lmda': 7.376797602527731} ...\n",
      "The training loss is 8.547023930225683 with std:16.6833725755695. The val loss is 31.406598931006997 with std:196.40518004844552.\n",
      "31.406598931006997 7.376797602527731\n",
      "The training loss is 7.473311363396607 with std:14.128050863257458. The val loss is 16.76479065095131 with std:83.93841206391899.\n",
      "16.76479065095131 7.376797602527731\n",
      "The training loss is 9.202705714439322 with std:18.48348144244777. The val loss is 7.702371514631219 with std:10.683356561615373.\n",
      "7.702371514631219 7.376797602527731\n",
      "The training loss is 8.418698280140134 with std:14.183740512950699. The val loss is 11.847149962231391 with std:28.687989265949742.\n",
      "11.847149962231391 7.376797602527731\n",
      "Evaluating for {'lmda': 7.445122910795136} ...\n",
      "The training loss is 8.556626620037054 with std:16.703653030858096. The val loss is 31.551490583413717 with std:197.6051555625051.\n",
      "31.551490583413717 7.445122910795136\n",
      "The training loss is 7.4812327407280845 with std:14.154819367816676. The val loss is 16.751330425164156 with std:83.79264615749454.\n",
      "16.751330425164156 7.445122910795136\n",
      "The training loss is 9.212419938269425 with std:18.508435863841992. The val loss is 7.708300861111729 with std:10.698923359054605.\n",
      "7.708300861111729 7.445122910795136\n",
      "The training loss is 8.428051128514472 with std:14.204210060385279. The val loss is 11.864171060402912 with std:28.7401568175293.\n",
      "11.864171060402912 7.445122910795136\n",
      "Evaluating for {'lmda': 7.51408106111697} ...\n",
      "The training loss is 8.566320381249456 with std:16.724049737555475. The val loss is 31.696642142935026 with std:198.80584025196072.\n",
      "31.696642142935026 7.51408106111697\n",
      "The training loss is 7.489260404221785 with std:14.181839921506848. The val loss is 16.737811234043146 with std:83.64578472419419.\n",
      "16.737811234043146 7.51408106111697\n",
      "The training loss is 9.222225142039305 with std:18.53352994877323. The val loss is 7.714336448663626 with std:10.71466361470745.\n",
      "7.714336448663626 7.51408106111697\n",
      "The training loss is 8.437501118759833 with std:14.224828158802477. The val loss is 11.88140241215355 with std:28.792780156280834.\n",
      "11.88140241215355 7.51408106111697\n",
      "Evaluating for {'lmda': 7.583677914997191} ...\n",
      "The training loss is 8.576106846039114 with std:16.744564497825092. The val loss is 31.84204770402491 with std:200.00715902073455.\n",
      "31.84204770402491 7.583677914997191\n",
      "The training loss is 7.49739612088946 with std:14.209115122602183. The val loss is 16.724234590472175 with std:83.49783042559802.\n",
      "16.724234590472175 7.583677914997191\n",
      "The training loss is 9.232123015853754 with std:18.558765608499. The val loss is 7.72047997258257 with std:10.73057961886683.\n",
      "7.72047997258257 7.583677914997191\n",
      "The training loss is 8.447049960955669 with std:14.245596809196497. The val loss is 11.898846904291124 with std:28.845863191758532.\n",
      "11.898846904291124 7.583677914997191\n",
      "Evaluating for {'lmda': 7.653919388230148} ...\n",
      "The training loss is 8.585987673134303 with std:16.765199137359108. The val loss is 31.987701368929926 with std:201.20903657308597.\n",
      "31.987701368929926 7.653919388230148\n",
      "The training loss is 7.505641685963549 with std:14.236647591229426. The val loss is 16.710602038450276 with std:83.34878599954233.\n",
      "16.710602038450276 7.653919388230148\n",
      "The training loss is 9.2421152775151 with std:18.584144779590634. The val loss is 7.72673315423629 with std:10.746673689178886.\n",
      "7.72673315423629 7.653919388230148\n",
      "The training loss is 8.456699391963095 with std:14.266518036330844. The val loss is 11.916507460767033 with std:28.89940986178014.\n",
      "11.916507460767033 7.653919388230148\n",
      "Evaluating for {'lmda': 7.7248114514034} ...\n",
      "The training loss is 8.595964548235461 with std:16.785955505644356. The val loss is 32.13359724967749 with std:202.41139742726375.\n",
      "32.13359724967749 7.7248114514034\n",
      "The training loss is 7.5139989233208455 with std:14.26443996941692. The val loss is 16.696915153534928 with std:83.19865426066507.\n",
      "16.696915153534928 7.7248114514034\n",
      "The training loss is 9.252203672956302 with std:18.60966942420586. The val loss is 7.733097741516909 with std:10.762948170746604.\n",
      "7.733097741516909 7.7248114514034\n",
      "The training loss is 8.466451175829278 with std:14.28759388900991. The val loss is 11.934387043151814 with std:28.953424132569335.\n",
      "11.934387043151814 7.7248114514034\n",
      "Evaluating for {'lmda': 7.796360130405229} ...\n",
      "The training loss is 8.606039184441304 with std:16.806835476232138. The val loss is 32.27972947007408 with std:203.61416592919176.\n",
      "32.27972947007408 7.796360130405229\n",
      "The training loss is 7.522469685911765 with std:14.292494921141065. The val loss is 16.68317554328954 with std:83.04743810094246.\n",
      "16.68317554328954 7.796360130405229\n",
      "The training loss is 9.262389976679916 with std:18.635341530363366. The val loss is 7.739575509300287 with std:10.779405436227295.\n",
      "7.739575509300287 7.796360130405229\n",
      "The training loss is 8.476307104197176 with std:14.308826440352522. The val loss is 11.952488651115168 with std:29.007909998897457.\n",
      "11.952488651115168 7.796360130405229\n",
      "Evaluating for {'lmda': 7.868571506936852} ...\n",
      "The training loss is 8.61621332268112 with std:16.827840947011147. The val loss is 32.426092167718444 with std:204.8172662662338.\n",
      "32.426092167718444 7.868571506936852\n",
      "The training loss is 7.531055856195504 with std:14.320815132370917. The val loss is 16.669384847735135 with std:82.89514049021241.\n",
      "16.669384847735135 7.868571506936852\n",
      "The training loss is 9.272675992202966 with std:18.661163112218983. The val loss is 7.746168259911579 with std:10.796047885923873.\n",
      "7.746168259911579 7.868571506936852\n",
      "The training loss is 8.486268996721094 with std:14.330217788067594. The val loss is 11.970815322911044 with std:29.062871484224555.\n",
      "11.970815322911044 7.868571506936852\n",
      "Evaluating for {'lmda': 7.94145171902934} ...\n",
      "The training loss is 8.626488732153172 with std:16.84897384048382. The val loss is 32.572679496021486 with std:206.0206224809599.\n",
      "32.572679496021486 7.94145171902934\n",
      "The training loss is 7.53975934658097 with std:14.349403311110681. The val loss is 16.655544739806857 with std:82.74176447669186.\n",
      "16.655544739806857 7.94145171902934\n",
      "The training loss is 9.283063552507647 with std:18.68713621034375. The val loss is 7.752877823597683 with std:10.812877947870025.\n",
      "7.752877823597683 7.94145171902934\n",
      "The training loss is 8.496338701487707 with std:14.351770054731537. The val loss is 11.98937013586654 with std:29.118312640837473.\n",
      "11.98937013586654 7.94145171902934\n",
      "Evaluating for {'lmda': 8.015006961565405} ...\n",
      "The training loss is 8.636867210769394 with std:16.8702361040462. The val loss is 32.71948562623729 with std:207.22415848495828.\n",
      "32.71948562623729 8.015006961565405\n",
      "The training loss is 7.548582099873787 with std:14.378262187440093. The val loss is 16.641656925815205 with std:82.5873131874879.\n",
      "16.641656925815205 8.015006961565405\n",
      "The training loss is 9.293554520498098 with std:18.713262892003513. The val loss is 7.7597060590062545 with std:10.829898077908961.\n",
      "7.7597060590062545 8.015006961565405\n",
      "The training loss is 8.506518095442999 with std:14.373485388067568. The val loss is 12.008156206876313 with std:29.174237549988252.\n",
      "12.008156206876313 8.015006961565405\n",
      "Evaluating for {'lmda': 8.089243486805938} ...\n",
      "The training loss is 8.647350585606349 with std:16.8916297102708. The val loss is 32.866504749503164 with std:208.427798072668.\n",
      "32.866504749503164 8.089243486805938\n",
      "The training loss is 7.557526089729101 with std:14.407394513552408. The val loss is 16.627723145911432 with std:82.43178982909379.\n",
      "16.627723145911432 8.089243486805938\n",
      "The training loss is 9.304150789463078 with std:18.739545251439804. The val loss is 7.766654853671571 with std:10.847110759765943.\n",
      "7.766654853671571 8.089243486805938\n",
      "The training loss is 8.516809084824853 with std:14.395365961226968. The val loss is 12.027176692901072 with std:29.230650322030087.\n",
      "12.027176692901072 8.089243486805938\n",
      "Evaluating for {'lmda': 8.164167604921463} ...\n",
      "The training loss is 8.657940713362475 with std:16.91315665719281. The val loss is 33.01373107888957 with std:209.63146493524573.\n",
      "33.01373107888957 8.164167604921463\n",
      "The training loss is 7.56659332111076 with std:14.436803063790892. The val loss is 16.613745174557835 with std:82.27519768787802.\n",
      "16.613745174557835 8.164167604921463\n",
      "The training loss is 9.31485428354495 with std:18.76598541015282. The val loss is 7.773726124507041 with std:10.86451850511439.\n",
      "7.773726124507041 8.164167604921463\n",
      "The training loss is 8.527213605601597 with std:14.417413973072106. The val loss is 12.046434791471038 with std:29.287555096551852.\n",
      "12.046434791471038 8.164167604921463\n",
      "Evaluating for {'lmda': 8.23978568452852} ...\n",
      "The training loss is 8.668639480821842 with std:16.934818968600037. The val loss is 33.16115885145154 with std:210.83508267438307.\n",
      "33.16115885145154 8.23978568452852\n",
      "The training loss is 7.5757858307562485 with std:14.466490634682039. The val loss is 16.599724821002773 with std:82.11754013056544.\n",
      "16.599724821002773 8.23978568452852\n",
      "The training loss is 9.32566695821453 with std:18.792585517185355. The val loss is 7.780921818304636 with std:10.88212385363566.\n",
      "7.780921818304636 8.23978568452852\n",
      "The training loss is 8.537733623916276 with std:14.439631648460976. The val loss is 12.065933741194076 with std:29.344956042511082.\n",
      "12.065933741194076 8.23978568452852\n",
      "Evaluating for {'lmda': 8.316104153230961} ...\n",
      "The training loss is 8.679448805324322 with std:16.956618694325385. The val loss is 33.308782330301085 with std:212.03857481625968.\n",
      "33.308782330301085 8.316104153230961\n",
      "The training loss is 7.585105687648158 with std:14.496460044967785. The val loss is 16.58566392976028 with std:81.95882060470538.\n",
      "16.58566392976028 8.316104153230961\n",
      "The training loss is 9.336590800752358 with std:18.819347749408287. The val loss is 7.788243912241267 with std:10.899929373072492.\n",
      "7.788243912241267 8.316104153230961\n",
      "The training loss is 8.548371136537252 with std:14.462021238533765. The val loss is 12.085676822268946 with std:29.40285735836585.\n",
      "12.085676822268946 8.316104153230961\n",
      "Evaluating for {'lmda': 8.393129498166365} ...\n",
      "The training loss is 8.690370635242264 with std:16.97855791054288. The val loss is 33.45659580666859 with std:213.24186482533014.\n",
      "33.45659580666859 8.393129498166365\n",
      "The training loss is 7.594554993491747 with std:14.526714135634716. The val loss is 16.5715643810949 with std:81.79904263913707.\n",
      "16.5715643810949 8.393129498166365\n",
      "The training loss is 9.347627830736242 with std:18.846274311807978. The val loss is 7.795694414391881 with std:10.91793765927572.\n",
      "7.795694414391881 8.393129498166365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 8.559128171314452 with std:14.48458502100082. The val loss is 12.105667357002904 with std:29.461263272203944.\n",
      "12.105667357002904 8.393129498166365\n",
      "Evaluating for {'lmda': 8.470868266557403} ...\n",
      "The training loss is 8.701406950463706 with std:17.000638720066828. The val loss is 33.604593601982835 with std:214.44487611822595.\n",
      "33.604593601982835 8.470868266557403\n",
      "The training loss is 7.604135883198912 with std:14.557255769941227. The val loss is 16.55742809151028 with std:81.63820984443082.\n",
      "16.55742809151028 8.470868266557403\n",
      "The training loss is 9.358780100534958 with std:18.873367437773968. The val loss is 7.803275364249832 with std:10.936151336244968.\n",
      "7.803275364249832 8.470868266557403\n",
      "The training loss is 8.570006787641919 with std:14.50732530043215. The val loss is 12.125908710334235 with std:29.520178041870093.\n",
      "12.125908710334235 8.470868266557403\n",
      "Evaluating for {'lmda': 8.549327066268376} ...\n",
      "The training loss is 8.712559762882387 with std:17.022863252654073. The val loss is 33.752770069954586 with std:215.64753207762405.\n",
      "33.752770069954586 8.549327066268376\n",
      "The training loss is 7.613850525378569 with std:14.588087833442827. The val loss is 16.543257014244375 with std:81.47632591334018.\n",
      "16.543257014244375 8.549327066268376\n",
      "The training loss is 9.370049695808472 with std:18.900629389388868. The val loss is 7.810988833253929 with std:10.954573056162381.\n",
      "7.810988833253929 8.549327066268376\n",
      "The training loss is 8.58100907692656 with std:14.53024440854883. The val loss is 12.146404290360174 with std:29.579605955092454.\n",
      "12.146404290360174 8.549327066268376\n",
      "Evaluating for {'lmda': 8.628512566366895} ...\n",
      "The training loss is 8.723831116894281 with std:17.0452336653092. The val loss is 33.90111959866107 with std:216.84975606606466.\n",
      "33.90111959866107 8.628512566366895\n",
      "The training loss is 7.623701122833489 with std:14.619213234014582. The val loss is 16.529053139767974 with std:81.31339462122594.\n",
      "16.529053139767974 8.628512566366895\n",
      "The training loss is 9.381438736014625 with std:18.92806245771908. The val loss is 7.818836925322861 with std:10.973205499420207.\n",
      "7.818836925322861 8.628512566366895\n",
      "The training loss is 8.592137163063004 with std:14.553344704515391. The val loss is 12.167157548869408 with std:29.639551329606753.\n",
      "12.167157548869408 8.628512566366895\n",
      "Evaluating for {'lmda': 8.708431497690723} ...\n",
      "The training loss is 8.735223089900932 with std:17.067752142592926. The val loss is 34.049636612636064 with std:218.05147143975918.\n",
      "34.049636612636064 8.708431497690723\n",
      "The training loss is 7.63368991306372 with std:14.650634901871914. The val loss is 16.51481849628887 with std:81.14941982647831.\n",
      "16.51481849628887 8.708431497690723\n",
      "The training loss is 9.392949374922178 with std:18.955668963106362. The val loss is 7.826821777396518 with std:10.992051374641605.\n",
      "7.826821777396518 8.708431497690723\n",
      "The training loss is 8.603393202914795 with std:14.576628575234007. The val loss is 12.188171981879213 with std:29.700018513276344.\n",
      "12.188171981879213 8.708431497690723\n",
      "Evaluating for {'lmda': 8.789090653419954} ...\n",
      "The training loss is 8.746737792819674 with std:17.090420896933278. The val loss is 34.19831557496799 with std:219.2526015624053.\n",
      "34.19831557496799 8.789090653419954\n",
      "The training loss is 7.643819168776599 with std:14.682355789588666. The val loss is 16.50055515026134 with std:80.98440547093041.\n",
      "16.50055515026134 8.789090653419954\n",
      "The training loss is 9.404583801130594 with std:18.98345125546112. The val loss is 7.83494555998461 with std:11.011113418695174.\n",
      "7.83494555998461 8.789090653419954\n",
      "The training loss is 8.61477938680195 with std:14.60009843563942. The val loss is 12.209451130178579 with std:29.761011884214124.\n",
      "12.209451130178579 8.789090653419954\n",
      "Evaluating for {'lmda': 8.870496889654403} ...\n",
      "The training loss is 8.758377370600629 with std:17.113242168939852. The val loss is 34.347150989393725 with std:220.45306981890474.\n",
      "34.347150989393725 8.870496889654403\n",
      "The training loss is 7.654091198403479 with std:14.714378872113135. The val loss is 16.486265206900384 with std:80.81835558025898.\n",
      "16.486265206900384 8.870496889654403\n",
      "The training loss is 9.416344238596306 with std:19.011411714556168. The val loss is 7.843210477722361 with std:11.030394396702519.\n",
      "7.843210477722361 8.870496889654403\n",
      "The training loss is 8.626297938994883 with std:14.623756728996021. The val loss is 12.230998579875253 with std:29.82253585089774.\n",
      "12.230998579875253 8.870496889654403\n",
      "Evaluating for {'lmda': 8.952657125996392} ...\n",
      "The training loss is 8.770144002750651 with std:17.136218227720633. The val loss is 34.49613740240311 with std:221.65279962911322.\n",
      "34.49613740240311 8.952657125996392\n",
      "The training loss is 7.6645083466232355 with std:14.746707146781878. The val loss is 16.471950810701102 with std:80.65127426437559.\n",
      "16.471950810701102 8.952657125996392\n",
      "The training loss is 9.428232947165778 with std:19.03955275032165. The val loss is 7.8516187699336975 with std:11.04989710203992.\n",
      "7.8516187699336975 8.952657125996392\n",
      "The training loss is 8.637951118214877 with std:14.647605927195325. The val loss is 12.25281796294873 with std:29.884594852285435.\n",
      "12.25281796294873 8.952657125996392\n",
      "Evaluating for {'lmda': 9.03557834613893} ...\n",
      "The training loss is 8.782039903864323 with std:17.159351371202195. The val loss is 34.64526940533902 with std:222.8517144614869.\n",
      "34.64526940533902 9.03557834613893\n",
      "The training loss is 7.67507299489254 with std:14.779343633330946. The val loss is 16.457614145963433 with std:80.4831657178098.\n",
      "16.457614145963433 9.03557834613893\n",
      "The training loss is 9.440252223115197 with std:19.067876803140727. The val loss is 7.86017271120133 with std:11.069624356332618.\n",
      "7.86017271120133 9.03557834613893\n",
      "The training loss is 8.649741218141079 with std:14.671648531055087. The val loss is 12.274912957808096 with std:29.94719335792895.\n",
      "12.274912957808096 9.03557834613893\n",
      "Evaluating for {'lmda': 9.119267598459299} ...\n",
      "The training loss is 8.794067324161826 with std:17.182643926451654. The val loss is 34.7945416365001 with std:224.04973784669133.\n",
      "34.7945416365001 9.119267598459299\n",
      "The training loss is 7.6857875619831315 with std:14.81229137390522. The val loss is 16.443257437322227 with std:80.31403422008177.\n",
      "16.443257437322227 9.119267598459299\n",
      "The training loss is 9.452404399697027 with std:19.096386344146197. The val loss is 7.868874611944603 with std:11.089579009443135.\n",
      "7.868874611944603 9.119267598459299\n",
      "The training loss is 8.661670567923958 with std:14.695887070619007. The val loss is 12.297287289854799 with std:30.01033586808274.\n",
      "12.297287289854799 9.119267598459299\n",
      "Evaluating for {'lmda': 9.20373199661822} ...\n",
      "The training loss is 8.806228550034128 with std:17.206098250002512. The val loss is 34.943948783246405 with std:225.24679339116918.\n",
      "34.943948783246405 9.20373199661822\n",
      "The training loss is 7.696654504525976 with std:14.845553433065074. The val loss is 16.428882950282123 with std:80.14388413606106.\n",
      "16.428882950282123 9.20373199661822\n",
      "The training loss is 9.464691847693471 with std:19.125083875517788. The val loss is 7.877726819004504 with std:11.10976393945289.\n",
      "7.877726819004504 9.20373199661822\n",
      "The training loss is 8.67374153270568 with std:14.720324105457896. The val loss is 12.319944732050994 with std:30.07402691381184.\n",
      "12.319944732050994 9.20373199661822\n",
      "Evaluating for {'lmda': 9.288978720164497} ...\n",
      "The training loss is 8.818525904595143 with std:17.22971672818195. The val loss is 35.093485584105004 with std:226.44280479064855.\n",
      "35.093485584105004 9.288978720164497\n",
      "The training loss is 7.70767631756252 with std:14.879132897790925. The val loss is 16.414492991758863 with std:79.9727199163262.\n",
      "16.414492991758863 9.288978720164497\n",
      "The training loss is 9.47711697597679 with std:19.153971930780077. The val loss is 7.886731716236151 with std:11.130182052637213.\n",
      "7.886731716236151 9.288978720164497\n",
      "The training loss is 8.685956514146946 with std:14.744962224971196. The val loss is 12.342889105493073 with std:30.138271057096787.\n",
      "12.342889105493073 9.288978720164497\n",
      "Evaluating for {'lmda': 9.375015015145289} ...\n",
      "The training loss is 8.830961748241176 with std:17.253501777441425. The val loss is 35.24314683087081 with std:227.6376958435379.\n",
      "35.24314683087081 9.375015015145289\n",
      "The training loss is 7.718855535103067 with std:14.913032877485564. The val loss is 16.400089910624825 with std:79.8005460975023.\n",
      "16.400089910624825 9.375015015145289\n",
      "The training loss is 9.489682232076598 with std:19.18305307510096. The val loss is 7.895891725108754 with std:11.150836283433676.\n",
      "7.895891725108754 9.375015015145289\n",
      "The training loss is 8.698317950960893 with std:14.769804048689865. The val loss is 12.366124279990188 with std:30.203072890934973.\n",
      "12.366124279990188 9.375015015145289\n",
      "Evaluating for {'lmda': 9.461848194722002} ...\n",
      "The training loss is 8.843538479217644 with std:17.277455844689342. The val loss is 35.392927370716315 with std:228.83139046434218.\n",
      "35.392927370716315 9.461848194722002\n",
      "The training loss is 7.730194730692355 with std:14.947256503973925. The val loss is 16.385676098261115 with std:79.62736730259893.\n",
      "16.385676098261115 9.461848194722002\n",
      "The training loss is 9.502390102754209 with std:19.21232990559048. The val loss is 7.905209305313182 with std:11.171729594404232.\n",
      "7.905209305313182 9.461848194722002\n",
      "The training loss is 8.710828319453523 with std:14.79485222657943. The val loss is 12.389654174648426 with std:30.26843703944004.\n",
      "12.389654174648426 9.461848194722002\n",
      "Evaluating for {'lmda': 9.549485639791966} ...\n",
      "The training loss is 8.856258534193033 with std:17.3015814076261. The val loss is 35.54282210828332 with std:230.02381269684432.\n",
      "35.54282210828332 9.549485639791966\n",
      "The training loss is 7.741696517982241 with std:14.98180693150056. The val loss is 16.371253989114447 with std:79.4531882413327.\n",
      "16.371253989114447 9.549485639791966\n",
      "The training loss is 9.515243114584145 with std:19.241805051600576. The val loss is 7.914686955376948 with std:11.192864976190387.\n",
      "7.914686955376948 9.549485639791966\n",
      "The training loss is 8.723490134071403 with std:14.820109439343966. The val loss is 12.413482758460471 with std:30.334368157938535.\n",
      "12.413482758460471 9.549485639791966\n",
      "Evaluating for {'lmda': 9.637934799615786} ...\n",
      "The training loss is 8.869124388840365 with std:17.32588097508159. The val loss is 35.692826007792924 with std:231.21488672739275.\n",
      "35.692826007792924 9.637934799615786\n",
      "The training loss is 7.753363551312046 with std:15.016687336725322. The val loss is 16.356826061260062 with std:79.27801371044247.\n",
      "16.356826061260062 9.637934799615786\n",
      "The training loss is 9.528243834542696 with std:19.271481175024192. The val loss is 7.924327213287021 with std:11.214245447462213.\n",
      "7.924327213287021 9.637934799615786\n",
      "The training loss is 8.736305947956035 with std:14.845578398730542. The val loss is 12.437614050899985 with std:30.40087093306158.\n",
      "12.437614050899985 9.637934799615786\n",
      "Evaluating for {'lmda': 9.727203192450537} ...\n",
      "The training loss is 8.882138558425957 with std:17.350357087354908. The val loss is 35.842934095134744 with std:232.40453689793043.\n",
      "35.842934095134744 9.727203192450537\n",
      "The training loss is 7.7651985262959435 with std:15.051900918715877. The val loss is 16.342394836970364 with std:79.10184859399695.\n",
      "16.342394836970364 9.727203192450537\n",
      "The training loss is 9.541394870603806 with std:19.301360970595848. The val loss is 7.9341326571201 with std:11.235874054860778.\n",
      "7.9341326571201 9.727203192450537\n",
      "The training loss is 8.749278353505485 with std:14.87126184783404. The val loss is 12.4620521225223 with std:30.467950082835927.\n",
      "12.4620521225223 9.727203192450537\n",
      "Evaluating for {'lmda': 9.817298406188842} ...\n",
      "The training loss is 8.895303598405757 with std:17.375012316555665. The val loss is 35.993141459964825 with std:233.592687719037.\n",
      "35.993141459964825 9.817298406188842\n",
      "The training loss is 7.777204180418027 with std:15.087450898938576. The val loss is 16.327962883289285 with std:78.92469786369257.\n",
      "16.327962883289285 9.817298406188842\n",
      "The training loss is 9.554698872342197 with std:19.331447166191264. The val loss is 7.944105905680744 with std:11.25775387293392.\n",
      "7.944105905680744 9.817298406188842\n",
      "The training loss is 8.762409982942982 with std:14.897162561402311. The val loss is 12.48680109557002 with std:30.53561035677002.\n",
      "12.48680109557002 9.817298406188842\n",
      "Evaluating for {'lmda': 9.908228099003798} ...\n",
      "The training loss is 8.90862210502917 with std:17.399849266948255. The val loss is 36.14344325779384 with std:234.77926388281807.\n",
      "36.14344325779384 9.908228099003798\n",
      "The training loss is 7.789383293634894 with std:15.123340521246629. The val loss is 16.313532812612365 with std:78.74656657914205.\n",
      "16.313532812612365 9.908228099003798\n",
      "The training loss is 9.568158531543808 with std:19.361742523127656. The val loss is 7.954249619147204 with std:11.279888004065917.\n",
      "7.954249619147204 9.908228099003798\n",
      "The training loss is 8.775703508892803 with std:14.923283346141622. The val loss is 12.511865144584434 with std:30.603856535937087.\n",
      "12.511865144584434 9.908228099003798\n",
      "Evaluating for {'lmda': 10.0} ...\n",
      "The training loss is 8.922096715950502 with std:17.424870575296982. The val loss is 36.29383471207358 with std:235.96419027570957.\n",
      "36.29383471207358 10.0\n",
      "The training loss is 7.80173868898579 with std:15.159573051866222. The val loss is 16.29910728327267 with std:78.56745988815268.\n",
      "16.29910728327267 10.0\n",
      "The training loss is 9.581776582823615 with std:19.392249836463584. The val loss is 7.964566499725013 with std:11.302279578400439.\n",
      "7.964566499725013 10.0\n",
      "The training loss is 8.789161644963226 with std:14.949627041022014. The val loss is 12.537248497022308 with std:30.672693433054903.\n",
      "12.537248497022308 10.0\n"
     ]
    }
   ],
   "source": [
    "#list of lambda values to try.. use np.logspace\n",
    "search_lambda = np.logspace(-3,1,num=1000)\n",
    "\n",
    "params = {'lmda':search_lambda,}\n",
    "k_fold = 4\n",
    "from helper import grid_search_cv, fold_indices\n",
    "fold_ind = fold_indices(X_train.shape[0],k_fold)\n",
    "#call to the grid search function\n",
    "grid_val,grid_val_std = grid_search_cv(params,k_fold,fold_ind,do_cross_validation_reg,X_train_aug,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper import plot_cv_result\n",
    "plot_cv_result(np.log((grid_val.T)),search_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.829735791796951\n",
      "Best score achieved using lambda:0.12305240043592616\n"
     ]
    }
   ],
   "source": [
    "# best val score\n",
    "best_score = np.min(grid_val)\n",
    "print(best_score)\n",
    "\n",
    "# params which give best val score\n",
    "l= np.unravel_index(np.argmin(grid_val),grid_val.shape)\n",
    "# best_degree = search_degree[d]\n",
    "best_lambda = search_lambda[l]\n",
    "print('Best score achieved using lambda:{}'.format(best_lambda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 6.574523165737544 with std:12.363245731326806. The test loss is 8.295172688238726 with std:12.54941245145334.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.295172688238726"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate on the test set\n",
    "w = get_w_analytical_with_regularization(X_train_aug,y_train,best_lambda)\n",
    "\n",
    "get_loss(w,X_train_aug,y_train,X_test_aug,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: How can you interpret the linear regression coefficients?\n",
    "\n",
    "**A**: The coefficients value closer to zero means that corresponding features are unimportant for the model. Positive coefficient represent positive correlation with the value to be predicted and similar for negative case.\n",
    "\n",
    "**Q**: Is it good to have coefficients' values close to zero? \n",
    "\n",
    "**A**: Having coefficients close to zero means corresponding features are redundant for the model predictions. Hence we can represent our data with smaller feature sets. It might become important, when we might have several thousand features and prune out redundant ones, improving the interpretibility of the model.\n",
    "\n",
    "**Q**: How would you proceed to improve the prediction?\n",
    "\n",
    "**A**: Better feature expansion. One way would be to choose feature combinations which have high coefficient values. Choosing better hyperparameter. Till now we have done Grid Search, it would better to RandomSearch on the parameter space. Trying out different methods like Random Forest or Neural Nets. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
