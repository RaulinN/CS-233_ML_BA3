{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1 Introduction\n",
    "\n",
    "Welcome to the fifth practical session of CS233 - Introduction to Machine Learning.  \n",
    "In this exercise class we will start using Machine Learning methods to solve regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Regression Problem\n",
    "\n",
    "Let $f$ be a function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^v$ with a data set $\\left(X \\subseteq \\mathbb{R}^d, y\n",
    "\\subseteq\\mathbb{R}^v \\right )$. The regression problem is the task of estimating an approximation $\\hat{f}$ of $f$.\n",
    "Within this exercise we consider the special case of $v=1$, i.e. the problem is univariate as opposed to multivariate.\n",
    "Specifically, we will analyze the Boston house prices data set and predict costs based on properties such as per capita crime rate by town, pupil-teacher ratio by town etc.\n",
    "\n",
    "We will model the given data by means of a linear regression model, i.e. a model that explains a dependent variable in terms of a linear combination of independent variables.\n",
    "\n",
    "**Q.** How does a regression problem differ from a classification problem?  \n",
    "**Q.** Why is the linear regression model a linear model? Is it linear in the dependent variables? Is it linear in the parameters?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Load and inspect data\n",
    "\n",
    "For this exercise, we have Boston Housing dataset. The task is to predict the price of a house given a set of 12 [features](https://scikit-learn.org/stable/datasets/index.html#boston-dataset). Before jumping into the algorithm directly, it is good visualize some features and the price distribution. This step is called Data Exploration. We load the train and test data.\n",
    "\n",
    "**Q.** Explore the relation between different features and the house prices. Describe what you see. Can you identify any trends?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train    = np.load('X_train.npy')\n",
    "y_train    = np.load('y_train.npy')\n",
    "X_test     = np.load('X_test.npy')\n",
    "y_test     = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {X_train.shape[0]}')\n",
    "print(f'Number of test examples: {X_train.shape[0]}')\n",
    "print(f'Number of features: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of prices\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "plt.title(f\"Price Distribution for Train Set\")\n",
    "plt.hist(y_train)\n",
    "plt.xlabel(f\"Price Bins $y$\")\n",
    "plt.ylabel(f\"Occurances\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory analysis of the data. Have a look at the distribution of prices vs features\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.subplot(1,2,1)\n",
    "feature =  # choose different feature index\n",
    "plt.scatter(X_train[:,feature], y_train)\n",
    "plt.xlabel(f\"Attribute {feature}$\")\n",
    "plt.ylabel(\"Price $y$\")\n",
    "plt.title(f\"Attribute {feature} vs Price $y$\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "feature =  # choose different feature index\n",
    "plt.scatter(X_train[:,feature], y_train)\n",
    "plt.xlabel(f\"Attribute {feature}$\")\n",
    "plt.ylabel(\"Price $y$\")\n",
    "plt.title(f\"Attribute {feature} vs Price $y$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the data such that each feature has zero mean and unit standard deviation. Please fill in the required code and complete the function `normalize`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make mean 0 and std dev 1 of the data.\n",
    "'''\n",
    "def normalize(X,mean,std):\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\" \n",
    "    X  = \n",
    "    return X\n",
    "\n",
    "# normalize the data\n",
    "mean  = \n",
    "std   = \n",
    "norm_X_train = normalize(X_train,mean,std)\n",
    "norm_X_test = normalize(X_test,mean,std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Closed-form solution for linear regression\n",
    "\n",
    "We represent our output $\\mathbf{y} \\in R^{N\\times1}$ as linear combination of variables $\\mathbf{X} \\in R^{N\\times D}$. The objective of linear regression task is to find set of weights $\\mathbf{w} \\in R^{D\\times1}$ for predictor varibles which minimize our loss, simplest being $l_2$ loss function as shown below \n",
    "\\begin{align}\n",
    "L(\\mathbf{w}) &=\\frac{1}{N} \\| \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\|^2  \\\\\n",
    "\\nabla L(\\mathbf{w}) &= -\\frac{2}{N}\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) \n",
    "\\end{align}\n",
    "\n",
    "$\\nabla L(\\mathbf{w}) = 0$ for minimum condition, we get\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "This is called an [analytical solution](https://en.wikipedia.org/wiki/Linear_least_squares). \n",
    "Please use this solution to complete the function `get_w_analytical` and `get_loss`. Tip: before implementation think about the dimension of returned variable. \n",
    "\n",
    "**Q.** What is the time complexity of this approach?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_analytical(X_train,y_train):\n",
    "    \"\"\"\n",
    "    compute the weight parameters w\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "        \n",
    "    # compute w via the normal equation\n",
    "    # Tip: use np.linalg.solve instead of np.linalg.inv as it provides stable solution\n",
    "    w = \n",
    "    return w\n",
    "\n",
    "def get_loss(w, X_train, y_train,X_test,y_test,val=False):\n",
    "    # predict dependent variables and MSE loss for seen training data\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    loss_train = \n",
    "    loss_train_std = \n",
    "    \n",
    "    # predict dependent variables and MSE loss for unseen test data\n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    loss_test = \n",
    "    loss_test_std = \n",
    "    if not val:\n",
    "        print(\"The training loss is {} with std:{}. The test loss is {} with std:{}.\".format(loss_train, loss_train_std, loss_test,loss_test_std))\n",
    "    else:\n",
    "        print(\"The training loss is {} with std:{}. The val loss is {} with std:{}.\".format(loss_train, loss_train_std, loss_test,loss_test_std))\n",
    "\n",
    "    return loss_train, loss_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute w and calculate its goodness\n",
    "w_ana = get_w_analytical(norm_X_train,y_train)\n",
    "get_loss(w_ana, norm_X_train, y_train, norm_X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Adding $w_0$\n",
    "We add a bias term i.e. $w_0$ to our formulation such that $y_n = x_n * w_n + x_{n-1} * w_{n-1} + ... + x_1 * w_1 + 1 * w_0$ . This involves making our feature dimension from $D$ to $D+1$ and $\\mathbf{X} = [\\mathbf{1}  ~~~ \\mathbf{X} ]$, which corresponds to adding a column of ones.\n",
    "\n",
    "**Q.** How does this term help?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column of ones to X_train and X_test and see if loss values change.\n",
    "X_train_aug = \n",
    "X_test_aug = \n",
    "w_ana = get_w_analytical(X_train_aug,y_train)\n",
    "get_loss(w_ana, X_train_aug,y_train, X_test_aug,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Numerical solution for linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The linear regression model has an analytical solution, but we can also get the weight parameters $w$ numerically, e.g. via gradient descent. Please use this approach to complete the function `get_w_numerical` below.\n",
    "\n",
    "**Q.** How do these results compare against those of the analytical solution? Explain the differences or similarities!   \n",
    "**Q.** In which cases, it maybe be preferable to use the numerical approach over the analytical solution?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the gradient of loss function\n",
    "def gradient(w,X,y):\n",
    "    return \n",
    "\n",
    "# implement the numerical method to solve linear regression\n",
    "# max_iteration: number of times Gradient update happens\n",
    "# lr: learning rate \n",
    "def get_w_numerical(X_train,y_train,X_test,y_test,max_iteration,lr):\n",
    "    \"\"\"compute the weight parameters w\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the weights\n",
    "    w  = np.ones(X_train.shape[1])\n",
    "    \n",
    "    # iterate a given number of epochs over the training data\n",
    "    for iteration in range(max_iteration):\n",
    "        \n",
    "            \n",
    "        #calculate gradient \n",
    "        grad = gradient(w,X_train,y_train)\n",
    "\n",
    "        # update the weights\n",
    "        w =\n",
    "            \n",
    "        if iteration % 500 == 0:\n",
    "            print(f\"Iteration {500+iteration}/{max_iteration}\")\n",
    "            get_loss(w, X_train,y_train, X_test,y_test)\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute w and calculate its goodness, try different learning rate and see what happens\n",
    "\n",
    "w_num = get_w_numerical(X_train_aug,y_train,X_test_aug,y_test,6500,1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** What is the stopping criteria for this method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the lecture, we suffer from overfitting when the our model complexity increases. There are different ways to tackle this problem, like getting more data, changing the prediction method, regularization, etc. For the task of regression, we'll add a regularization to our training objective to mitigate this problem. Intutively, regularization restricts the domain from which the values of model parameters are taken, which means that we are biasing our model.  \n",
    "\n",
    "In Ridge Regression, we restrict the $l_2$ norm of the coefficients $\\mathbf{w}$. Our loss function looks as following,\n",
    "\\begin{align}\n",
    "L(\\mathbf{w}) &=\\frac{1}{N} \\| \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\|^2 + \\frac{\\lambda}{N}\\|\\mathbf{w}\\|^2 \\\\\n",
    "\\nabla L(\\mathbf{w}) &= -\\frac{2}{N}\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + 2\\frac{\\lambda}{N}\\mathbf{w}\n",
    "\\end{align}\n",
    "\n",
    "$\\nabla L(\\mathbf{w}) = 0$ for minimum condition, we get\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w} &= (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "dimensions are following: $\\mathbf{w}$ is $D\\times1$; $\\mathbf{y}$ is $N\\times1$; $\\mathbf{X}$ is $N\\times D$; $\\mathbf{I}$ is identity matrix of dimension $D \\times D$ .\n",
    "\n",
    "$\\lambda$ is our penality term, also know as weight decay. By varying its value, we can allow biasing in our model.\n",
    "\n",
    "**Q.**:\n",
    "When $\\lambda$ is high, our model is more complex or less?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we load same Boston Housing dataset with feature expansion strategy(will be covered in coming lectures) to get a complex model.  \n",
    "**Q.** What is the features dimension in the complex model?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the expanded features\n",
    "X_train_expanded = np.load('X_train_feat_augmentation.npy',)\n",
    "X_test_expanded = np.load('X_test_feat_augmentation.npy')\n",
    "\n",
    "# normalize the features\n",
    "mean  = \n",
    "std   = \n",
    "norm_X_train = normalize(X_train_expanded,mean,std)\n",
    "norm_X_test = normalize(X_test_expanded,mean,std)\n",
    "\n",
    "# add the ones column for bias-term\n",
    "X_train_aug = \n",
    "X_test_aug = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_analytical_with_regularization(X_train,y_train,lmda):\n",
    "    \"\"\"compute the weight parameters w with ridge regression\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Please fill in the required code here\n",
    "    \"\"\"\n",
    "    #create lambda matrix \n",
    "    lmda_mat = \n",
    "    # compute w via the normal equation\n",
    "    # np.linalg.solve is more stable than np.linalg.inv\n",
    "    w = \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_reg = get_w_analytical_with_regularization(X_train_aug,y_train,lmda=0)\n",
    "get_loss(w_reg,X_train_aug,y_train,X_test_aug,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Compare the above train and test losses w.r.t regression with simpler model?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.1 Cross Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation(CV) is used to choose value of $\\lambda$. As seen in previous exercise, we will use K-fold CV.\n",
    "We will use our training set and create K splits of it to choose best $\\lambda$ and finally evaluate on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data indices\n",
    "# num_examples: total samples in the dataset\n",
    "# k_fold: number fold of CV\n",
    "# returns: array of shuffled indices with shape (k_fold, num_examples//k_fold)\n",
    "def fold_indices(num_examples,k_fold):\n",
    "    ind = np.arange(num_examples)\n",
    "    split_size = num_examples//k_fold\n",
    "    \n",
    "    #important to shuffle your data\n",
    "    np.random.shuffle(ind)\n",
    "    \n",
    "    k_fold_indices = []\n",
    "    # Generate k_fold set of indices\n",
    "    k_fold_indices = \n",
    "    print(k_fold_indices)     \n",
    "    return np.array(k_fold_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for using kth split as validation set to get accuracy\n",
    "# and k-1 splits to train our model\n",
    "def do_cross_validation_reg(k,k_fold_ind,X,Y,lmda=0):\n",
    "    \n",
    "    # use one split as val\n",
    "    val_ind = k_fold_ind[k]\n",
    "    # use k-1 split to train\n",
    "    train_splits = [i for i in range(k_fold_ind.shape[0]) if i is not k]\n",
    "    train_ind = k_fold_ind[train_splits,:].reshape(-1)\n",
    "   \n",
    "    #Get train and val using train and val indices\n",
    "    cv_X_train = \n",
    "    cv_Y_train = \n",
    "    cv_X_val = \n",
    "    cv_Y_val =\n",
    "    \n",
    "    #fit on train set using regularised version\n",
    "    w = get_w_analytical_with_regularization(cv_X_train,cv_Y_train,lmda)\n",
    "    \n",
    "    #get loss for val\n",
    "    loss_train,loss_test = get_loss(w,cv_X_train,cv_Y_train,cv_X_val,cv_Y_val,val=True)\n",
    "    print(loss_test,lmda)\n",
    "    return loss_train,loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grid Search Function\n",
    "# params: hyperparameter to tune\n",
    "# k_fold: fold for CV to be done\n",
    "# fold_ind: splits of training set\n",
    "# X,Y: training examples\n",
    "# return: returns the training and validation loss for the range of hyperparamter\n",
    "\n",
    "def grid_search_cv(params,k_fold,fold_ind,function,X,Y):\n",
    "    \n",
    "    #save the values for the combination of hyperparameters\n",
    "    grid_train = np.zeros(len(params))\n",
    "    grid_val = np.zeros(len(params))\n",
    "       \n",
    "    for i, p in enumerate(params):\n",
    "        print('Evaluating for {} ...'.format(p))\n",
    "        loss_train = np.zeros(k_fold)\n",
    "        loss_test = np.zeros(k_fold)\n",
    "        for k in range(k_fold):\n",
    "            loss_train,loss_test[k] = function(k,fold_ind,X,Y,p)\n",
    "        grid_train[i] = \n",
    "        grid_val[i] = \n",
    "        \n",
    "    \n",
    "    return grid_train, grid_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do 4-fold CV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 4\n",
    "fold_ind = fold_indices(X_train_aug.shape[0],k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of lambda values to try.. use np.logspace\n",
    "minimum_pow =\n",
    "maximum_pow = \n",
    "search_lambda = np.logspace(minimum_pow,maximum_pow,num=5000)\n",
    "\n",
    "#call to the grid search function\n",
    "grid_train,grid_val = grid_search_cv(search_lambda,k_fold,fold_ind,do_cross_validation_reg,X_train_aug,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the curves for losses\n",
    "search_lambda = [round(s,4) for s in search_lambda]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(grid_train)\n",
    "plt.xticks(np.arange(0,len(search_lambda),500), search_lambda[::500])\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Train loss')\n",
    "plt.title('Train Loss for different lambda')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(grid_val)\n",
    "plt.xticks(np.arange(0,len(search_lambda),500), search_lambda[::500])\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Val loss')\n",
    "plt.title('Val Loss for different lambda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Look at the above plots and what can you conclude about model complexity as we increase lambda?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best val score\n",
    "best_score = \n",
    "print(best_score)\n",
    "\n",
    "# params which give best val score\n",
    "l= \n",
    "# best_degree = search_degree[d]\n",
    "best_lambda = search_lambda[l]\n",
    "print('Best score achieved using lambda:{}'.format(best_lambda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate on the test set\n",
    "w = get_w_analytical_with_regularization(X_train_aug,y_train,best_lambda)\n",
    "\n",
    "get_loss(w,X_train_aug,y_train,X_test_aug,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**: Compare this value with before Cross Validation?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: How would you proceed to improve the prediction?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
