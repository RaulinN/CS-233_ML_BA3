{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Session 7 -  Support Vector Machine (SVM)\n",
    "\n",
    "Welcome to the 7th excersie session of CS233 - Introduction to Machine Learning.  \n",
    "\n",
    "We will use Scikit-learn, a Python package of machine learning methods, in this exercise. We are going to start with a toy binary classification example to understand Linear SVM, then to address more difficult problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Support Vector Machine (SVM)\n",
    "SVM tries to solve linear classification problem of the **primal form**:  \n",
    "    \\begin{align}\n",
    "        \\underset{\\tilde{\\mathbf{w}},w_0}{\\operatorname{min}}  \\ \\ & \\frac{1}{2}\\|\\tilde{\\mathbf{w}}\\|^2 + C \\sum^N_{i=1}\\zeta_i \\\\\n",
    "        \\operatorname{subject \\  to} \\ \\ &  y_i(\\tilde{\\mathbf{w}}^T\\mathbf{x_i}+w_0) \\geq 1-\\zeta_i , \\forall \\  i \\\\\n",
    "                          &  \\zeta_i \\geq 0 , \\forall \\  i\n",
    "    \\end{align}\n",
    "where, $\\tilde{\\mathbf{w}}$,$w_0$ are weight and bias. C is penalty term, $\\zeta_n$ is error in terms of how far data point is beyond correct margin and $y_i \\in\\{-1,1\\}$ for binary classification. $\\|\\tilde{\\mathbf{w}}\\|$ is inversely related to margin width, so minimizing it means maximizing the margin, hence we minimize $\\|\\tilde{\\mathbf{w}}\\|$. As our data may not be linearly separable, hence maximizing margin will lead to some misclassifications. $\\zeta_i$ is greater than zero when a data point is beyond margin and how many such data points are allowed is controlled by C. We choose the right value for C, given the data, through validation set. Hence with this objective function we get a maximum margin with certain amount of misclassification.\n",
    "\n",
    "The corresponding **dual problem** is given by:\n",
    "\\begin{align}\n",
    "    \\underset{\\{\\alpha_i\\}}{\\operatorname{max}} \\ \\ \n",
    "    & \\sum_{i=1}^N \\alpha_i - \\frac 1 2 \\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i\\alpha_jy_iy_j\\mathbf{x}_i^T\\mathbf{x}_j  \\\\   \n",
    "    \\operatorname{subject \\ to} & \\ \\ \\sum_{i=1}^N \\alpha_iy_i = 0 \\\\\n",
    "                 & \\ \\ 0 \\leq \\alpha_i \\leq C, \\forall i \\ \\ \n",
    "\\end{align}\n",
    "**Question**\n",
    "   * How can you write $\\tilde{\\mathbf{w}}$ using $\\alpha_i$s? This relates primal and dual coefficents.\n",
    "   * How is $y(\\mathbf{x})$ represented using $\\alpha_i$s?\n",
    " \n",
    "**Answer**\n",
    "   * $\\tilde{\\mathbf{w}} = \\sum_{i=1}^N \\alpha_iy_i\\mathbf{x_i} $\n",
    "   * We plugging the $\\tilde{\\mathbf{w}}$ as, \n",
    "     \\begin{align}\n",
    "       \\hat{y}(\\mathbf{x}) &= \\tilde{\\mathbf{w}}^T\\mathbf{x} + w_0 \\\\\n",
    "                           &= \\sum_{i=1}^N \\alpha_iy_i\\mathbf{x}_i^T\\mathbf{x} + w_0\n",
    "     \\end{align}\n",
    "   * The sum can be computed on the support vectors ($\\delta$) only, \n",
    "       \\begin{align}\n",
    "       \\hat{y}(\\mathbf{x}) & = \\sum_{i \\in \\delta} \\alpha_iy_i\\mathbf{x}_i^T\\mathbf{x} + w_0\n",
    "     \\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Scikit-Learn\n",
    "\n",
    "Training a SVM classifer is not a easy task, so in this session, we are going to use Scikit-Learn, which is a machine learning library written in python. Most of the machine learning algorithms and tools are already implemented. In this exercise, we'll use this package to train and understand SVM. If you are interested in how to optimize a SVM, you can refer to [this](https://xavierbourretsicotte.github.io/SVM_implementation.html).\n",
    "\n",
    "Please install scikit-learn in your conda environment by following instructions at this link:https://scikit-learn.org/stable/install.html if you don't have it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn has modules implemented broadly for \n",
    "- Data Transformations: https://scikit-learn.org/stable/data_transforms.html\n",
    "- Model Selection and Training: https://scikit-learn.org/stable/model_selection.html\n",
    "- Supervised Techniques: https://scikit-learn.org/stable/supervised_learning.html\n",
    "- Unsupervised Techniques: https://scikit-learn.org/stable/unsupervised_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the magic happens under the hood, but gives you freedom to try out more complicated stuff.  \n",
    "Different methods to be noted here are\n",
    "- `fit`: Train a model with the data\n",
    "- `predict`: Use the model to predict on test data\n",
    "- `score`: Return mean accuracy on the given test data\n",
    "\n",
    "Have a look at [this](https://scikit-learn.org/stable/tutorial/basic/tutorial.html#learning-and-predicting) for simple example.\n",
    "\n",
    "We will explore linear SVM in this session: [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) with linear kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Binary Classification\n",
    "\n",
    "Let's begin with a simple **binary** classification using Linear SVM.\n",
    "The data is simplely **linear** separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple data\n",
    "from plots import plot_simple_data\n",
    "x = np.array([[2,4],[1,4],[2,3],[6,-1],[7,-1],[5,-3]] )\n",
    "y = np.array([-1,-1, -1, 1, 1 , 1 ])\n",
    "plot_simple_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Linear SVM\n",
    "In this part, you are asked to build a SVM classifier using SVC and to understand the outputs from the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let use SVC with linear kernel\n",
    "from sklearn.svm import SVC\n",
    "from plots import plot\n",
    "\n",
    "\n",
    "# 1. Declare a SVC with C=1.0 and kernel='linear'\n",
    "## CODE HERE\n",
    "clf = ...\n",
    "\n",
    "# 2. use x and y to fit the model\n",
    "## CODE HERE\n",
    " \n",
    "\n",
    "# 3. show the fitted model\n",
    "plot(x, y, clf)\n",
    "\n",
    "print('w = ',clf.coef_)\n",
    "print('w0 = ',clf.intercept_)\n",
    "print('Number of support vectors for each class = ', clf.n_support_)\n",
    "print('Support vectors = ', clf.support_vectors_)\n",
    "print('Indices of support vectors = ', clf.support_)\n",
    "print('a (Coefficients of the support vector in the decision function) = ', clf.dual_coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the weights (w) from the fitted model to predict the labels of input data points\n",
    "\n",
    "def raw_predict(x, w, w0):\n",
    "    '''\n",
    "    given input data, w and w0, output the prediction result\n",
    "    \n",
    "    input:\n",
    "    x: data, np.array of shape (N, D) where N is the number of datapoints and D is the dimension of features.\n",
    "    w: weights, np.array of shape (N,)\n",
    "    w0: bias, np.array of shape (1,)\n",
    "    \n",
    "    output:\n",
    "    out: predictions, np.array of shape (N, ). tip: .astype(int) \n",
    "    '''\n",
    "    ## CODE HERE\n",
    "    \n",
    "    \n",
    "    return out\n",
    "\n",
    "x_test = np.array([\n",
    "    [4, 2],\n",
    "    [ 6, -3]])\n",
    "\n",
    "#output the predictions on x_test\n",
    "## CODE HERE\n",
    "raw_pred = ...\n",
    "\n",
    "print(\"Prediction from the model: \", clf.predict(x_test))\n",
    "print(\"Prediction from your implementation: \", raw_pred)\n",
    "assert(raw_pred.all() == clf.predict(x_test).all())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the indices of support vectors by using w and w0\n",
    "\n",
    "desicion_function_from_model = clf.decision_function(x) # this function outputs the results of wx+w0\n",
    "\n",
    "## we can also calculate the decision function manually\n",
    "## CODE HERE\n",
    "decision_function = ...\n",
    "\n",
    "assert(desicion_function_from_model.all() == decision_function.all())\n",
    "\n",
    "## according to the condition that support vectors should satisfy\n",
    "## CODE HERE tips: use np.where to put the condition in.\n",
    "support_vector_indices = ...\n",
    "\n",
    "print('I find the indices of support vectors = ', support_vector_indices)\n",
    "assert(support_vector_indices.all() == clf.support_.all())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Dual Coefficients VS Primal Coefficients\n",
    "\n",
    "By using `dual_coef_` attribute of the model, we can get the dual coefficients $\\alpha_i$ of the support vectors.  \n",
    "**Question** Scikit returns dual coeff in slightly different form, can you identify the difference?\n",
    "\n",
    "**Answer** Dual coefficients $a_n$ must satisfy constraint $0\\leq \\alpha_i \\leq C$.  \n",
    "Support vector which lies on margin has $\\alpha_i<C$ and ones between margins have $a_i=C$\n",
    "\n",
    "Scikit return coefficients with label of the class {-1,1}, i.e. it returns $a_iy_i$, where $y_i \\in \\{1,-1\\}$. Also, the coefficients are only of support vectors.\n",
    "\n",
    "Given support vectors ($\\delta$) and their dual_coefficients, the weights can be computed by:\n",
    "\\begin{align}\n",
    "\\tilde{\\mathbf{w}} & = \\sum_{i \\in \\delta}^N \\alpha_iy_i\\mathbf{x_i} \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute primal coefficients given dual coefficients and support vectors\n",
    "\n",
    "def compute_w(dual_coef, support_vectors):\n",
    "    '''\n",
    "    given dual coefficients and support_vectors, compute the primal coefficients\n",
    "    \n",
    "    input:\n",
    "    dual_coef: dual coefficients, np.array of shape (1, n) where n is the number of support vectors.\n",
    "    support_vectors: np.array of shape (n, D) where n is the number of support vectors and D is the dimension of features.\n",
    "    w0: bias, np.array of shape (1,)\n",
    "    \n",
    "    output:\n",
    "    w: primal coefficients, np.array of shape (D, )\n",
    "    '''\n",
    "    ## CODE HERE\n",
    "    w = \n",
    "    \n",
    "    return w\n",
    "\n",
    "\n",
    "w = compute_w(clf.dual_coef_, clf.support_vectors_)\n",
    "\n",
    "print(\"Primal coefficients from the model: \", clf.coef_[0])\n",
    "print(\"Primal coefficients from your implementation: \", w)\n",
    "\n",
    "assert(w.all() == clf.coef_[0].all())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Different C values\n",
    "Let's try different values of C. In the code, vary the C value from 0.001 to 100 and notice the changes on a bigger dataset.  \n",
    "**Question**: How does the margin vary with C? **Hint**: have a look at the optimization formulation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from helpers import get_simple_dataset\n",
    "from plots import plot\n",
    "\n",
    "# Get the simple dataset\n",
    "X, Y = get_simple_dataset()\n",
    "plot(X,Y,None,dataOnly=True)\n",
    "\n",
    "#Declare a SVM model with linear kernel and C=1.0\n",
    "clf = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "#call the fit method\n",
    "clf.fit(X, Y)\n",
    "\n",
    "#plot the decision boundary\n",
    "plot(X, Y, clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the decision boundary and margins of the learnt model. Encircled points are the support vectors.  \n",
    "WARNING: if the margins go beyound the limits of axis, they are not shown or shown close to decision plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary C and plot the boundaries\n",
    "# Use np.logspace to generate 6 c values from (10e-3, 10e2) \n",
    "## CODE HERE \n",
    "\n",
    "C = ...\n",
    "\n",
    "for ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Lower C allows more misclassification and hence larger margin, while bigger C reduces misclassfication and hence smaller margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Reading (if interested)\n",
    "- Multiclass SVM (Bishop- Multiclass SVMs 7.1.3)\n",
    "- Can we have probabilistic interpretation of SVM? (Bishop- Relevance Support Machine 7.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
