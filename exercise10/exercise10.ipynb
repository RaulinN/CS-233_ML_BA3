{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise Session 10 - Kernel Methods\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "Welcome to the 10th practical session of CS233 - Introduction to Machine Learning.  \n",
    "\n",
    "We will continue using scikit-learn, a Python package of machine learning methods.  \n",
    "\n",
    "Use it to train SVM with different kernel functions and learn how tune their parameters.\n",
    "\n",
    "We will also capture the kernel PCA as told in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from plots import plot\n",
    "from sklearn import svm \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Recap: Scikit-Learn\n",
    "Scikit-Learn is a machine learning library written in python. Most of the machine learning algorithms and tools are already implemented for you to use. In this exercise, we'll use this package to train SVM. In order to install scikit-learn follow instructions at this link:https://scikit-learn.org/stable/install.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn has modules implemented broadly for \n",
    "- Data Transformations: https://scikit-learn.org/stable/data_transforms.html\n",
    "- Model Selection and Training: https://scikit-learn.org/stable/model_selection.html\n",
    "- Supervised Techniques: https://scikit-learn.org/stable/supervised_learning.html\n",
    "- Unsupervised Techniques: https://scikit-learn.org/stable/unsupervised_learning.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the magic happens under the hood, but gives you freedom to try out more complicated stuff.  \n",
    "Different methods to be noted here are\n",
    "- `fit`: Train a model with the data\n",
    "- `predict`: Use the model to predict on test data\n",
    "- `score`: Return mean accuracy on the given test data\n",
    "\n",
    "Have a look at [this](https://scikit-learn.org/stable/tutorial/basic/tutorial.html#learning-and-predicting) for simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the linear problem we discussed before, SVM can also solve non-linear classification problem by using kernel functions. We replace $\\mathbf{x}_i$ with $\\phi(\\mathbf{x}_i)$, and then $\\mathbf{x}_i^T\\mathbf{x}_j$ with $k(\\mathbf{x}_i,\\mathbf{x}_j)$. The **dual form** of this problem is given by:  \n",
    "\\begin{align}\n",
    "    \\underset{\\{\\alpha_i\\}}{\\operatorname{max}} \\ \\ \n",
    "    & \\sum_{i=1}^N \\alpha_i - \\frac 1 2 \\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i\\alpha_jy_iy_jk(\\mathbf{x}_i,\\mathbf{x}_j)  \\\\   \n",
    "    \\operatorname{subject \\ to} & \\ \\ \\sum_{i=1}^N \\alpha_iy_i = 0 \\\\\n",
    "                 & \\ \\ 0 \\leq \\alpha_i \\leq C, \\forall i \\ \\ \n",
    "\\end{align}\n",
    "**Question**\n",
    "   * How can you write $\\tilde{\\mathbf{w}}$ using $\\alpha_i$s and function $\\phi$?\n",
    "   * How is $y(\\mathbf{x})$ represented using $\\alpha_i$s?\n",
    " \n",
    "**Answer**\n",
    "   * $\\tilde{\\mathbf{w}} = \\sum_{i=1}^N \\alpha_iy_ik(\\mathbf{x_i},\\mathbf{x}) $\n",
    "   * We plugging the $\\tilde{\\mathbf{w}}$ as, \n",
    "     \\begin{align}\n",
    "       \\hat{y}(\\mathbf{x}) &= \\tilde{\\mathbf{w}}^T\\phi(\\mathbf{x}) + w_0 \\\\\n",
    "                           &= \\sum_{i=1}^N \\alpha_iy_ik(\\mathbf{x_i},\\mathbf{x}) + w_0\n",
    "     \\end{align}\n",
    "   * The sum can be computed on the support vectors ($\\delta$) only, \n",
    "       \\begin{align}\n",
    "       \\hat{y}(\\mathbf{x}) & = \\sum_{i \\in \\delta} \\alpha_iy_ik(\\mathbf{x_i},\\mathbf{x}) + w_0\n",
    "     \\end{align}\n",
    "   \n",
    "Have a look at the SVM function [here.](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) The main parameters you should look for are:\n",
    "- Kernel Function: Linear, Polynomial and RBF\n",
    "- Penalty term: C \n",
    "- Gamma: for RBF and Polynomial kernel\n",
    "- Degree: for polynomial kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Linear Classification\n",
    "\n",
    "Recap from exercise 7, let's try with simple **linear classification** using SVM, but with different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_simple_dataset\n",
    "\n",
    "# Get the simple dataset\n",
    "X_linear, Y_linear = get_simple_dataset()\n",
    "plot(X_linear, Y_linear, None, dataOnly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Linear Kernel\n",
    "Firstly, we use linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVM with Linear kernel where C=0.1, \n",
    "# and you can also try some other C values and see what will happen\n",
    "clf = svm.SVC(kernel='linear', C=0.1)\n",
    "\n",
    "# Call the fit method\n",
    "clf.fit(X_linear, Y_linear)\n",
    "\n",
    "# Plot the decision boundary and support vectors\n",
    "plot(X_linear, Y_linear, clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the decision boundary and margins of the learnt model. Encircled points are the support vectors.  \n",
    "WARNING: if the margins go beyound the limits of axis, they are not shown or shown close to decision plane. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Polynominal and RBF Kernels \n",
    "We will try out Polynomial and RBF kernels. Have a look at the type of decision boundary created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVM with Polynomial kernel where C=0.1, degree=4, \n",
    "# and you can also try some other C and degree values and see what will happen\n",
    "\n",
    "### CODE HERE ###\n",
    "clf = \n",
    "\n",
    "clf.fit(X_linear, Y_linear)\n",
    "plot(X_linear, Y_linear, clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVM with RBF kernel where C=0.1, gamma=0.1 , \n",
    "# and you can also try some other C and gamma values and see what will happen\n",
    "\n",
    "### CODE HERE ###\n",
    "clf = \n",
    "\n",
    "clf.fit(X_linear, Y_linear)\n",
    "plot(X_linear, Y_linear, clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Non-Linear Classification\n",
    "Util now we have worked with linear data in the given feature space. Most of the time that's not the case. We'll use non linear data and use non linear kernels to do classification. These kernels project the data to higher dimension and which help find out the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_circle_dataset\n",
    "\n",
    "X,Y = get_circle_dataset()\n",
    "plot(X,Y,None,dataOnly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVM with Polynomial Kernel of different degrees\n",
    "D = 2 ** np.linspace(0, 6, num=7)\n",
    "for d in D:\n",
    "    ## use poly kernel with C=10. and different degrees\n",
    "    ### CODE HERE ###\n",
    "    clf = \n",
    "    \n",
    "    clf.fit(X, Y)\n",
    "    plot(X, Y, clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RBF Kernel with differen gammas\n",
    "G = np.logspace(-2,2,num=5)\n",
    "for g in G:\n",
    "    ## use rbf kernel with C=0.1 and different gammas\n",
    "    ### CODE HERE ###\n",
    "    clf = \n",
    "    \n",
    "    clf.fit(X, Y)\n",
    "    plot(X, Y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!!** It is important to choose the approperiate parameters in the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Look at the given dataset again, which will be the best non-linear kernel in your mind? If you choose polynominal kernel, which degree will be the best? If you choose RBF kernel, which gamma will be the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Let's use Grid Search and Cross Validation techniques to find out the answer. We can apply K-Fold cross validation to different sets of parameters. We make use of the mean of K obtained accuracies to figure out the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the K-Fold cross validation for RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold cross validation for searching parameters of RBF kernel.\n",
    "from helpers import do_cross_validation, fold_indices\n",
    "\n",
    "# seach in log space\n",
    "grid_search_c = np.logspace(-4, 10, num=15)\n",
    "grid_search_gamma = np.logspace(-9, 5, num=15)\n",
    "\n",
    "#save the accuracies for the combination of hyperparameters\n",
    "grid_val = np.zeros((len(grid_search_c), len(grid_search_gamma)))\n",
    "\n",
    "# Do 4 fold cross validation\n",
    "k_fold = 4\n",
    "k_fold_ind = fold_indices(X.shape[0], k_fold)\n",
    "\n",
    "for i, c in enumerate(grid_search_c):\n",
    "    for j, g in enumerate(grid_search_gamma):\n",
    "        print('Evaluating for C:{} gamma:{} ...'.format(c, g))\n",
    "        \n",
    "        ## call SVM with c,g as params.\n",
    "        ### CODE HERE ####\n",
    "        clf = \n",
    "        \n",
    "        acc = np.zeros(k_fold)\n",
    "        ## do cross validation\n",
    "        for k in range(k_fold):\n",
    "            acc[k] = do_cross_validation(clf, k, k_fold_ind, X, Y)\n",
    "            \n",
    "        ## fill out the grid_val by computing the mean accuracy from k_fold runs.\n",
    "        ### CODE HERE ####\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import plot_cv_result_rbf\n",
    "## show all results and the best one\n",
    "plot_cv_result_rbf(grid_val,grid_search_c,grid_search_gamma)\n",
    "print('Best acc:{}'.format(np.max(grid_val)))\n",
    "## best params\n",
    "cin,gin = np.unravel_index(np.argmax(grid_val),grid_val.shape)\n",
    "print('Best Params- C:{}, Gamma:{}'.format(grid_search_c[cin],grid_search_gamma[gin]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above heatmap shows accuracies for different Gamma and C values. The best params are used on test set.   \n",
    "**Question** Is there a relation between C and Gamma?   \n",
    "**Hint**: Think how increase in one value changes other. Look at the heatmap to get the idea. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: High Gamma will lead to overfitting, hence smaller C would more misclassification to counteract. Lower Gamma is underfitting and hence high C is to stop misclassification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do the K-Fold cross validation for polynominal kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold cross validation for searching parameters of Polynominal kernel.\n",
    "from helpers import do_cross_validation, fold_indices\n",
    "\n",
    "grid_search_c = np.logspace(-5,5,num=11)\n",
    "grid_search_degree = 2 ** np.linspace(0, 8, num=9)\n",
    "\n",
    "#save the accuracies for the combination of hyperparameters\n",
    "grid_val = np.zeros((len(grid_search_c),len(grid_search_degree)))\n",
    "\n",
    "# Do 4 fold cross validation\n",
    "k_fold = 4\n",
    "k_fold_ind = fold_indices(X.shape[0],k_fold)\n",
    "\n",
    "for i, c in enumerate(grid_search_c):\n",
    "    for j, d in enumerate(grid_search_degree):\n",
    "        print('Evaluating for C:{} degree:{} ...'.format(c, d))\n",
    "        \n",
    "        ## call SVM with c,d as params.\n",
    "        ### CODE HERE ####\n",
    "        clf = \n",
    "        \n",
    "        acc = np.zeros(k_fold)\n",
    "        # do cross validation\n",
    "        for k in range(k_fold):\n",
    "            acc[k] = do_cross_validation(clf,k,k_fold_ind,X,Y)\n",
    "            \n",
    "        ## fill out the grid_val by computing the mean accuracy from k_fold runs. \n",
    "        ### CODE HERE ####\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import plot_cv_result_poly\n",
    "## show all results and the best one\n",
    "plot_cv_result_poly(grid_val, grid_search_c, grid_search_degree)\n",
    "print('Best acc:{}'.format(np.max(grid_val)))\n",
    "## best params\n",
    "cin,gin = np.unravel_index(np.argmax(grid_val),grid_val.shape)\n",
    "print('Best Params- C:{}, Degree:{}'.format(grid_search_c[cin],grid_search_degree[gin]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4  Kernel PCA\n",
    "In this section, we will cover the kernel PCA, in order to extend PCA to kernel PCA, we first map the data to high-dimensional space\n",
    "\n",
    "   \\begin{align}\n",
    "   \\mathbf{x}_i &\\rightarrow \\phi(\\mathbf{x}_i)\n",
    "   \\end{align}\n",
    "   \n",
    "Then we perform PCA on $\\phi(\\mathbf{x}_i)$ instead of $\\mathbf{x}_i$, our goal will be to replace dot products with kernel values\n",
    "\n",
    "   \\begin{align}\n",
    "   k(\\mathbf{x}_i, \\mathbf{x}_j) &= \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j)\n",
    "   \\end{align}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "np.random.seed(0)\n",
    "X, y = make_circles(n_samples=400, factor=.3, noise=.05)\n",
    "\n",
    "# Plot the dataset\n",
    "plt.figure()\n",
    "plt.title(\"Original space\")\n",
    "reds = y == 0\n",
    "blues = y == 1\n",
    "\n",
    "plt.scatter(X[reds, 0], X[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(X[blues, 0], X[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's use the normal PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions are exactly the same as previous exercises\n",
    "from helpers import PCA\n",
    "\n",
    "# choose the top-2 principal components\n",
    "d = 2\n",
    "mean, W, eg, X_hat, exvar = PCA(X, d)\n",
    "\n",
    "# Plot the results \n",
    "plt.figure()\n",
    "plt.title(\"Projection by PCA\")\n",
    "reds = y == 0\n",
    "blues = y == 1\n",
    "\n",
    "plt.scatter(X_hat[reds, 0], X_hat[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(X_hat[blues, 0], X_hat[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it is still not linearly separable for PCA projection in two deminsions, now, let's try kernel PCA with RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel PCA function\n",
    "'''\n",
    "Input:\n",
    "    K: NxN matrix representing kernel matrix\n",
    "    d: Number of principal components to be used to reduce dimensionality\n",
    "    \n",
    "Output:\n",
    "    W: Nxd principal components\n",
    "    eg: d values representing variance corresponding to principal components\n",
    "    C_hat: Nxd data projected in principal components' direction\n",
    "    exvar: explained variance by principal components\n",
    "'''\n",
    "def KPCA(K, d):\n",
    "    \n",
    "    # Compute eigenvector and eigenvalues. Hint use: np.linalg.eigh\n",
    "    eigvals, eigvecs = np.linalg.eigh(K)\n",
    "    # Choose top d eigenvalues and corresponding eigenvectors. Sort eigenvalues( with corresponding eigenvector )\n",
    "    # in decreasing order first.\n",
    "    eigvals = eigvals[::-1]\n",
    "    eigvecs = eigvecs[:, ::-1]\n",
    "\n",
    "    W = eigvecs[:, 0:d]\n",
    "    eg = eigvals[0:d]\n",
    "\n",
    "    # project the data using W \n",
    "    K_hat = K@W\n",
    "    \n",
    "    #explained variance\n",
    "    exvar = eg.sum()/eigvals.sum()\n",
    "\n",
    "    return W, eg, K_hat, exvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we utilise Kernel PCA, we should center the data $\\phi(\\mathbf{x}_i)$ which is after mapping original data $\\mathbf{x}_i$ to feature space by kernel function $\\phi$. Let's define:\n",
    "\\begin{align}\n",
    "   \\tilde \\phi(\\mathbf{x}_i) &= \\phi(\\mathbf{x}_i) -  \\frac{1}{N} \\sum_{i=1}^N \\phi(\\mathbf{x}_i)\n",
    "   \\end{align}\n",
    "   \n",
    "Then the $(i, j)^{th}$ elements of the corresponding kernel matraix is given by \n",
    "\n",
    "   \\begin{align}\n",
    "   \\mathbf{\\tilde K}_{i,j} &= \\tilde \\phi(\\mathbf{x}_i)^T \\tilde \\phi(\\mathbf{x}_j)\n",
    "   \\end{align}\n",
    "   \n",
    "This kernel matrix can be computed based on the original one:\n",
    "\n",
    "\\begin{align}\n",
    "   \\mathbf{\\tilde K} &= \\mathbf{K} - \\mathbf{1}_N \\mathbf{K} - \\mathbf{K} \\mathbf{1}_N + \\mathbf{1}_N \\mathbf{K} \\mathbf{1}_N\n",
    "  \\end{align}\n",
    " \n",
    "where $\\mathbf{1}_N$ is an $N \\times N $ matrix with every elemjent equal to $1/N$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions are exactly the same as previous exercises\n",
    "from helpers import kernel_rbf, kernel_matrix\n",
    "\n",
    "sigma = 0.3\n",
    "\n",
    "## compute original RBF kernel matrix\n",
    "### CODE HERE ###\n",
    "K_RBF =\n",
    "\n",
    "## define the 1_N normal matrix\n",
    "### CODE HERE ###\n",
    "normal_matrix = \n",
    "\n",
    "## compute the centered RBF kernel matrix \n",
    "### CODE HERE ###\n",
    "K_RBF_center =\n",
    "\n",
    "# use the first and second principal component obtained by kernel PCA with RBF kernel\n",
    "d = 2\n",
    "\n",
    "## call the KPCA to do the kernel PCA w.r.t centered RBF kernel matrix \n",
    "### CODE HERE ###\n",
    "W_RBF_PCA, eg_RBF_PCA, K_RBF_PCA, exvar_RBF_PCA = \n",
    "\n",
    "# Visualize the kernel PCA result\n",
    "plt.figure()\n",
    "plt.title(\"Projection by kernel PCA\")\n",
    "reds = y == 0\n",
    "blues = y == 1\n",
    "\n",
    "plt.scatter(K_RBF_PCA[reds, 0], K_RBF_PCA[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(K_RBF_PCA[blues, 0], K_RBF_PCA[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is linearly separable now :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
