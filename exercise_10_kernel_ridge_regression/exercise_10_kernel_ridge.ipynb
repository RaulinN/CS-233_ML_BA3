{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cf067296-24f3-4e3c-bd05-618d7fc1d164"
    }
   },
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Welcome to the fifth practical session of CS233 - Introduction to Machine Learning.  \n",
    "In this exercise class, we will work with higher-dimensional feature spaces and kernel ridge regression.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "3b86f801-06b7-4daf-9b87-b7a0a6eee24b"
    }
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as skd\n",
    "import sklearn.metrics.pairwise as skmp\n",
    "import scipy\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4c058526-d27d-4e36-93bb-8a78322f76a7"
    }
   },
   "source": [
    "In this week's lecture we have heard of the benefits of higher-dimensional feature spaces for linear classifiers. To obtain higher-dimensional data one may  \n",
    "* directly map observations into a higher-dimensional feature space or \n",
    "* make use of a kernel term that can be utilized to compute a dot product in a latent higher-dimensional space\n",
    "\n",
    "Within this exercise we will encounter both techniques and kernelize the previously encountered ridge regression. We will also practice how to compute our kernels from the feature expansion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will be working with the Boston Housing dataset, from exercise 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Boston houinsg prices dataset and exclude the categorical feature 'CHAS'\n",
    "data_boston = skd.load_boston()\n",
    "X = data_boston['data']  # (506, 13)\n",
    "X = np.concatenate([X[:, :3], X[:, 4:]], axis=1)  # (506, 12)\n",
    "Y = data_boston['target']  # (506, )\n",
    "\n",
    "assert X.shape == (506, 12)\n",
    "assert Y.shape == (506, )\n",
    "\n",
    "N = X.shape[0]\n",
    "split = 0.8\n",
    "\n",
    "rinds = np.random.permutation(N)\n",
    "X_tr = X[rinds[:int(N * split)]]\n",
    "y_tr = Y[rinds[:int(N * split)]]\n",
    "X_te = X[rinds[int(N * split):]]\n",
    "y_te = Y[rinds[int(N * split):]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ridge Regression with Feature Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in Ridge Regression, we restrict the $l_2$ norm of the coefficients $\\mathbf{w}$. Our loss function looks as following,\n",
    "\\begin{align}\n",
    "L(\\mathbf{w}) &=\\frac{1}{N} \\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\rVert^2 + \\frac{\\lambda}{N}\\lVert\\mathbf{w}\\rVert^2 \n",
    "\\end{align}\n",
    "\n",
    "We find the closed form solution for the weights vector as:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w} &= (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "dimensions are following: $\\mathbf{w}$ is $D\\times1$; $\\mathbf{y}$ is $N\\times1$; $\\mathbf{X}$ is $N\\times D$; $\\mathbf{I}$ is identity matrix of dimension $D \\times D$ .\n",
    "\n",
    "\n",
    "\n",
    "We would like to do feature expansion on our data $\\mathbf{X}$ by applying the function $\\phi(\\cdot)$. Now the loss function becomes:\n",
    "\n",
    "\\begin{align}\n",
    "L(\\mathbf{w}) &=\\frac{1}{N} \\| \\mathbf{y} - \\mathbf{\\Phi}\\mathbf{w} \\|^2 + \\frac{\\lambda}{N}\\|\\mathbf{w}\\|^2 \n",
    "\\end{align}\n",
    "\n",
    "And our new weights vector is:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w} &= (\\mathbf{\\Phi}^T\\mathbf{\\Phi}+\\lambda\\mathbf{I})^{-1}\\mathbf{\\Phi}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "We already coded the analytical solution for the weights previously as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_analytical(data_tr, y_tr, lmda):\n",
    "    \"\"\" Computes the weight parameters w\n",
    "    \n",
    "        Args:\n",
    "        data_tr (np.array): Training data, shape(N, D).\n",
    "        y_tr (np.array): Training labels, shape (N, ).\n",
    "        lmda (float): Lambda hyperparameter of ridge regression.\n",
    "    \n",
    "    Returns:\n",
    "        (np.array): weights, shape (D, ).\n",
    "    \"\"\"\n",
    "    # compute w via the normal equation\n",
    "    # np.linalg.solve is more stable than np.linalg.inv\n",
    "    lmda_mat = lmda*np.eye(data_tr.shape[1])\n",
    "    w = np.linalg.solve(data_tr.T@data_tr+lmda_mat,data_tr.T@y_tr)\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the MSE loss is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_mse(y_gt, y_pred):\n",
    "    \"\"\" Computes the mean square error (MSE) of the GT and predicted values.\n",
    "    \n",
    "    Args:\n",
    "        y_gt (np.array): GT values, shape (N, ).\n",
    "        y_pred (np.array): Predicted values, shape (N, ).\n",
    "    \n",
    "    Returns:\n",
    "        float: MSE loss.\n",
    "    \"\"\"\n",
    "    return np.mean((y_gt - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to first normalize our data, then do polynomial feature expension on it. Fill in the function `expand_X()`. You should add a bias term, but omit the interaction terms. An example:\n",
    "\n",
    "For $D=2$, $\\text{degree_of_expansion}=3$ you have:\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}^{(0)}& \\mathbf{x}^{(1)}\\end{bmatrix}\n",
    "$$\n",
    "After the polynomial feature expansion, you would like to have:\n",
    "$$ \n",
    "\\mathbf{\\Phi} = \\begin{bmatrix}\\mathbf{1} & \\mathbf{x}^{(0)} & \\mathbf{x}^{(1)} & (\\mathbf{x}^{(0)})^2 & (\\mathbf{x}^{(1)})^2 & (\\mathbf{x}^{(0)})^3 & (\\mathbf{x}^{(1)})^3\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_X(X, mean, std):\n",
    "    '''\n",
    "    Make mean 0 and std dev 1 of the data.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): data, shape (N, D).\n",
    "        mean (np.array): Mean of samples per feature. shape(D,)\n",
    "        std (np.array): Std of samples per feature. shape(D,)\n",
    "    \n",
    "    Returns:\n",
    "        (np.array): Normalized data with shape (N, D).\n",
    "    '''\n",
    "    \n",
    "    normalized_X = (X-mean)/std\n",
    "    return normalized_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_X(X, degree_of_expansion):\n",
    "    \"\"\"  Perform degree-d polynomial feature expansion of X, \n",
    "        with bias but omitting interaction terms\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): data, shape (N, D).\n",
    "        degree_of_expansion (int): The degree of the polynomial feature expansion.\n",
    "    \n",
    "    Returns:\n",
    "        (np.array): Expanded data with shape (N, new_D), \n",
    "                    where new_D is D*degree_of_expansion+1\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "    ## Your code here\n",
    "    expanded_X = ...\n",
    "    return expanded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data has 3 features.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-28842ba1093f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The original data has {} features.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_toy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"After degree-{} polynomial feature expansion (with bias, without interaction terms) the data has {} features.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree_of_expansion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexpanded_X_toy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# perform polynomial feature expansion\n",
    "degree_of_expansion = 2\n",
    "\n",
    "#normalize the data after expansion\n",
    "X_toy = np.arange(15).reshape((5,3))\n",
    "mean = np.mean(X_toy, axis=0, keepdims=True)\n",
    "std = np.std(X_toy, axis=0, keepdims=True)\n",
    "X_toy_norm = normalize_X(X_toy, mean, std)\n",
    "expanded_X_toy = expand_X(X_toy_norm, degree_of_expansion)\n",
    "\n",
    "print(\"The original data has {} features.\".format(X_toy.shape[1]))\n",
    "print(\"After degree-{} polynomial feature expansion (with bias, without interaction terms) the data has {} features.\".format(degree_of_expansion,expanded_X_toy.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose best degree we'll use Cross Validation. We're going to K-Fold CV for that. We will use our training set and create K splits of it to choose best degree and finally evaluate on our test set. Fill in the `do_cross_validation()` function below to normalize then expand the data. Don't forget to normalize according to the statistics of the training split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cross_validation(k, k_fold_ind, X, Y, degree_of_expansion=1, lmda=0.1):\n",
    "    # use one split as val\n",
    "    val_ind = k_fold_ind[k]\n",
    "    # use k-1 split to train\n",
    "    train_splits = [i for i in range(k_fold_ind.shape[0]) if i is not k]\n",
    "    train_ind = k_fold_ind[train_splits,:].reshape(-1)\n",
    "    \n",
    "    #Get train and val\n",
    "    cv_X_tr = X[train_ind,:]\n",
    "    cv_Y_tr = Y[train_ind]\n",
    "    cv_X_val = X[val_ind,:]\n",
    "    cv_Y_val = Y[val_ind]\n",
    "    \n",
    "    #expand and normalize for degree d. \n",
    "    ##### Your code here!    \n",
    "    cv_X_tr_poly = ...\n",
    "    cv_X_val_poly = ...\n",
    "    \n",
    "    #fit on train set\n",
    "    w =  ... \n",
    "    #predict on validation set\n",
    "    pred = ... \n",
    "    #####\n",
    "    \n",
    "    #get loss for val\n",
    "    loss_test = metric_mse(pred, cv_Y_val)\n",
    "    return loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `grid_search_cv()` function is already written for you and it will return the validation scores in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import fold_indices, grid_search_cv\n",
    "\n",
    "#list of lambda values to try.. use np.logspace\n",
    "search_lambda = np.logspace(-2,1,num=10)\n",
    "#list of degrees\n",
    "search_degree = np.arange(1,15,1)\n",
    "\n",
    "params = {'degree_of_expansion':search_degree,'lmda':search_lambda,}\n",
    "\n",
    "k_fold = 3\n",
    "fold_ind = fold_indices(X_tr.shape[0],k_fold)\n",
    "#call to the grid search function\n",
    "grid_val, _ = grid_search_cv(params,k_fold,fold_ind,do_cross_validation,X_tr,y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the cell below to find the best validation score, the best degree for polynomial feature expansion and the best lambda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the best validation score\n",
    "best_score = np.min(grid_val)\n",
    "print('Best val score {}'.format(best_score))\n",
    "\n",
    "#get degree and lambda pair which gives best score from grid_val\n",
    "##### Your code here\n",
    "best_degree = ...\n",
    "best_lambda = ...\n",
    "print('Best score achieved using degree:{} and lambda:{}'.format(best_degree,best_lambda))\n",
    "#####\n",
    "\n",
    "from helper import plot_cv_result\n",
    "plot_cv_result(np.log((grid_val.T)),search_lambda,search_degree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, evaluate your result on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Evaluate on the test set\n",
    "mean = np.mean(X_tr, axis=0, keepdims=True)\n",
    "std = np.std(X_te, axis=0, keepdims=True)\n",
    "\n",
    "norm_X_tr = normalize_X(X_tr, mean, std)\n",
    "norm_X_te = normalize_X(X_te, mean, std)\n",
    "\n",
    "poly_X_tr = expand_X(norm_X_tr, degree_of_expansion)\n",
    "poly_X_te = expand_X(norm_X_te, degree_of_expansion)\n",
    "\n",
    "w = get_w_analytical(poly_X_tr, y_tr, best_lambda)\n",
    "pred = poly_X_te@w\n",
    "test_loss = metric_mse(pred, y_te)\n",
    "\n",
    "print(\"Test loss is:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing feature expansion, we can also do kernel ridge regression, where we precompute the dot product between samples in our training set and save it as our kernel, i.e.\n",
    "$$ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)\\cdot\\phi(\\mathbf{x}_i)^T $$\n",
    "\n",
    "Recall that, using the Woodbury identity (slide 49), we transformed the formula \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w} &= (\\mathbf{\\Phi}^T\\mathbf{\\Phi}+\\lambda\\mathbf{I})^{-1}\\mathbf{\\Phi}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "into\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w} &= \\mathbf{\\Phi}^T(\\mathbf{\\Phi}\\mathbf{\\Phi}^T+\\lambda\\mathbf{I})^{-1}\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "and now we can replace $\\mathbf{\\Phi}\\mathbf{\\Phi}^T$ with kernel $\\mathbf{K}$,\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w}^* &= \\mathbf{\\Phi}^T(\\mathbf{K}+\\lambda\\mathbf{I})^{-1}\\mathbf{y}\n",
    "\\end{align}.\n",
    "\n",
    "\n",
    "Note that since we don't want our solution to be dependent on $\\mathbf{\\Phi}$, we never end up computing the weights $\\mathbf{w}^*$. Instead we compute directly the inference result.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y_i} &= (\\mathbf{w}^*)^T\\phi(\\mathbf{x}_i) \\\\\n",
    "\\hat{y_i} &= \\mathbf{y}^T(\\mathbf{K}+\\lambda\\mathbf{I})^{-1}\\mathbf{\\Phi}\\phi(\\mathbf{x}_i)\n",
    "\\end{align}.\n",
    "\n",
    "And $\\mathbf{\\Phi}\\phi(\\mathbf{x}_i)$ is also the result of the kernel function between test sample $\\mathbf{x}_i$ and all the training samples $\\mathbf{X}$. We denote it as $k(\\mathbf{X}, \\mathbf{x}_i)$.\n",
    "\n",
    "Therefore our prediction of test sample $\\mathbf{x}_i$ is:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y_i} &= \\mathbf{y}^T(\\mathbf{K}+\\lambda\\mathbf{I})^{-1}k(\\mathbf{X}, \\mathbf{x}_i)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**:\n",
    "\n",
    "This is known as the kernel trick. What does the kernel trick imply in terms of computational runtime? When is it (not) beneficial?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Kernel Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we ask you to code the following kernels:\n",
    "\n",
    "Linear kernel:\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i\\cdot\\mathbf{x}_j)\n",
    "$$\n",
    "Polynomial kernel:\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i\\cdot\\mathbf{x}_j+1)^d\n",
    "$$\n",
    "RBF Kernel:\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|{\\bf x}_i - {\\bf x}_j\\|^2}{2\\sigma^2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `kernel_polynomial()`  and  `kernel_rbf()` we ask you to write your own tests, by comparing your results with the results of `sklearn.metrics.pairwise`'s kernel functions (we import it as `skmp`). For `kernel_linear()` we provide the test, so you can use it as a guide to write the remaining two tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_linear(xi, xj):\n",
    "    \"\"\" Computes the linear kernel function for the input vectors `xi`, `xj`.\n",
    "    \n",
    "    Args:\n",
    "        xi (np.array): Input vector, shape (D, ).\n",
    "        xj (np.array): Input vector, shape (D, ).\n",
    "        \n",
    "    Returns:\n",
    "        float: Result of the kernel function.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    return ...\n",
    "    ######################\n",
    "\n",
    "\n",
    "# Test your implementation.\n",
    "D = 10\n",
    "x1 = np.random.uniform(-1., 1., (1, D)).astype(np.float32)\n",
    "x2 = np.random.uniform(-1., 1., (1, D)).astype(np.float32)\n",
    "\n",
    "y_sk = skmp.linear_kernel(x1, x2)[0, 0]\n",
    "y_our = kernel_linear(x1[0], x2[0])\n",
    "assert np.allclose(y_sk, y_our)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_polynomial(xi, xj, d):\n",
    "    \"\"\" Computes the polynomial kernel function for the input vectors `xi`, `xj`.\n",
    "    \n",
    "    Args:\n",
    "        xi (np.array): Input vector, shape (D, ).\n",
    "        xj (np.array): Input vector, shape (D, ).\n",
    "        \n",
    "    Returns:\n",
    "        float: Result of the kernel function.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    return ...\n",
    "    ######################\n",
    "\n",
    "# Test your implementation.\n",
    "D = 10\n",
    "d = 5\n",
    "\n",
    "x1 = np.random.uniform(-1., 1., (1, D)).astype(np.float32)\n",
    "x2 = np.random.uniform(-1., 1., (1, D)).astype(np.float32)\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "y_sk = ...\n",
    "y_our = ...\n",
    "assert np.allclose(y_sk, y_our)\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_rbf(xi, xj, sigma):\n",
    "    \"\"\" Computes the RBF kernel function for the input vectors `xi`, `xj`.\n",
    "    \n",
    "    Args:\n",
    "        xi (np.array): Input vector, shape (D, ).\n",
    "        xj (np.array): Input vector, shape (D, ).\n",
    "        \n",
    "    Returns:\n",
    "        float: Result of the kernel function.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    return ...\n",
    "    ######################\n",
    "\n",
    "# Test your implementation.\n",
    "D = 10\n",
    "sigma = 0.321\n",
    "\n",
    "x1 = np.random.uniform(-1., 1., (1, D)).astype(np.float32)\n",
    "x2 = np.random.uniform(-1., 1., (1, D)).astype(np.float32)\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "y_sk = ...\n",
    "y_our = ...\n",
    "assert np.allclose(y_sk, y_our)\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Kernel Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `kernel_matrix()` below, which computes the entire kernel matrix of shape $(N\\times N)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_matrix(X, kernel_func, *kernel_func_args):\n",
    "    \"\"\" Computes the kernel matrix for data `X` using kernel \n",
    "    function `kernel_func`.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Data matrix with data samples of dimension D in the \n",
    "            rows, shape (N, D).\n",
    "        kernel_func (callable): Kernel function.\n",
    "        kernel_func_args: Arguments of the `kernel_func` to be called.\n",
    "            \n",
    "    Returns:\n",
    "        np.array: Kernel matrix of the shape (N, N).\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    \n",
    "    K = np.zeros((N, N), dtype=np.float64)\n",
    "    ### YOUR CODE HERE ###\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            ...\n",
    "    ######################\n",
    "    assert np.allclose(K, K.T)\n",
    "    return K\n",
    " \n",
    "# Test your implementation.\n",
    "N = 5\n",
    "D = 3\n",
    "\n",
    "X = np.random.uniform(-1., 1., (N, D)).astype(np.float64)\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "# Test using the any kernel function you like\n",
    "km_sk = ...\n",
    "km_our = ...\n",
    "assert np.allclose(km_sk, km_our)\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that inference is done as\n",
    "\\begin{align}\n",
    "\\hat{y_i} &= \\mathbf{y}^T(\\mathbf{K}+\\lambda\\mathbf{I})^{-1}k(\\mathbf{X}, \\mathbf{x}_i)\n",
    "\\end{align}.\n",
    "\n",
    "The quantity of $\\mathbf{y}^T(\\mathbf{K}+\\lambda\\mathbf{I})^{-1}$ is only dependent on the training set. It can be precomputed and stored, and used later during prediction. Let's fill in the function `y_times_K()` that does exactly this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_times_K(y, K, lamb):\n",
    "    \"\"\" Computes the expression y^T(K + lambda * In)^-1 used in a kernel ridge regression\n",
    "    prediction function.\n",
    "    \n",
    "    Args:\n",
    "        y (np.array): GT outputs, shape (N, ). \n",
    "        K (np.array): Kernel matrix, shape (N, N).\n",
    "        lamb (float): Regularization parameter. \n",
    "        \n",
    "    Returns:\n",
    "        np.array: Result of shape (N, ).\n",
    "    \"\"\"\n",
    "    N = y.shape[0]\n",
    "    ### YOUR CODE HERE ###\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "What does the quantity $\\mathbf{y}^T(\\mathbf{K}+\\lambda\\mathbf{I})^{-1}$ mean in the dual problem? (Hint: refer to slide 56).\n",
    "* They are the dual parameters $\\alpha^*$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Prediction Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will fill in the function to compute $k(\\mathbf{X}, \\mathbf{x}_i)$, which is the result of the kernel function between the whole training set and one test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_kernel_sample(X, x, kernel_func, *kernel_func_args):\n",
    "    \"\"\" Evaluates the kernel function `kernel_func` for input sample\n",
    "    `x` on the whole dataset `X`. I.e. computes the term k(X, x).\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Dataset, shape (N, D).\n",
    "        x (np.array): Input sample, shape (D, ).\n",
    "        kernel_func (callable): Kernel function.\n",
    "        kernel_func_args: Arguments of the `kernel_func` to be called.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Result of shape (N, 1).\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    kXx =  ...\n",
    "    for i in range(N):\n",
    "        ....\n",
    "    \n",
    "    return kXx\n",
    "    ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us put everything together in the function `predict_kernel_ridge()`. Now, we will use `y_times_K()` and the function `eval_kernel_sample()` to do prediction on a test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_kernel_ridge(X, y, x, lamb, kernel_func, *kernel_func_args, yK=None):\n",
    "    \"\"\" Prediction function for kernel ridge regression. Given\n",
    "    dataset `X`, GT output `y` and kernel function `kernel_func`\n",
    "    it predicts the output for input sample `x`.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Dataset, shape (N, D).\n",
    "        y (np.array): GT outputs, shape (N, ).\n",
    "        x (np.array): Input sample, shape (D, ).\n",
    "        lamb (float): Regularization parameter.\n",
    "        kernel_func (callable): Kernel function.\n",
    "        kernel_func_args: Arguments of the `kernel_func` to be called.\n",
    "        yK (np.array): Constant term y^T(K + lambda * In)^-1, shape (N, ).\n",
    "    \n",
    "    Returns:\n",
    "        float: Predicted output.\n",
    "    \"\"\"    \n",
    "    if yK is None:\n",
    "        K = kernel_matrix(X, kernel_func, *kernel_func_args)\n",
    "        yK = y_times_K(y, K, lamb)\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    return ...\n",
    "    ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `predict_kernel_ridge()`, fill in the function below to do inference on the entire test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(X_tr, y_tr, X_te, lamb, kernel_func, *kernel_func_args):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    N_te = X_te.shape[0]\n",
    "\n",
    "    K = kernel_matrix(X_tr, kernel_func, *kernel_func_args)\n",
    "    yK = y_times_K(y_tr, K, lamb)\n",
    "    \n",
    "    y_pred = np.zeros((N_te, ), dtype=np.float64)\n",
    "    for i in range(N_te):\n",
    "        ### YOUR CODE HERE ###\n",
    "        y_pred[i] = ...\n",
    "        ######################\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Linear kernel.\n",
    "lamb = 100\n",
    "\n",
    "y_pred_lin = predict_all(X_tr, y_tr, X_te, lamb, kernel_linear)\n",
    "mse_lin = metric_mse(y_te, y_pred_lin)\n",
    "\n",
    "print('MSE for linear kernel: {:.3f}'.format(mse_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Polynomial kernel.\n",
    "lamb = 2\n",
    "d = 2\n",
    "\n",
    "y_pred_poly = predict_all(X_tr, y_tr, X_te, lamb, kernel_polynomial, d)\n",
    "mse_poly = metric_mse(y_te, y_pred_poly)\n",
    "\n",
    "print('MSE for polynomial kernel: {:.3f}'.format(mse_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RBF kernel.\n",
    "lamb = 0.1\n",
    "sigma = 50\n",
    "\n",
    "y_pred_rbf = predict_all(X_tr, y_tr, X_te, lamb, kernel_rbf, sigma)\n",
    "mse_rbf = metric_mse(y_te, y_pred_rbf)\n",
    "\n",
    "print('MSE for RBF kernel: {:.3f}'.format(mse_rbf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6e9b8461-792a-4485-9973-5291db440adf"
    }
   },
   "source": [
    "# 4. XOR problem\n",
    "\n",
    "Finally, let us try to construct a kernel from the feature expansion function to solve the XOR problem. Let us define the following toy data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1d55f373-aa78-4ad6-9506-d901ed303b5f"
    }
   },
   "outputs": [],
   "source": [
    "def simData():\n",
    "    \"\"\"construct a XOR problem toy data set\"\"\"\n",
    "    X = np.array([[0,0,1,1],[1,0,1,0]]).T\n",
    "    y = np.array([1,0,0,1])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f07b2284-4aef-4a75-8cfb-d5c355bbb1fd"
    }
   },
   "source": [
    "**Question:**\n",
    "* Does training a logistic regression on this data set converge? Why (not)?\n",
    "    \n",
    "Use the provided function `plot3Dscatter` to plot the given data set.  \n",
    "* Consider the function $\\phi(\\mathbf{x_1}): {\\rm I\\!R}^d \\xrightarrow{} {\\rm I\\!R}^m, \\left[ x_{11},x_{12}\\right] \\xrightarrow{} \\left[ x_{11}^2, x_{12}^2, \\sqrt{2} x_{11} x_{12} \\right]$ for $\\mathbf{x} \\in X$ and $d=2,m=3$. Write a function `transform_data` that transforms the given data set accordingly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f09ec7d0-57b9-48a8-ba09-522d56698811"
    }
   },
   "source": [
    "Please fill in the function `transform_data` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "ccd59626-765c-490a-b705-5fdaec121826"
    }
   },
   "outputs": [],
   "source": [
    "from plots import plot3Dscatter\n",
    "\n",
    "def transform_data(X):\n",
    "    \"\"\"transform the given data set\"\"\"\n",
    "    \n",
    "    transformed_X = X.copy()\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    transformed_X = ...\n",
    "    ######################\n",
    "    return transformed_X\n",
    "\n",
    "X, y = simData()\n",
    "plot3Dscatter(X)\n",
    "plot3Dscatter(transform_data(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e8333706-8d1a-4d53-9c4f-20151e0a936e"
    }
   },
   "source": [
    "**Question:**  \n",
    "* Show that the kernel $k(\\mathbf{x_1},\\mathbf{x_2}) = (\\mathbf{x_1}^T\\mathbf{x_2})^2$ implicitly specifies the feature transformation $\\phi(\\mathbf{x_1}) = \\left[ x_{11}^2, x_{12}^2, \\sqrt{2} x_{11} x_{12} \\right]$ of section 3.1.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
