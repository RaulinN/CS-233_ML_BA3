{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded Exercise 1\n",
    "\n",
    "**Date: 25.10.2019**\n",
    "\n",
    "Welcome to the first graded exercise. In this exercise, you will be tested on three topics you have learned so far: K-means clustering, KNN, and linear regression, along with some general machine learning practices. \n",
    "\n",
    "You are asked to fill in the code in a couple of cells throughout the exercise. For each such cell we provided tests which are run along with the cell and save your results to a file. The cells are independent of each other and you will receive points for each individual cell. The tests immediately show you whether your code is correct.\n",
    "\n",
    "Before you finish, please make sure to **upload two files to Moodle**:\n",
    "* **graded_exercise_1.ipynb**\n",
    "* **answers_SCIPER.npz (e.g. \"answers_280595.npz\")**\n",
    "\n",
    "Good luck! :-)\n",
    "\n",
    "<br>\n",
    "\n",
    "**PLEASE ENTER YOUR SCIPER NUMBER IN THE CELL BELOW.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your sciper number here\n",
    "sciper_number = 123456  # e.g. 123456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change the seed, we keep the same seed to evaluate the result.\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from matplotlib.image import imread\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers.helper import KMEANSHelper\n",
    "\n",
    "# Unit tests.\n",
    "import tests.tests as tests\n",
    "\n",
    "# set matplotlib to display all plots inline with the notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 $K$-means Clustering (6 pts)\n",
    "\n",
    "In this part, you are asked to understand and complete the K-means algorithm including important concepts you have learned from the lecture and exercise. \n",
    "\n",
    "In this graded session, you will apply the K-means algorithm to compress an image. In this case, given a RGB image, you can cluster all pixels into $K$ clusters so that the original image can be compressed into an image with only $K$ colors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Initialization  (1 pt)\n",
    "First, we will initialize the data centers randomly. Fill in the function `init_centers` below. The indices are already shuffled for you. Select the **first K indices** from the shuffled indices and use them to select the centers from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_centers(data, K):\n",
    "    \"\"\"\n",
    "    Randomly pick K data samples (i.e. pixels) from the input image as starting points \n",
    "    for centers.\n",
    "    \n",
    "    input: \n",
    "        data: ndarray of shape (N, d) where N is the number of pixels, d is number of features.\n",
    "        K: int, the number of clusters.\n",
    "    output:\n",
    "        centers: ndarray of shape (K, d). Initial cluster centers.        \n",
    "    \"\"\"    \n",
    "    np.random.seed(seed)\n",
    "    random_idx = np.random.permutation(data.shape[0])\n",
    "    \n",
    "    # please select the first K indices from the random_idx and use these indices to select centers from data\n",
    "    ## >>> YOUR CODE HERE\n",
    "    centers = ...\n",
    "       \n",
    "    return centers\n",
    "\n",
    "tests.test_kmeans_init_centers(locals())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Computing Distance (3 pts)\n",
    "\n",
    "Please fill in the function `compute_distance()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def compute_distance(data, centers, K):\n",
    "    \"\"\"\n",
    "    Compute the euclidean distance between each datapoint and each center.\n",
    "    \n",
    "    input:    \n",
    "        data: ndarray of shape (N, d) where N is the number of pixels, d is number of features.\n",
    "        centers: ndarray of shape (K, d). Centers of K clusters.\n",
    "        K: number of clusters.\n",
    "        \n",
    "    output:\n",
    "        distance: ndarray of shape (N, K).\n",
    "    \"\"\"\n",
    "    distance = np.zeros((data.shape[0], K))\n",
    "    for k in range(K):\n",
    "        \n",
    "        # please compute the euclidean distances between each pixel and each center\n",
    "        ## >>> YOUR CODE HERE\n",
    "        ...\n",
    "        \n",
    "    return distance\n",
    "\n",
    "tests.test_kmeans_compute_distance(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Perform K-Means (2 pts)\n",
    "\n",
    "Find the closest cluster to your data samples using the distances you just computed. Fill in the `find_closest_cluster()` function.\n",
    "\n",
    "You can fill in this function even if you haven't completed `complete_distance()` yet, as they are graded individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_cluster(distance):\n",
    "    \"\"\"\n",
    "    Assign cluster labels to pixels according to minimum input distance.\n",
    "    \n",
    "    input:\n",
    "        distance: ndarray of shape (N, K). \n",
    "\n",
    "    output:\n",
    "        labels: ndarray of shape (N,) where each value means the cluster label that is assigned to the pixel.\n",
    "    \"\"\"\n",
    "    \n",
    "    # please assign labels to all datapoints \n",
    "    ## >>> YOUR CODE HERE\n",
    "    labels = ...\n",
    "    return labels\n",
    "\n",
    "tests.test_kmeans_find_closest_cluster(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are done with Part 1! Just run the cells below and see the compressed image result.\n",
    "\n",
    "You can now move on to Part 2: K-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmean(data, K, max_iter):\n",
    "    \"\"\"\n",
    "    Main function that combines all the former functions together to build the K-means algorithm.\n",
    "    \n",
    "    Input: \n",
    "        data: ndarray of shape (N, d) where N is the number of pixels, d is number of features.\n",
    "        K: int, the number of clusters.\n",
    "    \n",
    "    output:\n",
    "        center: ndarray of shape (N, d). Final cluster centers.\n",
    "        labels: ndarray of shape (N,) where each value means the cluster label that assigned to the data.\n",
    "    \"\"\"\n",
    "    centers = init_centers(data, K)\n",
    "    for i in range(max_iter):\n",
    "        old_centers = centers\n",
    "        distance = compute_distance(data, old_centers, K)\n",
    "        labels = find_closest_cluster(distance)\n",
    "        centers = KMEANSHelper.compute_centers(data, labels, K)\n",
    "        if np.all(old_centers == centers):\n",
    "            break\n",
    "    return centers, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see how K-Means works for compressing image\n",
    "img = imread('road.jpg') # jpg\n",
    "img_size = img.shape\n",
    "# print ('The shape of input image is: ', img_size)\n",
    "\n",
    "# Reshape it to be 2-dimensio\n",
    "X = img.reshape(img_size[0] * img_size[1], img_size[2])\n",
    "# Run the Kmeans algorithm\n",
    "K = 10\n",
    "centers, labels = kmean(X, K, 100)\n",
    "# print(centers.shape, labels.shape)\n",
    "\n",
    "# Use the centroids to compress the image, clip it to image range\n",
    "X_compressed = centers[labels]\n",
    "X_compressed = np.clip(X_compressed.astype('uint8'), 0, 255)\n",
    "\n",
    "# Reshape X_recovered to have the same dimension as the original image 300 * 400 * 3\n",
    "X_compressed = X_compressed.reshape(img_size[0], img_size[1], img_size[2])\n",
    "\n",
    "# Plot the original and the compressed image next to each other\n",
    "fig, ax = plt.subplots(1, 2, figsize = (12, 8))\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title('Original Image')\n",
    "ax[1].imshow(X_compressed)\n",
    "ax[1].set_title('Compressed Image with %d colors'%(K))\n",
    "for ax in fig.axes:\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. kNN (9 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Weighted $k$-Nearest Neighbors Classifier (5 pts)\n",
    "\n",
    "\n",
    "\n",
    "Traditional $k$-NN assigns as label to a given example the most popular label from its surroundings. The method is very intuitive, and can be summarized as:\n",
    "- Compute the distance between the example to classify and all the training examples.\n",
    "- Select the closest $k$ training examples.\n",
    "- Assign to the example the most common label among those neighbors.\n",
    "\n",
    "However, in the **weighted k-NN**, instead of majority vote of the nearest neighbor labels, we assign a weight to each nearest neighbor that is inversely proportional to its distance from query point. Then to predict the label of the query point, the weights of the neighbors belonging to same label is summed up. The label with maximum sum is declared as query's label.\n",
    "\n",
    "Formally, for a query point $x_{q}$, $(x_{i},y_{i}) \\in \\text{neighborhood}_{k}(x_q)$ for i =1,2...k are k-nearest neighbours with label $y_i \\in [0,M-1]$ where $M$ is number of labels/classes , then the predicted label $y_{q}$ with weighted k-NN is given by\n",
    "\n",
    "\\begin{align}\n",
    "        \\text{scores}(v) &=  \\sum_{(x_i,y_i)\\in \\text{neighborhood}_{k}(x_{q})}  w_{i} \\times I(v,y_i)  \\\\\n",
    "        w_{i} &= \\frac{1.0}{\\text{distance}(x_{q},x_{i})}\\\\\n",
    "        y_q &= \\underset{v}{\\operatorname{argmax}} \\text{scores}(v) \n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "$$\n",
    "I(v,y_i) = \\begin{cases} 1, v=y_{i} \\\\ 0, v \\neq y_i \\end{cases} \\\\\n",
    "v \\in [0, M-1]\n",
    "$$\n",
    "\n",
    "**You are asked to implement the function below to predict label of a query point given the k- nearest neighbour labels and distances from them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label_with_weighted_distance(neighbor_labels, neighbor_distances, num_classes):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      neighbor_labels : labels of k-nearest neighbours with shape:(N,)\n",
    "      neighbor_distances: distances of k-nearest neighbours from query point with shape:(N,)\n",
    "      num-classes: num of classes/labels in the dataset, a scalar value\n",
    "\n",
    "    Output:\n",
    "      predicted_label: label of the query point predicted using weighted k-NN algorithm, a scalar value \n",
    "      w: weights of the neighbors with shape:(N,)\n",
    "    \"\"\"\n",
    "\n",
    "    # save the final sum of distances for each class/label\n",
    "    scores = np.zeros(num_classes, dtype=np.float32)  \n",
    "    # save the weight of each neighbor\n",
    "    w = np.zeros(len(neighbor_labels), dtype=np.float32)\n",
    "\n",
    "    for  j in range(len(neighbor_labels)):   \n",
    "        ## >>> YOUR CODE HERE\n",
    "        w[j] = ...\n",
    "        scores[neighbor_labels[j]] = ...   \n",
    "    \n",
    "    predicted_label = np.argmax(scores)\n",
    "    return predicted_label, w\n",
    "\n",
    "# Test your implementation.\n",
    "tests.test_knn_weighted(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Leave-one-out-cross-validation (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold is a type of CV technique that we'll see here. We'll split data in K parts and use kth part for validation, whereas k-1 for training. This process will be repeated K times.\n",
    "\n",
    "Leave-one-out-cross-validation (LOOCV) which is special case of k-fold CV. Please complete the below cell to find the number of folds in LOOCV for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def num_folds_LOOCV(train_data):\n",
    "\n",
    "    \"\"\"\n",
    "    train_data : size (NxD), N data points each of dimension D\n",
    "\n",
    "    Output:\n",
    "        num_folds :  return number of folds for Leave-one-out-cross-validation\n",
    "\n",
    "    \"\"\"\n",
    "    num_folds = ...\n",
    "    return num_folds\n",
    "\n",
    "# Test your implementation.\n",
    "tests.test_num_folds_LOOCV(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Linear Regression (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Min-max normalization (2 pts)\n",
    "\n",
    "Write the `min-max normalization()` function that takes a dataset, a max value and a min value. Also fill in the function `find_min_max_values()` that lets you find the minimum value and maximum value of each **feature**.\n",
    "\n",
    "Let's recall how the min max normalization works. We have a dataset of $N$ samples and $D$ features. Feature $d$ (where $d=1,...,D$) of data sample $x_i$ (where $i=1,...,N$) is denoted as $x_i^{(d)}$. The normalized feature $d$ of $x_i$ is denoted as $\\tilde{x}^{(d)}_i$.\n",
    "\n",
    "The min-max normalization formula is:\n",
    "$$\\tilde{x}^{(d)}_i = \\frac{x^{(d)}_i - x^{(d)}_{min}}{x^{(d)}_{max}-x^{(d)}_{min}}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$ x^{(d)}_{min} = \\min_{i=1}^N x^{(d)}_i \\\\\n",
    "x^{(d)}_{max} = \\max_{i=1}^N x^{(d)}_i \n",
    "$$\n",
    "\n",
    "Keep in mind that your data does not have a bias feature, so you do not have to account for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalization(dataset, min_value, max_value):\n",
    "    \"\"\" Normalizes the dataset by linearly scaling its features\n",
    "    to range [0, 1].\n",
    "    \n",
    "    Args:\n",
    "        dataset (np.array): Dataset, shape (N, D), N is number of \n",
    "            data samples, D is number of features.\n",
    "        min_value (np.array): Per-feature min value, shape (D, ).\n",
    "        max_value (np.array): Per-feature max value, shape (D, ).\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Normalized dataset, shape (N, D).\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ds_normalized = ...\n",
    "    return ds_normalized\n",
    "    ###\n",
    "    \n",
    "def find_min_max_values(dataset):\n",
    "    \"\"\" Finds the minimum and maximum value for each feature.\n",
    "    \n",
    "    Args:\n",
    "        dataset (np.array): Dataset, shape (N, D), N is number of \n",
    "            data samples, D is number of features.\n",
    "    \n",
    "    Returns:\n",
    "        min_value (np.array): Per-feature min value, shape (D, ).\n",
    "        max_value (np.array): Per-feature max value, shape (D, ).\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    min_value = ...\n",
    "    max_value = ...\n",
    "    return min_value, max_value\n",
    "    ###\n",
    "\n",
    "# Test your implementation.\n",
    "tests.test_normalization(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. The Most and the Least Influential Feature (2 pts)\n",
    "\n",
    "Assume we have normalized our data, added a bias term (column of ones as the 0'th feature), and then trained it with linear regression. Let us consider what the weights of our trained model mean. \n",
    "\n",
    "Can you, by looking at the  weights, tell us the least influential (feature that affects the label the least), and the most influential features? Fill in the `find_least_and_most_influential_features()` function to return the feature indices.\n",
    "\n",
    "**When finding the results, ignore the bias feature. This means that you should never return the bias feature index 0 as a result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_least_and_most_influential_features(w):\n",
    "    \"\"\" Finds the least and most influential feature indices and returns them. \n",
    "    \n",
    "    Args:\n",
    "        w (np.array): Weights, shape (D, ), where D is number of features.\n",
    "    \n",
    "    Returns:\n",
    "        least_inf (int): Least influential feature index.\n",
    "        most_inf (np.array): Most influential feature index.\n",
    "    \"\"\"\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    least_inf =  ...\n",
    "    most_inf =  ...\n",
    "    return least_inf, most_inf\n",
    "\n",
    "# Test your implementation.\n",
    "tests.test_influential_features(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Positively Correlated and Negatively Correlated Features (2 pts) \n",
    "\n",
    "By looking at the model weights, can you write a function that returns the features which have a positive correlation with the label and the features which have a negative correlation with the label? Return them as numpy arrays. Again, ignore the bias feature.\n",
    "\n",
    "Hint: `np.nonzero()` or `np.where()` might be useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positively_correlated_negatively_correlated(w):\n",
    "    \"\"\" Finds indices of features that are positively correlated with the label and\n",
    "        negatively correlated with the label.\n",
    "    \n",
    "    Args:\n",
    "        w (np.array): Weights, shape (D, ), where D is number of features.\n",
    "    \n",
    "    Returns:\n",
    "        negatively_corr (np.ndarray): Negatively correlated feature indices.\n",
    "        positively_corr (np.ndarray): Positively correlated feature indices.\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    negatively_corr = ...\n",
    "    positively_corr = ...\n",
    "    return negatively_corr, positively_corr\n",
    "\n",
    "# Test your implementation.\n",
    "tests.test_correlated_features(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Don't forget to upload your answers file and this completed jupyter notebook!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
